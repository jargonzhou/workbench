{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI: Message Passing Interface\n",
    "* https://www.mpi-forum.org/\n",
    "* [Open MPI](https://www.open-mpi.org/)\n",
    "  * [FAQ](https://www.open-mpi.org/faq/?category=running#simple-spmd-run): such as running on multiple nodes\n",
    "* [MPI Testing Tool](https://github.com/open-mpi/mtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/GoogleDrive/wiki/jupyter-notebooks/Concurrency and Parallel/MPI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhoujiagen/.local/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# working directory\n",
    "%cd MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install OpenMP in Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ubuntu 20.04\n",
    "\n",
    "```shell\n",
    "$ sudo apt-get install -y libopenmpi-dev\n",
    "...\n",
    "update-alternatives: using /usr/lib/libpsm1/libpsm_infinipath.so.1.16 to provide /usr/lib/x86_64-linux-gnu/libpsm_infinipath.so.1 (libpsm_infinipath.so.1) in auto mode\n",
    "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f95 (f95) in auto mode\n",
    "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f77 (f77) in auto mode\n",
    "update-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode\n",
    "update-alternatives: using /usr/bin/mpicc.openmpi to provide /usr/bin/mpicc (mpi) in auto mode\n",
    "update-alternatives: using /usr/lib/x86_64-linux-gnu/openmpi/include to provide /usr/include/x86_64-linux-gnu/mpi (mpi-x86_64-linux-gnu) inauto mode\n",
    "\n",
    "$ mpicc --version\n",
    "gcc (Ubuntu 11.2.0-19ubuntu1) 11.2.0\n",
    "$ mpiexec --version\n",
    "mpiexec (OpenRTE) 4.0.3\n",
    "$ mpirun --version\n",
    "mpirun (Open MPI) 4.0.3\n",
    "\n",
    "# add to include path: /usr/include/x86_64-linux-gnu/mpi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpiCC\n",
      "mpiCC.openmpi\n",
      "mpic++\n",
      "mpic++.openmpi\n",
      "mpicc\n",
      "mpicc.openmpi\n",
      "mpicxx\n",
      "mpicxx.openmpi\n",
      "mpiexec\n",
      "mpiexec.openmpi\n",
      "mpif77\n",
      "mpif77.openmpi\n",
      "mpif90\n",
      "mpif90.openmpi\n",
      "mpifort\n",
      "mpifort.openmpi\n",
      "mpirun\n",
      "mpirun.openmpi\n",
      "gcc (Ubuntu 11.2.0-19ubuntu1) 11.2.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "mpirun (Open MPI) 4.0.3\n",
      "\n",
      "Report bugs to http://www.open-mpi.org/community/help/\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/bin | grep -E '^mpi'\n",
    "!mpicc --version\n",
    "!mpirun --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mpirun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpirun (Open MPI) 4.0.3\n",
      "\n",
      "Usage: mpirun [OPTION]...  [PROGRAM]...\n",
      "Start the given program using Open RTE\n",
      "\n",
      "-c|-np|--np <arg0>       Number of processes to run\n",
      "-h|--help <arg0>         This help message\n",
      "   -n|--n <arg0>         Number of processes to run\n",
      "-q|--quiet               Suppress helpful messages\n",
      "-v|--verbose             Be verbose\n",
      "-V|--version             Print version and exit\n",
      "\n",
      "For additional mpirun arguments, run 'mpirun --help <category>'\n",
      "\n",
      "The following categories exist: general (Defaults to this option), debug,\n",
      "    output, input, mapping, ranking, binding, devel (arguments useful to OMPI\n",
      "    Developers), compatibility (arguments supported for backwards compatibility),\n",
      "    launch (arguments to modify launch options), and dvm (Distributed Virtual\n",
      "    Machine arguments).\n",
      "\n",
      "Report bugs to http://www.open-mpi.org/community/help/\n"
     ]
    }
   ],
   "source": [
    "!mpirun --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpirun (Open MPI) 4.0.3\n",
      "\n",
      "Usage: mpirun [OPTION]...  [PROGRAM]...\n",
      "Start the given program using Open RTE\n",
      "\n",
      "   -allow-run-as-root|--allow-run-as-root \n",
      "                         Allow execution as root (STRONGLY DISCOURAGED)\n",
      "   -am <arg0>            Aggregate MCA parameter set file list\n",
      "   --app <arg0>          Provide an appfile; ignore all other command line\n",
      "                         options\n",
      "   -default-hostfile|--default-hostfile <arg0>  \n",
      "                         Provide a default hostfile\n",
      "   -enable-instant-on-support|--enable-instant-on-support \n",
      "                         Enable PMIx-based instant on launch support\n",
      "                         (experimental)\n",
      "   -fwd-mpirun-port|--fwd-mpirun-port \n",
      "                         Forward mpirun port to compute node daemons so all\n",
      "                         will use it\n",
      "   -hostfile|--hostfile <arg0>  \n",
      "                         Provide a hostfile\n",
      "   -launch-agent|--launch-agent <arg0>  \n",
      "                         Command used to start processes on remote nodes\n",
      "                         (default: orted)\n",
      "   -machinefile|--machinefile <arg0>  \n",
      "                         Provide a hostfile\n",
      "   --noprefix            Disable automatic --prefix behavior\n",
      "   -path|--path <arg0>   PATH to be used to look for executables to start\n",
      "                         processes\n",
      "   -personality|--personality <arg0>  \n",
      "                         Comma-separated list of programming model,\n",
      "                         languages, and containers being used\n",
      "                         (default=\"ompi\")\n",
      "   --prefix <arg0>       Prefix where Open MPI is installed on remote nodes\n",
      "   --preload-files <arg0>  \n",
      "                         Preload the comma separated list of files to the\n",
      "                         remote machines current working directory before\n",
      "                         starting the remote process.\n",
      "-s|--preload-binary      Preload the binary on the remote machine before\n",
      "                         starting the remote process.\n",
      "   -set-cwd-to-session-dir|--set-cwd-to-session-dir \n",
      "                         Set the working directory of the started processes\n",
      "                         to their session directory\n",
      "   -show-progress|--show-progress \n",
      "                         Output a brief periodic report on launch progress\n",
      "   -use-regexp|--use-regexp \n",
      "                         Use regular expressions for launch\n",
      "   -wd|--wd <arg0>       Synonym for --wdir\n",
      "   -wdir|--wdir <arg0>   Set the working directory of the started processes\n",
      "-x <arg0>                Export an environment variable, optionally\n",
      "                         specifying a value (e.g., \"-x foo\" exports the\n",
      "                         environment variable foo and takes its value from\n",
      "                         the current environment; \"-x foo=bar\" exports the\n",
      "                         environment variable name foo and sets its value to\n",
      "                         \"bar\" in the started processes)\n",
      "\n",
      "Report bugs to http://www.open-mpi.org/community/help/\n"
     ]
    }
   ],
   "source": [
    "!mpirun --help launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpirun (Open MPI) 4.0.3\n",
      "\n",
      "Usage: mpirun [OPTION]...  [PROGRAM]...\n",
      "Start the given program using Open RTE\n",
      "\n",
      "   -dvm|--dvm            Create a persistent distributed virtual machine\n",
      "                         (DVM)\n",
      "   -hnp|--hnp <arg0>     Specify the URI of the HNP, or the name of the file\n",
      "                         (specified as file:filename) that contains that\n",
      "                         info\n",
      "   -max-vm-size|--max-vm-size <arg0>  \n",
      "                         Number of processes to run\n",
      "   -novm|--novm          Execute without creating an allocation-spanning\n",
      "                         virtual machine (only start daemons on nodes\n",
      "                         hosting application procs)\n",
      "   -ompi-server|--ompi-server <arg0>  \n",
      "                         Specify the URI of the publish/lookup server, or\n",
      "                         the name of the file (specified as file:filename)\n",
      "                         that contains that info\n",
      "\n",
      "Report bugs to http://www.open-mpi.org/community/help/\n"
     ]
    }
   ],
   "source": [
    "!mpirun --help dvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World\n",
    "* [MPI/aipp_3_1.c](./MPI/aipp_3_1.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  compile\n",
    "!mpicc -g -Wall -o aipp_3_1 aipp_3_1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings from process 0 of 1!\n"
     ]
    }
   ],
   "source": [
    "#  run with 1 process\n",
    "!mpiexec -n 1 aipp_3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings from process 0 of 4!\n",
      "Greetings from process 1 of 4!\n",
      "Greetings from process 2 of 4!\n",
      "Greetings from process 3 of 4!\n"
     ]
    }
   ],
   "source": [
    "# run with 4 processes\n",
    "!mpiexec -n 4 aipp_3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "!rm -f aipp_3_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help\n",
    "* Man pages: https://docs.open-mpi.org/en/v5.0.x/man-openmpi/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPIRUN(1)                          Open MPI                          MPIRUN(1)\n",
      "\n",
      "NAME\n",
      "       orterun,  mpirun,  mpiexec  -  Execute serial and parallel jobs in Open\n",
      "       MPI.  oshrun, shmemrun - Execute  serial  and  parallel  jobs  in  Open\n",
      "       SHMEM.\n",
      "\n",
      "       Note:  mpirun,  mpiexec, and orterun are all synonyms for each other as\n",
      "       well as oshrun, shmemrun in case Open SHMEM is installed.  Using any of\n",
      "       the names will produce the same behavior.\n",
      "\n",
      "SYNOPSIS\n",
      "       Single Process Multiple Data (SPMD) Model:\n",
      "\n",
      "       mpirun [ options ] <program> [ <args> ]\n",
      "\n",
      "       Multiple Instruction Multiple Data (MIMD) Model:\n",
      "\n",
      "       mpirun [ global_options ]\n",
      "              [ local_options1 ] <program1> [ <args1> ] :\n",
      "              [ local_options2 ] <program2> [ <args2> ] :\n",
      "              ... :\n",
      "              [ local_optionsN ] <programN> [ <argsN> ]\n",
      "\n",
      "       Note  that in both models, invoking mpirun via an absolute path name is\n",
      "       equivalent to specifying the --prefix option with a <dir> value equiva‐\n",
      "       lent  to  the  directory where mpirun resides, minus its last subdirec‐\n",
      "       tory.  For example:\n",
      "\n",
      "           % /usr/local/bin/mpirun ...\n",
      "\n",
      "       is equivalent to\n",
      "\n",
      "           % mpirun --prefix /usr/local\n",
      "\n",
      "QUICK SUMMARY\n",
      "       If you are simply looking for how to run an MPI application, you proba‐\n",
      "       bly want to use a command line of the following form:\n",
      "\n",
      "           % mpirun [ -np X ] [ --hostfile <filename> ]  <program>\n",
      "\n",
      "       This  will  run X copies of <program> in your current run-time environ‐\n",
      "       ment (if running under a supported resource manager, Open MPI's  mpirun\n",
      "       will  usually  automatically  use  the  corresponding  resource manager\n",
      "       process starter, as opposed to, for example, rsh or ssh, which  require\n",
      "       the  use  of a hostfile, or will default to running all X copies on the\n",
      "       localhost), scheduling (by default) in a  round-robin  fashion  by  CPU\n",
      "       slot.  See the rest of this page for more details.\n",
      "\n",
      "       Please  note  that mpirun automatically binds processes as of the start\n",
      "       of the v1.8 series. Three binding patterns are used in the  absence  of\n",
      "       any further directives:\n",
      "\n",
      "       Bind to core:     when the number of processes is <= 2\n",
      "\n",
      "       Bind to socket:   when the number of processes is > 2\n",
      "\n",
      "       Bind to none:     when oversubscribed\n",
      "\n",
      "       If your application uses threads, then you probably want to ensure that\n",
      "       you are either not bound at all  (by  specifying  --bind-to  none),  or\n",
      "       bound  to multiple cores using an appropriate binding level or specific\n",
      "       number of processing elements per application process.\n",
      "\n",
      "OPTIONS\n",
      "       mpirun will send the name of the directory where it was invoked on  the\n",
      "       local  node  to each of the remote nodes, and attempt to change to that\n",
      "       directory.  See the \"Current Working Directory\" section below for  fur‐\n",
      "       ther details.\n",
      "\n",
      "       <program> The  program executable. This is identified as the first non-\n",
      "                 recognized argument to mpirun.\n",
      "\n",
      "       <args>    Pass these run-time arguments to every  new  process.   These\n",
      "                 must  always  be the last arguments to mpirun. If an app con‐\n",
      "                 text file is used, <args> will be ignored.\n",
      "\n",
      "       -h, --help\n",
      "                 Display help for this command\n",
      "\n",
      "       -q, --quiet\n",
      "                 Suppress informative messages from orterun during application\n",
      "                 execution.\n",
      "\n",
      "       -v, --verbose\n",
      "                 Be verbose\n",
      "\n",
      "       -V, --version\n",
      "                 Print  version number.  If no other arguments are given, this\n",
      "                 will also cause orterun to exit.\n",
      "\n",
      "       -N <num>\n",
      "                 Launch num processes per node on all allocated nodes (synonym\n",
      "                 for npernode).\n",
      "\n",
      "       -display-map, --display-map\n",
      "                 Display  a  table showing the mapped location of each process\n",
      "                 prior to launch.\n",
      "\n",
      "       -display-allocation, --display-allocation\n",
      "                 Display the detected resource allocation.\n",
      "\n",
      "       -output-proctable, --output-proctable\n",
      "                 Output the debugger proctable after launch.\n",
      "\n",
      "       -dvm, --dvm\n",
      "                 Create a persistent distributed virtual machine (DVM).\n",
      "\n",
      "       -max-vm-size, --max-vm-size <size>\n",
      "                 Number of processes to run.\n",
      "\n",
      "       -novm, --novm\n",
      "                 Execute without creating an allocation-spanning  virtual  ma‐\n",
      "                 chine  (only  start  daemons  on  nodes  hosting  application\n",
      "                 procs).\n",
      "\n",
      "       -hnp, --hnp <arg0>\n",
      "                 Specify the URI of the Head Node Process (HNP), or  the  name\n",
      "                 of  the  file (specified as file:filename) that contains that\n",
      "                 info.\n",
      "\n",
      "       Use one of the following options to specify which hosts (nodes) of  the\n",
      "       cluster  to  run  on.  Note  that  as of the start of the v1.8 release,\n",
      "       mpirun will launch a daemon onto each host in the allocation (as  modi‐\n",
      "       fied  by the following options) at the very beginning of execution, re‐\n",
      "       gardless of whether or not application  processes  will  eventually  be\n",
      "       mapped  to  execute there. This is done to allow collection of hardware\n",
      "       topology information from the remote nodes, thus  allowing  us  to  map\n",
      "       processes  against known topology. However, it is a change from the be‐\n",
      "       havior in prior releases where daemons were only launched after mapping\n",
      "       was  complete,  and  thus only occurred on nodes where application pro‐\n",
      "       cesses would actually be executing.\n",
      "\n",
      "       -H, -host, --host <host1,host2,...,hostN>\n",
      "              List of hosts on which to invoke processes.\n",
      "\n",
      "       -hostfile, --hostfile <hostfile>\n",
      "              Provide a hostfile to use.\n",
      "\n",
      "       -default-hostfile, --default-hostfile <hostfile>\n",
      "              Provide a default hostfile.\n",
      "\n",
      "       -machinefile, --machinefile <machinefile>\n",
      "              Synonym for -hostfile.\n",
      "\n",
      "       -cpu-set, --cpu-set <list>\n",
      "              Restrict launched processes to the  specified  logical  cpus  on\n",
      "              each  node (comma-separated list). Note that the binding options\n",
      "              will still apply within the specified envelope - e.g.,  you  can\n",
      "              elect  to bind each process to only one cpu within the specified\n",
      "              cpu set.\n",
      "\n",
      "       The following options specify the number of processes to  launch.  Note\n",
      "       that  none of the options imply a particular binding policy - e.g., re‐\n",
      "       questing N processes for each socket does not imply that the  processes\n",
      "       will be bound to the socket.\n",
      "\n",
      "       -c, -n, --n, -np <#>\n",
      "              Run  this  many  copies of the program on the given nodes.  This\n",
      "              option indicates that the specified file is an  executable  pro‐\n",
      "              gram and not an application context. If no value is provided for\n",
      "              the number of copies to execute (i.e., neither the \"-np\" nor its\n",
      "              synonyms  are provided on the command line), Open MPI will auto‐\n",
      "              matically execute a copy of the program  on  each  process  slot\n",
      "              (see  below  for description of a \"process slot\"). This feature,\n",
      "              however, can only be used in the SPMD model and will  return  an\n",
      "              error  (without  beginning  execution of the application) other‐\n",
      "              wise.\n",
      "\n",
      "       —map-by ppr:N:<object>\n",
      "              Launch N times the number of objects of the  specified  type  on\n",
      "              each node.\n",
      "\n",
      "       -npersocket, --npersocket <#persocket>\n",
      "              On  each  node,  launch  this many processes times the number of\n",
      "              processor sockets on the  node.   The  -npersocket  option  also\n",
      "              turns  on  the  -bind-to-socket option.  (deprecated in favor of\n",
      "              --map-by ppr:n:socket)\n",
      "\n",
      "       -npernode, --npernode <#pernode>\n",
      "              On each node, launch this many processes.  (deprecated in  favor\n",
      "              of --map-by ppr:n:node)\n",
      "\n",
      "       -pernode, --pernode\n",
      "              On  each  node, launch one process -- equivalent to -npernode 1.\n",
      "              (deprecated in favor of --map-by ppr:1:node)\n",
      "\n",
      "       To map processes:\n",
      "\n",
      "       --map-by <foo>\n",
      "              Map to the specified object, defaults to socket.  Supported  op‐\n",
      "              tions  include  slot, hwthread, core, L1cache, L2cache, L3cache,\n",
      "              socket, numa, board, node, sequential, distance,  and  ppr.  Any\n",
      "              object  can  include modifiers by adding a : and any combination\n",
      "              of PE=n (bind n processing elements to each  proc),  SPAN  (load\n",
      "              balance the processes across the allocation), OVERSUBSCRIBE (al‐\n",
      "              low more processes on a  node  than  processing  elements),  and\n",
      "              NOOVERSUBSCRIBE.   This includes PPR, where the pattern would be\n",
      "              terminated by another colon to separate it from the modifiers.\n",
      "\n",
      "       -bycore, --bycore\n",
      "              Map processes by core (deprecated in favor of --map-by core)\n",
      "\n",
      "       -byslot, --byslot\n",
      "              Map and rank processes round-robin by slot.\n",
      "\n",
      "       -nolocal, --nolocal\n",
      "              Do not run any copies of the launched application  on  the  same\n",
      "              node  as  orterun is running.  This option will override listing\n",
      "              the localhost with --host or any  other  host-specifying  mecha‐\n",
      "              nism.\n",
      "\n",
      "       -nooversubscribe, --nooversubscribe\n",
      "              Do not oversubscribe any nodes; error (without starting any pro‐\n",
      "              cesses) if the requested number of processes would  cause  over‐\n",
      "              subscription.   This option implicitly sets \"max_slots\" equal to\n",
      "              the \"slots\" value for each node. (Enabled by default).\n",
      "\n",
      "       -oversubscribe, --oversubscribe\n",
      "              Nodes are allowed to be oversubscribed, even on a  managed  sys‐\n",
      "              tem, and overloading of processing elements.\n",
      "\n",
      "       -bynode, --bynode\n",
      "              Launch  processes one per node, cycling by node in a round-robin\n",
      "              fashion.  This spreads processes evenly among nodes and  assigns\n",
      "              MPI_COMM_WORLD ranks in a round-robin, \"by node\" manner.\n",
      "\n",
      "       -cpu-list, --cpu-list <cpus>\n",
      "              Comma-delimited list of processor IDs to which to bind processes\n",
      "              [default=NULL].  Processor IDs are interpreted as hwloc  logical\n",
      "              core  IDs.   Run  the  hwloc  lstopo(1) command to see a list of\n",
      "              available cores and their logical IDs.\n",
      "\n",
      "       To order processes' ranks in MPI_COMM_WORLD:\n",
      "\n",
      "       --rank-by <foo>\n",
      "              Rank in round-robin fashion according to the  specified  object,\n",
      "              defaults  to  slot.  Supported  options  include slot, hwthread,\n",
      "              core, L1cache, L2cache, L3cache, socket, numa, board, and node.\n",
      "\n",
      "       For process binding:\n",
      "\n",
      "       --bind-to <foo>\n",
      "              Bind processes to the specified object, defaults to  core.  Sup‐\n",
      "              ported  options  include slot, hwthread, core, l1cache, l2cache,\n",
      "              l3cache, socket, numa, board, cpu-list, and none.\n",
      "\n",
      "       -cpus-per-proc, --cpus-per-proc <#perproc>\n",
      "              Bind each process to the specified number of cpus.   (deprecated\n",
      "              in favor of --map-by <obj>:PE=n)\n",
      "\n",
      "       -cpus-per-rank, --cpus-per-rank <#perrank>\n",
      "              Alias  for  -cpus-per-proc.   (deprecated  in  favor of --map-by\n",
      "              <obj>:PE=n)\n",
      "\n",
      "       -bind-to-core, --bind-to-core\n",
      "              Bind processes to cores (deprecated in favor of --bind-to core)\n",
      "\n",
      "       -bind-to-socket, --bind-to-socket\n",
      "              Bind processes to processor sockets   (deprecated  in  favor  of\n",
      "              --bind-to socket)\n",
      "\n",
      "       -report-bindings, --report-bindings\n",
      "              Report any bindings for launched processes.\n",
      "\n",
      "       For rankfiles:\n",
      "\n",
      "       -rf, --rankfile <rankfile>\n",
      "              Provide a rankfile file.\n",
      "\n",
      "       To manage standard I/O:\n",
      "\n",
      "       -output-filename, --output-filename <filename>\n",
      "              Redirect  the  stdout, stderr, and stddiag of all processes to a\n",
      "              process-unique version of the specified filename.  Any  directo‐\n",
      "              ries in the filename will automatically be created.  Each output\n",
      "              file will consist of filename.id, where the id will be the  pro‐\n",
      "              cesses' rank in MPI_COMM_WORLD, left-filled with zero's for cor‐\n",
      "              rect ordering in listings. A relative path value  will  be  con‐\n",
      "              verted to an absolute path based on the cwd where mpirun is exe‐\n",
      "              cuted. Note that this will not work on  environments  where  the\n",
      "              file  system  on compute nodes differs from that where mpirun is\n",
      "              executed.\n",
      "\n",
      "       -stdin, --stdin <rank>\n",
      "              The MPI_COMM_WORLD rank of the process that is to receive stdin.\n",
      "              The  default  is  to forward stdin to MPI_COMM_WORLD rank 0, but\n",
      "              this option can be used to forward stdin to any process.  It  is\n",
      "              also  acceptable  to  specify none, indicating that no processes\n",
      "              are to receive stdin.\n",
      "\n",
      "       -merge-stderr-to-stdout, --merge-stderr-to-stdout\n",
      "              Merge stderr to stdout for each process.\n",
      "\n",
      "       -tag-output, --tag-output\n",
      "              Tag each line of output to stdout, stderr, and stddiag with [jo‐\n",
      "              bid,   MCW_rank]<stdxxx>   indicating   the  process  jobid  and\n",
      "              MPI_COMM_WORLD rank of the process that  generated  the  output,\n",
      "              and the channel which generated it.\n",
      "\n",
      "       -timestamp-output, --timestamp-output\n",
      "              Timestamp each line of output to stdout, stderr, and stddiag.\n",
      "\n",
      "       -xml, --xml\n",
      "              Provide all output to stdout, stderr, and stddiag in an xml for‐\n",
      "              mat.\n",
      "\n",
      "       -xml-file, --xml-file <filename>\n",
      "              Provide all output in XML format to the specified file.\n",
      "\n",
      "       -xterm, --xterm <ranks>\n",
      "              Display the  output  from  the  processes  identified  by  their\n",
      "              MPI_COMM_WORLD  ranks  in  separate xterm windows. The ranks are\n",
      "              specified as a comma-separated list of ranges, with a  -1  indi‐\n",
      "              cating all. A separate window will be created for each specified\n",
      "              process.  Note: xterm will normally terminate  the  window  upon\n",
      "              termination of the process running within it. However, by adding\n",
      "              a \"!\" to the end of the list of specified ranks, the proper  op‐\n",
      "              tions  will  be  provided  to ensure that xterm keeps the window\n",
      "              open after the process terminates, thus allowing you to see  the\n",
      "              process' output.  Each xterm window will subsequently need to be\n",
      "              manually closed.  Note: In some environments, xterm may  require\n",
      "              that  the  executable  be in the user's path, or be specified in\n",
      "              absolute or relative terms. Thus, it may be necessary to specify\n",
      "              a  local  executable  as \"./foo\" instead of just \"foo\". If xterm\n",
      "              fails to find the executable, mpirun will hang,  but  still  re‐\n",
      "              spond correctly to a ctrl-c.  If this happens, please check that\n",
      "              the executable is being specified correctly and try again.\n",
      "\n",
      "       To manage files and runtime environment:\n",
      "\n",
      "       -path, --path <path>\n",
      "              <path> that will be used when attempting to locate the requested\n",
      "              executables.   This  is  used prior to using the local PATH set‐\n",
      "              ting.\n",
      "\n",
      "       --prefix <dir>\n",
      "              Prefix directory that will be used to set the  PATH  and  LD_LI‐\n",
      "              BRARY_PATH  on  the  remote node before invoking Open MPI or the\n",
      "              target process.  See the \"Remote Execution\" section, below.\n",
      "\n",
      "       --noprefix\n",
      "              Disable the automatic --prefix behavior\n",
      "\n",
      "       -s, --preload-binary\n",
      "              Copy the specified executable(s) to  remote  machines  prior  to\n",
      "              starting remote processes. The executables will be copied to the\n",
      "              Open MPI session directory and will be deleted  upon  completion\n",
      "              of the job.\n",
      "\n",
      "       --preload-files <files>\n",
      "              Preload the comma separated list of files to the current working\n",
      "              directory  of  the  remote  machines  where  processes  will  be\n",
      "              launched prior to starting those processes.\n",
      "\n",
      "       -set-cwd-to-session-dir, --set-cwd-to-session-dir\n",
      "              Set the working directory of the started processes to their ses‐\n",
      "              sion directory.\n",
      "\n",
      "       -wd <dir>\n",
      "              Synonym for -wdir.\n",
      "\n",
      "       -wdir <dir>\n",
      "              Change to the directory <dir> before  the  user's  program  exe‐\n",
      "              cutes.  See the \"Current Working Directory\" section for notes on\n",
      "              relative paths.  Note: If the -wdir option appears both  on  the\n",
      "              command  line  and  in  an application context, the context will\n",
      "              take precedence over the command line. Thus, if the path to  the\n",
      "              desired  wdir is different on the backend nodes, then it must be\n",
      "              specified as an absolute path that is correct  for  the  backend\n",
      "              node.\n",
      "\n",
      "       -x <env>\n",
      "              Export  the  specified environment variables to the remote nodes\n",
      "              before executing the program.  Only one environment variable can\n",
      "              be  specified per -x option.  Existing environment variables can\n",
      "              be specified or new variable names specified with  corresponding\n",
      "              values.  For example:\n",
      "                  % mpirun -x DISPLAY -x OFILE=/tmp/out ...\n",
      "\n",
      "              The  parser for the -x option is not very sophisticated; it does\n",
      "              not even understand quoted values.  Users  are  advised  to  set\n",
      "              variables in the environment, and then use -x to export (not de‐\n",
      "              fine) them.\n",
      "\n",
      "       Setting MCA parameters:\n",
      "\n",
      "       -gmca, --gmca <key> <value>\n",
      "              Pass global MCA parameters that are applicable to all  contexts.\n",
      "              <key> is the parameter name; <value> is the parameter value.\n",
      "\n",
      "       -mca, --mca <key> <value>\n",
      "              Send  arguments  to various MCA modules.  See the \"MCA\" section,\n",
      "              below.\n",
      "\n",
      "       -am <arg0>\n",
      "              Aggregate MCA parameter set file list.\n",
      "\n",
      "       -tune, --tune <tune_file>\n",
      "              Specify a tune file to set arguments for various MCA modules and\n",
      "              environment  variables.  See the \"Setting MCA parameters and en‐\n",
      "              vironment variables from file\" section, below.\n",
      "\n",
      "       For debugging:\n",
      "\n",
      "       -debug, --debug\n",
      "              Invoke   the    user-level    debugger    indicated    by    the\n",
      "              orte_base_user_debugger MCA parameter.\n",
      "\n",
      "       --get-stack-traces\n",
      "              When  paired  with  the --timeout option, mpirun will obtain and\n",
      "              print out stack traces from  all  launched  processes  that  are\n",
      "              still alive when the timeout expires.  Note that obtaining stack\n",
      "              traces can take a little time and produce a lot of output, espe‐\n",
      "              cially for large process-count jobs.\n",
      "\n",
      "       -debugger, --debugger <args>\n",
      "              Sequence  of  debuggers to search for when --debug is used (i.e.\n",
      "              a synonym for orte_base_user_debugger MCA parameter).\n",
      "\n",
      "       --timeout <seconds>\n",
      "              The maximum  number  of  seconds  that  mpirun  (also  known  as\n",
      "              mpiexec, oshrun, orterun, etc.)  will run.  After this many sec‐\n",
      "              onds, mpirun will abort the launched job and exit  with  a  non-\n",
      "              zero  exit status.  Using --timeout can be also useful when com‐\n",
      "              bined with the --get-stack-traces option.\n",
      "\n",
      "       -tv, --tv\n",
      "              Launch processes under the TotalView debugger.  Deprecated back‐\n",
      "              wards compatibility flag. Synonym for --debug.\n",
      "\n",
      "       There are also other options:\n",
      "\n",
      "       --allow-run-as-root\n",
      "              Allow  mpirun  to run when executed by the root user (mpirun de‐\n",
      "              faults to aborting when launched as the root user).  Be sure  to\n",
      "              see the Running as root section, below, for more detail.\n",
      "\n",
      "       --app <appfile>\n",
      "              Provide an appfile, ignoring all other command line options.\n",
      "\n",
      "       -cf, --cartofile <cartofile>\n",
      "              Provide a cartography file.\n",
      "\n",
      "       -continuous, --continuous\n",
      "              Job is to run until explicitly terminated.\n",
      "\n",
      "       -disable-recovery, --disable-recovery\n",
      "              Disable recovery (resets all recovery options to off).\n",
      "\n",
      "       -do-not-launch, --do-not-launch\n",
      "              Perform all necessary operations to prepare to launch the appli‐\n",
      "              cation, but do not actually launch it.\n",
      "\n",
      "       -do-not-resolve, --do-not-resolve\n",
      "              Do not attempt to resolve interfaces.\n",
      "\n",
      "       -enable-recovery, --enable-recovery\n",
      "              Enable recovery from process failure [Default = disabled].\n",
      "\n",
      "       -index-argv-by-rank, --index-argv-by-rank\n",
      "              Uniquely index argv[0] for each process using its rank.\n",
      "\n",
      "       -leave-session-attached, --leave-session-attached\n",
      "              Do not detach OmpiRTE daemons used by this application. This al‐\n",
      "              lows  error  messages from the daemons as well as the underlying\n",
      "              environment (e.g., when failing to launch a daemon) to  be  out‐\n",
      "              put.\n",
      "\n",
      "       -max-restarts, --max-restarts <num>\n",
      "              Max number of times to restart a failed process.\n",
      "\n",
      "       -ompi-server, --ompi-server <uri or file>\n",
      "              Specify the URI of the Open MPI server (or the mpirun to be used\n",
      "              as the server), the name of the file  (specified  as  file:file‐\n",
      "              name)  that  contains that info, or the PID (specified as pid:#)\n",
      "              of the mpirun to be used as the server.  The Open MPI server  is\n",
      "              used  to  support  multi-application data exchange via the MPI-2\n",
      "              MPI_Publish_name and MPI_Lookup_name functions.\n",
      "\n",
      "       -personality, --personality <list>\n",
      "              Comma-separated list of programming model, languages,  and  con‐\n",
      "              tainers being used (default=\"ompi\").\n",
      "\n",
      "       --ppr <list>\n",
      "              Comma-separated  list of number of processes on a given resource\n",
      "              type [default: none].\n",
      "\n",
      "       -report-child-jobs-separately, --report-child-jobs-separately\n",
      "              Return the exit status of the primary job only.\n",
      "\n",
      "       -report-events, --report-events <URI>\n",
      "              Report events to a tool listening at the specified URI.\n",
      "\n",
      "       -report-pid, --report-pid <channel>\n",
      "              Print out mpirun's PID during startup. The channel must  be  ei‐\n",
      "              ther a '-' to indicate that the pid is to be output to stdout, a\n",
      "              '+' to indicate that the pid is to be output  to  stderr,  or  a\n",
      "              filename to which the pid is to be written.\n",
      "\n",
      "       -report-uri, --report-uri <channel>\n",
      "              Print  out  mpirun's URI during startup. The channel must be ei‐\n",
      "              ther a '-' to indicate that the URI is to be output to stdout, a\n",
      "              '+'  to  indicate  that  the URI is to be output to stderr, or a\n",
      "              filename to which the URI is to be written.\n",
      "\n",
      "       -show-progress, --show-progress\n",
      "              Output a brief periodic report on launch progress.\n",
      "\n",
      "       -terminate, --terminate\n",
      "              Terminate the DVM.\n",
      "\n",
      "       -use-hwthread-cpus, --use-hwthread-cpus\n",
      "              Use hardware threads as independent cpus.\n",
      "\n",
      "       -use-regexp, --use-regexp\n",
      "              Use regular expressions for launch.\n",
      "\n",
      "       The following options are useful for developers; they are not generally\n",
      "       useful to most ORTE and/or MPI users:\n",
      "\n",
      "       -d, --debug-devel\n",
      "              Enable  debugging  of  the  OmpiRTE  (the run-time layer in Open\n",
      "              MPI).  This is not generally useful for most users.\n",
      "\n",
      "       --debug-daemons\n",
      "              Enable debugging of any OmpiRTE daemons used  by  this  applica‐\n",
      "              tion.\n",
      "\n",
      "       --debug-daemons-file\n",
      "              Enable  debugging  of  any OmpiRTE daemons used by this applica‐\n",
      "              tion, storing output in files.\n",
      "\n",
      "       -display-devel-allocation, --display-devel-allocation\n",
      "              Display a detailed list of the allocation  being  used  by  this\n",
      "              job.\n",
      "\n",
      "       -display-devel-map, --display-devel-map\n",
      "              Display  a  more  detailed  table showing the mapped location of\n",
      "              each process prior to launch.\n",
      "\n",
      "       -display-diffable-map, --display-diffable-map\n",
      "              Display a diffable process map just before launch.\n",
      "\n",
      "       -display-topo, --display-topo\n",
      "              Display the topology as part of  the  process  map  just  before\n",
      "              launch.\n",
      "\n",
      "       -launch-agent, --launch-agent\n",
      "              Name  of the executable that is to be used to start processes on\n",
      "              the remote nodes. The default is \"orted\".  This  option  can  be\n",
      "              used to test new daemon concepts, or to pass options back to the\n",
      "              daemons without having mpirun  itself  see  them.  For  example,\n",
      "              specifying  a launch agent of orted -mca odls_base_verbose 5 al‐\n",
      "              lows the developer to ask the orted for debugging output without\n",
      "              clutter from mpirun itself.\n",
      "\n",
      "       --report-state-on-timeout\n",
      "              When  paired  with the --timeout command line option, report the\n",
      "              run-time subsystem state of each process when  the  timeout  ex‐\n",
      "              pires.\n",
      "\n",
      "       There may be other options listed with mpirun --help.\n",
      "\n",
      "   Environment Variables\n",
      "       MPIEXEC_TIMEOUT\n",
      "              Synonym for the --timeout command line option.\n",
      "\n",
      "DESCRIPTION\n",
      "       One  invocation  of mpirun starts an MPI application running under Open\n",
      "       MPI. If the application is single process multiple data (SPMD), the ap‐\n",
      "       plication can be specified on the mpirun command line.\n",
      "\n",
      "       If  the  application is multiple instruction multiple data (MIMD), com‐\n",
      "       prising of multiple programs, the set of programs and argument  can  be\n",
      "       specified  in one of two ways: Extended Command Line Arguments, and Ap‐\n",
      "       plication Context.\n",
      "\n",
      "       An application context describes the MIMD program set including all ar‐\n",
      "       guments  in  a  separate file.  This file essentially contains multiple\n",
      "       mpirun command lines, less the command name  itself.   The  ability  to\n",
      "       specify  different options for different instantiations of a program is\n",
      "       another reason to use an application context.\n",
      "\n",
      "       Extended command line arguments allow for the description of the appli‐\n",
      "       cation  layout  on  the  command  line using colons (:) to separate the\n",
      "       specification of programs and arguments. Some options are globally  set\n",
      "       across  all specified programs (e.g. --hostfile), while others are spe‐\n",
      "       cific to a single program (e.g. -np).\n",
      "\n",
      "   Specifying Host Nodes\n",
      "       Host nodes can be identified on the mpirun command line with the  -host\n",
      "       option or in a hostfile.\n",
      "\n",
      "       For example,\n",
      "\n",
      "       mpirun -H aa,aa,bb ./a.out\n",
      "           launches two processes on node aa and one on bb.\n",
      "\n",
      "       Or, consider the hostfile\n",
      "\n",
      "          % cat myhostfile\n",
      "          aa slots=2\n",
      "          bb slots=2\n",
      "          cc slots=2\n",
      "\n",
      "       Here,  we  list  both the host names (aa, bb, and cc) but also how many\n",
      "       \"slots\" there are for each.  Slots indicate how many processes can  po‐\n",
      "       tentially execute on a node.  For best performance, the number of slots\n",
      "       may be chosen to be the number of cores on the node or  the  number  of\n",
      "       processor sockets.  If the hostfile does not provide slots information,\n",
      "       Open MPI will attempt to discover the number of cores (or hwthreads, if\n",
      "       the use-hwthreads-as-cpus option is set) and set the number of slots to\n",
      "       that value. This default behavior also occurs when specifying the -host\n",
      "       option with a single hostname. Thus, the command\n",
      "\n",
      "       mpirun -H aa ./a.out\n",
      "           launches a number of processes equal to the number of cores on node\n",
      "           aa.\n",
      "\n",
      "       mpirun -hostfile myhostfile ./a.out\n",
      "           will launch two processes on each of the three nodes.\n",
      "\n",
      "       mpirun -hostfile myhostfile -host aa ./a.out\n",
      "           will launch two processes, both on node aa.\n",
      "\n",
      "       mpirun -hostfile myhostfile -host dd ./a.out\n",
      "           will find no hosts to run on and abort with an error.  That is, the\n",
      "           specified host dd is not in the specified hostfile.\n",
      "\n",
      "       When  running under resource managers (e.g., SLURM, Torque, etc.), Open\n",
      "       MPI will obtain both the hostnames and the  number  of  slots  directly\n",
      "       from the resource manger.\n",
      "\n",
      "   Specifying Number of Processes\n",
      "       As  we  have just seen, the number of processes to run can be set using\n",
      "       the hostfile.  Other mechanisms exist.\n",
      "\n",
      "       The number of processes launched can be specified as a multiple of  the\n",
      "       number of nodes or processor sockets available.  For example,\n",
      "\n",
      "       mpirun -H aa,bb -npersocket 2 ./a.out\n",
      "           launches processes 0-3 on node aa and process 4-7 on node bb, where\n",
      "           aa and bb are both dual-socket nodes.  The -npersocket option  also\n",
      "           turns  on the -bind-to-socket option, which is discussed in a later\n",
      "           section.\n",
      "\n",
      "       mpirun -H aa,bb -npernode 2 ./a.out\n",
      "           launches processes 0-1 on node aa and processes 2-3 on node bb.\n",
      "\n",
      "       mpirun -H aa,bb -npernode 1 ./a.out\n",
      "           launches one process per host node.\n",
      "\n",
      "       mpirun -H aa,bb -pernode ./a.out\n",
      "           is the same as -npernode 1.\n",
      "\n",
      "       Another alternative is to specify the number of processes with the  -np\n",
      "       option.  Consider now the hostfile\n",
      "\n",
      "          % cat myhostfile\n",
      "          aa slots=4\n",
      "          bb slots=4\n",
      "          cc slots=4\n",
      "\n",
      "       Now,\n",
      "\n",
      "       mpirun -hostfile myhostfile -np 6 ./a.out\n",
      "           will  launch processes 0-3 on node aa and processes 4-5 on node bb.\n",
      "           The remaining slots in the hostfile will not be used since the  -np\n",
      "           option indicated that only 6 processes should be launched.\n",
      "\n",
      "   Mapping Processes to Nodes: Using Policies\n",
      "       The  examples above illustrate the default mapping of process processes\n",
      "       to nodes.  This mapping can also be controlled with various mpirun  op‐\n",
      "       tions that describe mapping policies.\n",
      "\n",
      "       Consider the same hostfile as above, again with -np 6:\n",
      "\n",
      "                                 node aa      node bb      node cc\n",
      "\n",
      "         mpirun                  0 1 2 3      4 5\n",
      "\n",
      "         mpirun --map-by node    0 3          1 4          2 5\n",
      "\n",
      "         mpirun -nolocal                      0 1 2 3      4 5\n",
      "\n",
      "       The  --map-by  node  option  will load balance the processes across the\n",
      "       available nodes, numbering each process in a round-robin fashion.\n",
      "\n",
      "       The -nolocal option prevents any processes from being mapped  onto  the\n",
      "       local host (in this case node aa).  While mpirun typically consumes few\n",
      "       system resources, -nolocal can be helpful for launching very large jobs\n",
      "       where  mpirun  may  actually  need  to use noticeable amounts of memory\n",
      "       and/or processing time.\n",
      "\n",
      "       Just as -np can specify fewer processes than there are  slots,  it  can\n",
      "       also oversubscribe the slots.  For example, with the same hostfile:\n",
      "\n",
      "       mpirun -hostfile myhostfile -np 14 ./a.out\n",
      "           will  launch  processes  0-3 on node aa, 4-7 on bb, and 8-11 on cc.\n",
      "           It will then add the remaining two processes to whichever nodes  it\n",
      "           chooses.\n",
      "\n",
      "       One can also specify limits to oversubscription.  For example, with the\n",
      "       same hostfile:\n",
      "\n",
      "       mpirun -hostfile myhostfile -np 14 -nooversubscribe ./a.out\n",
      "           will produce an error since -nooversubscribe prevents oversubscrip‐\n",
      "           tion.\n",
      "\n",
      "       Limits  to  oversubscription  can also be specified in the hostfile it‐\n",
      "       self:\n",
      "        % cat myhostfile\n",
      "        aa slots=4 max_slots=4\n",
      "        bb         max_slots=4\n",
      "        cc slots=4\n",
      "\n",
      "       The max_slots field specifies such a limit.  When it  does,  the  slots\n",
      "       value defaults to the limit.  Now:\n",
      "\n",
      "       mpirun -hostfile myhostfile -np 14 ./a.out\n",
      "           causes the first 12 processes to be launched as before, but the re‐\n",
      "           maining two processes will be forced onto node cc.  The  other  two\n",
      "           nodes  are  protected  by  the hostfile against oversubscription by\n",
      "           this job.\n",
      "\n",
      "       Using the --nooversubscribe option can be helpful since Open  MPI  cur‐\n",
      "       rently does not get \"max_slots\" values from the resource manager.\n",
      "\n",
      "       Of course, -np can also be used with the -H or -host option.  For exam‐\n",
      "       ple,\n",
      "\n",
      "       mpirun -H aa,bb -np 8 ./a.out\n",
      "           launches 8 processes.  Since only two hosts  are  specified,  after\n",
      "           the  first  two  processes are mapped, one to aa and one to bb, the\n",
      "           remaining processes oversubscribe the specified hosts.\n",
      "\n",
      "       And here is a MIMD example:\n",
      "\n",
      "       mpirun -H aa -np 1 hostname : -H bb,cc -np 2 uptime\n",
      "           will launch process 0 running hostname on node aa and  processes  1\n",
      "           and 2 each running uptime on nodes bb and cc, respectively.\n",
      "\n",
      "   Mapping, Ranking, and Binding: Oh My!\n",
      "       Open  MPI  employs  a three-phase procedure for assigning process loca‐\n",
      "       tions and ranks:\n",
      "\n",
      "       mapping   Assigns a default location to each process\n",
      "\n",
      "       ranking   Assigns an MPI_COMM_WORLD rank value to each process\n",
      "\n",
      "       binding   Constrains each process to run on specific processors\n",
      "\n",
      "       The mapping step is used to assign a default location to  each  process\n",
      "       based  on the mapper being employed. Mapping by slot, node, and sequen‐\n",
      "       tially results in the assignment of the processes to the node level. In\n",
      "       contrast, mapping by object, allows the mapper to assign the process to\n",
      "       an actual object on each node.\n",
      "\n",
      "       Note: the location assigned to the process is independent of  where  it\n",
      "       will  be  bound - the assignment is used solely as input to the binding\n",
      "       algorithm.\n",
      "\n",
      "       The mapping of process processes to nodes can be defined not just  with\n",
      "       general  policies but also, if necessary, using arbitrary mappings that\n",
      "       cannot be described by a simple policy.  One can  use  the  \"sequential\n",
      "       mapper,\"  which reads the hostfile line by line, assigning processes to\n",
      "       nodes in whatever order the hostfile specifies.  Use the -mca rmaps seq\n",
      "       option.  For example, using the same hostfile as before:\n",
      "\n",
      "       mpirun -hostfile myhostfile -mca rmaps seq ./a.out\n",
      "\n",
      "       will  launch  three processes, one on each of nodes aa, bb, and cc, re‐\n",
      "       spectively.  The slot counts don't matter;  one process is launched per\n",
      "       line on whatever node is listed on the line.\n",
      "\n",
      "       Another  way  to  specify  arbitrary mappings is with a rankfile, which\n",
      "       gives you detailed control over process binding as well.  Rankfiles are\n",
      "       discussed below.\n",
      "\n",
      "       The second phase focuses on the ranking of the process within the job's\n",
      "       MPI_COMM_WORLD.  Open MPI separates this from the mapping procedure  to\n",
      "       allow more flexibility in the relative placement of MPI processes. This\n",
      "       is best illustrated by considering the following  two  cases  where  we\n",
      "       used the —map-by ppr:2:socket option:\n",
      "\n",
      "                                 node aa       node bb\n",
      "\n",
      "           rank-by core         0 1 ! 2 3     4 5 ! 6 7\n",
      "\n",
      "          rank-by socket        0 2 ! 1 3     4 6 ! 5 7\n",
      "\n",
      "          rank-by socket:span   0 4 ! 1 5     2 6 ! 3 7\n",
      "\n",
      "       Ranking  by  core  and  by slot provide the identical result - a simple\n",
      "       progression of MPI_COMM_WORLD ranks across each node. Ranking by socket\n",
      "       does  a  round-robin  ranking within each node until all processes have\n",
      "       been assigned an MCW rank, and then progresses to the next node. Adding\n",
      "       the span modifier to the ranking directive causes the ranking algorithm\n",
      "       to treat the entire allocation as a single entity - thus, the MCW ranks\n",
      "       are  assigned across all sockets before circling back around to the be‐\n",
      "       ginning.\n",
      "\n",
      "       The binding phase actually binds each process to a given set of proces‐\n",
      "       sors.  This  can improve performance if the operating system is placing\n",
      "       processes suboptimally.   For  example,  it  might  oversubscribe  some\n",
      "       multi-core  processor  sockets,  leaving  other sockets idle;  this can\n",
      "       lead processes to contend unnecessarily for common resources.   Or,  it\n",
      "       might  spread  processes out too widely;  this can be suboptimal if ap‐\n",
      "       plication performance is sensitive to interprocess communication costs.\n",
      "       Binding can also keep the operating system from migrating processes ex‐\n",
      "       cessively, regardless of how optimally those processes were  placed  to\n",
      "       begin with.\n",
      "\n",
      "       The  processors  to  be  used for binding can be identified in terms of\n",
      "       topological groupings - e.g., binding to  an  l3cache  will  bind  each\n",
      "       process  to all processors within the scope of a single L3 cache within\n",
      "       their assigned location. Thus, if a process is assigned by  the  mapper\n",
      "       to  a  certain socket, then a —bind-to l3cache directive will cause the\n",
      "       process to be bound to the processors that  share  a  single  L3  cache\n",
      "       within that socket.\n",
      "\n",
      "       Alternatively,  processes  can be assigned to processors based on their\n",
      "       local rank on a node using the --bind-to cpu-list:ordered  option  with\n",
      "       an associated --cpu-list \"0,2,5\". In this example, the first process on\n",
      "       a node will be bound to cpu 0, the second process on the node  will  be\n",
      "       bound  to cpu 2, and the third process on the node will be bound to cpu\n",
      "       5. --bind-to will also accept cpulist:ortered  as  a  synonym  to  cpu-\n",
      "       list:ordered.  Note that an error will result if more processes are as‐\n",
      "       signed to a node than cpus are provided.\n",
      "\n",
      "       To help balance loads, the binding directive uses a round-robin  method\n",
      "       when binding to levels lower than used in the mapper. For example, con‐\n",
      "       sider the case where a job is mapped to  the  socket  level,  and  then\n",
      "       bound  to  core.  Each  socket will have multiple cores, so if multiple\n",
      "       processes are mapped to a given socket, the binding algorithm will  as‐\n",
      "       sign each process located to a socket to a unique core in a round-robin\n",
      "       manner.\n",
      "\n",
      "       Alternatively, processes mapped by l2cache and  then  bound  to  socket\n",
      "       will simply be bound to all the processors in the socket where they are\n",
      "       located. In this manner, users can exert detailed control over relative\n",
      "       MCW rank location and binding.\n",
      "\n",
      "       Finally, --report-bindings can be used to report bindings.\n",
      "\n",
      "       As  an  example,  consider a node with two processor sockets, each com‐\n",
      "       prising four cores.  We run mpirun with -np 4 --report-bindings and the\n",
      "       following additional options:\n",
      "\n",
      "        % mpirun ... --map-by core --bind-to core\n",
      "        [...] ... binding child [...,0] to cpus 0001\n",
      "        [...] ... binding child [...,1] to cpus 0002\n",
      "        [...] ... binding child [...,2] to cpus 0004\n",
      "        [...] ... binding child [...,3] to cpus 0008\n",
      "\n",
      "        % mpirun ... --map-by socket --bind-to socket\n",
      "        [...] ... binding child [...,0] to socket 0 cpus 000f\n",
      "        [...] ... binding child [...,1] to socket 1 cpus 00f0\n",
      "        [...] ... binding child [...,2] to socket 0 cpus 000f\n",
      "        [...] ... binding child [...,3] to socket 1 cpus 00f0\n",
      "\n",
      "        % mpirun ... --map-by core:PE=2 --bind-to core\n",
      "        [...] ... binding child [...,0] to cpus 0003\n",
      "        [...] ... binding child [...,1] to cpus 000c\n",
      "        [...] ... binding child [...,2] to cpus 0030\n",
      "        [...] ... binding child [...,3] to cpus 00c0\n",
      "\n",
      "        % mpirun ... --bind-to none\n",
      "\n",
      "       Here,  --report-bindings  shows  the binding of each process as a mask.\n",
      "       In the first case, the processes bind to successive cores as  indicated\n",
      "       by the masks 0001, 0002, 0004, and 0008.  In the second case, processes\n",
      "       bind to all cores on successive sockets as indicated by the masks  000f\n",
      "       and  00f0.   The  processes  cycle  through  the processor sockets in a\n",
      "       round-robin fashion as many times as are needed.  In  the  third  case,\n",
      "       the  masks  show  us  that 2 cores have been bound per process.  In the\n",
      "       fourth case, binding is turned off and no bindings are reported.\n",
      "\n",
      "       Open MPI's support for process binding depends on the underlying  oper‐\n",
      "       ating  system.   Therefore,  certain process binding options may not be\n",
      "       available on every system.\n",
      "\n",
      "       Process binding can also be set with MCA parameters.   Their  usage  is\n",
      "       less  convenient  than  that of mpirun options.  On the other hand, MCA\n",
      "       parameters can be set not only on the mpirun command line, but alterna‐\n",
      "       tively in a system or user mca-params.conf file or as environment vari‐\n",
      "       ables, as described in the MCA section below.  Some examples include:\n",
      "\n",
      "           mpirun option          MCA parameter key         value\n",
      "\n",
      "         --map-by core          rmaps_base_mapping_policy   core\n",
      "         --map-by socket        rmaps_base_mapping_policy   socket\n",
      "         --rank-by core         rmaps_base_ranking_policy   core\n",
      "         --bind-to core         hwloc_base_binding_policy   core\n",
      "         --bind-to socket       hwloc_base_binding_policy   socket\n",
      "         --bind-to none         hwloc_base_binding_policy   none\n",
      "\n",
      "   Rankfiles\n",
      "       Rankfiles are text files that specify detailed  information  about  how\n",
      "       individual  processes  should  be mapped to nodes, and to which proces‐\n",
      "       sor(s) they should be bound.  Each line of a rankfile specifies the lo‐\n",
      "       cation  of one process (for MPI jobs, the process' \"rank\" refers to its\n",
      "       rank in MPI_COMM_WORLD).  The general form of each line in the rankfile\n",
      "       is:\n",
      "\n",
      "           rank <N>=<hostname> slot=<slot list>\n",
      "\n",
      "       For example:\n",
      "\n",
      "           $ cat myrankfile\n",
      "           rank 0=aa slot=1:0-2\n",
      "           rank 1=bb slot=0:0,1\n",
      "           rank 2=cc slot=1-2\n",
      "           $ mpirun -H aa,bb,cc,dd -rf myrankfile ./a.out\n",
      "\n",
      "       Means that\n",
      "\n",
      "         Rank 0 runs on node aa, bound to logical socket 1, cores 0-2.\n",
      "         Rank 1 runs on node bb, bound to logical socket 0, cores 0 and 1.\n",
      "         Rank 2 runs on node cc, bound to logical cores 1 and 2.\n",
      "\n",
      "       Rankfiles can alternatively be used to specify physical processor loca‐\n",
      "       tions. In this case, the syntax is somewhat different. Sockets  are  no\n",
      "       longer  recognized, and the slot number given must be the number of the\n",
      "       physical PU as most OS's do not assign a unique physical identifier  to\n",
      "       each core in the node. Thus, a proper physical rankfile looks something\n",
      "       like the following:\n",
      "\n",
      "           $ cat myphysicalrankfile\n",
      "           rank 0=aa slot=1\n",
      "           rank 1=bb slot=8\n",
      "           rank 2=cc slot=6\n",
      "\n",
      "       This means that\n",
      "\n",
      "         Rank 0 will run on node aa, bound to the core that contains  physical\n",
      "       PU 1\n",
      "         Rank  1 will run on node bb, bound to the core that contains physical\n",
      "       PU 8\n",
      "         Rank 2 will run on node cc, bound to the core that contains  physical\n",
      "       PU 6\n",
      "\n",
      "       Rankfiles  are  treated  as  logical  by default, and the MCA parameter\n",
      "       rmaps_rank_file_physical must be set to 1 to indicate that the rankfile\n",
      "       is to be considered as physical.\n",
      "\n",
      "       The hostnames listed above are \"absolute,\" meaning that actual resolve‐\n",
      "       able hostnames are specified.  However, hostnames can also be specified\n",
      "       as \"relative,\" meaning that they are specified in relation to an exter‐\n",
      "       nally-specified list of hostnames (e.g., by mpirun's --host argument, a\n",
      "       hostfile, or a job scheduler).\n",
      "\n",
      "       The  \"relative\" specification is of the form \"+n<X>\", where X is an in‐\n",
      "       teger specifying the Xth hostname in the set  of  all  available  host‐\n",
      "       names, indexed from 0.  For example:\n",
      "\n",
      "           $ cat myrankfile\n",
      "           rank 0=+n0 slot=1:0-2\n",
      "           rank 1=+n1 slot=0:0,1\n",
      "           rank 2=+n2 slot=1-2\n",
      "           $ mpirun -H aa,bb,cc,dd -rf myrankfile ./a.out\n",
      "\n",
      "       Starting  with  Open  MPI  v1.7,  all socket/core slot locations are be\n",
      "       specified as logical indexes (the Open MPI v1.6  series  used  physical\n",
      "       indexes).  You can use tools such as HWLOC's \"lstopo\" to find the logi‐\n",
      "       cal indexes of socket and cores.\n",
      "\n",
      "   Application Context or Executable Program?\n",
      "       To distinguish the two different forms, mpirun  looks  on  the  command\n",
      "       line  for --app option.  If it is specified, then the file named on the\n",
      "       command line is assumed to be an application context.   If  it  is  not\n",
      "       specified, then the file is assumed to be an executable program.\n",
      "\n",
      "   Locating Files\n",
      "       If  no relative or absolute path is specified for a file, Open MPI will\n",
      "       first look for files by searching  the  directories  specified  by  the\n",
      "       --path  option.  If there is no --path option set or if the file is not\n",
      "       found at the --path location, then Open MPI will search the user's PATH\n",
      "       environment variable as defined on the source node(s).\n",
      "\n",
      "       If  a  relative directory is specified, it must be relative to the ini‐\n",
      "       tial working directory determined by the specific starter used. For ex‐\n",
      "       ample  when  using  the  rsh  or ssh starters, the initial directory is\n",
      "       $HOME by default. Other starters may set the initial directory  to  the\n",
      "       current working directory from the invocation of mpirun.\n",
      "\n",
      "   Current Working Directory\n",
      "       The  -wdir  mpirun  option  (and  its  synonym, -wd) allows the user to\n",
      "       change to an arbitrary directory before the program is invoked.  It can\n",
      "       also  be  used in application context files to specify working directo‐\n",
      "       ries on specific nodes and/or for specific applications.\n",
      "\n",
      "       If the -wdir option appears both in a context file and on  the  command\n",
      "       line, the context file directory will override the command line value.\n",
      "\n",
      "       If  the  -wdir  option is specified, Open MPI will attempt to change to\n",
      "       the specified directory on all of the  remote  nodes.  If  this  fails,\n",
      "       mpirun will abort.\n",
      "\n",
      "       If  the -wdir option is not specified, Open MPI will send the directory\n",
      "       name where mpirun was invoked to each of the remote nodes.  The  remote\n",
      "       nodes  will  try to change to that directory. If they are unable (e.g.,\n",
      "       if the directory does not exist on that node), then Open MPI  will  use\n",
      "       the default directory determined by the starter.\n",
      "\n",
      "       All  directory changing occurs before the user's program is invoked; it\n",
      "       does not wait until MPI_INIT is called.\n",
      "\n",
      "   Standard I/O\n",
      "       Open MPI directs UNIX standard input to /dev/null on all processes  ex‐\n",
      "       cept  the  MPI_COMM_WORLD  rank  0  process.  The MPI_COMM_WORLD rank 0\n",
      "       process inherits standard input from mpirun.  Note: The node  that  in‐\n",
      "       voked  mpirun need not be the same as the node where the MPI_COMM_WORLD\n",
      "       rank 0 process resides. Open MPI handles the  redirection  of  mpirun's\n",
      "       standard input to the rank 0 process.\n",
      "\n",
      "       Open  MPI  directs  UNIX standard output and error from remote nodes to\n",
      "       the node that invoked mpirun and prints it on the standard output/error\n",
      "       of mpirun.  Local processes inherit the standard output/error of mpirun\n",
      "       and transfer to it directly.\n",
      "\n",
      "       Thus it is possible to redirect standard I/O for Open MPI  applications\n",
      "       by using the typical shell redirection procedure on mpirun.\n",
      "\n",
      "             % mpirun -np 2 my_app < my_input > my_output\n",
      "\n",
      "       Note  that  in this example only the MPI_COMM_WORLD rank 0 process will\n",
      "       receive the stream from my_input on stdin.  The stdin on all the  other\n",
      "       nodes  will  be  tied to /dev/null.  However, the stdout from all nodes\n",
      "       will be collected into the my_output file.\n",
      "\n",
      "   Signal Propagation\n",
      "       When orterun receives a SIGTERM and SIGINT, it will attempt to kill the\n",
      "       entire  job  by  sending  all processes in the job a SIGTERM, waiting a\n",
      "       small number of seconds, then  sending  all  processes  in  the  job  a\n",
      "       SIGKILL.\n",
      "\n",
      "       SIGUSR1  and  SIGUSR2 signals received by orterun are propagated to all\n",
      "       processes in the job.\n",
      "\n",
      "       A SIGTSTOP signal to mpirun will cause a SIGSTOP signal to be  sent  to\n",
      "       all  of the programs started by mpirun and likewise a SIGCONT signal to\n",
      "       mpirun will cause a SIGCONT sent.\n",
      "\n",
      "       Other signals are not currently propagated by orterun.\n",
      "\n",
      "   Process Termination / Signal Handling\n",
      "       During the run of an MPI application, if any  process  dies  abnormally\n",
      "       (either exiting before invoking MPI_FINALIZE, or dying as the result of\n",
      "       a signal), mpirun will print out an error message and kill the rest  of\n",
      "       the MPI application.\n",
      "\n",
      "       User  signal handlers should probably avoid trying to cleanup MPI state\n",
      "       (Open MPI is currently not  async-signal-safe;  see  MPI_Init_thread(3)\n",
      "       for details about MPI_THREAD_MULTIPLE and thread safety).  For example,\n",
      "       if a segmentation fault occurs in MPI_SEND (perhaps because a bad  buf‐\n",
      "       fer  was  passed in) and a user signal handler is invoked, if this user\n",
      "       handler attempts to invoke MPI_FINALIZE, Bad Things could happen  since\n",
      "       Open  MPI  was  already \"in\" MPI when the error occurred.  Since mpirun\n",
      "       will notice that the process died due to a signal, it is  probably  not\n",
      "       necessary (and safest) for the user to only clean up non-MPI state.\n",
      "\n",
      "   Process Environment\n",
      "       Processes  in  the  MPI  application inherit their environment from the\n",
      "       Open RTE daemon upon the node on which they are running.  The  environ‐\n",
      "       ment  is  typically  inherited from the user's shell.  On remote nodes,\n",
      "       the exact environment is determined by the boot MCA module  used.   The\n",
      "       rsh  launch module, for example, uses either rsh/ssh to launch the Open\n",
      "       RTE daemon on remote nodes, and typically executes one or more  of  the\n",
      "       user's  shell-setup  files  before launching the Open RTE daemon.  When\n",
      "       running  dynamically  linked  applications  which  require  the  LD_LI‐\n",
      "       BRARY_PATH environment variable to be set, care must be taken to ensure\n",
      "       that it is correctly set when booting Open MPI.\n",
      "\n",
      "       See the \"Remote Execution\" section for more details.\n",
      "\n",
      "   Remote Execution\n",
      "       Open MPI requires that the PATH environment variable be set to find ex‐\n",
      "       ecutables  on remote nodes (this is typically only necessary in rsh- or\n",
      "       ssh-based environments -- batch/scheduled environments  typically  copy\n",
      "       the current environment to the execution of remote jobs, so if the cur‐\n",
      "       rent environment has PATH and/or LD_LIBRARY_PATH set properly, the  re‐\n",
      "       mote  nodes  will also have it set properly).  If Open MPI was compiled\n",
      "       with shared library support, it may  also  be  necessary  to  have  the\n",
      "       LD_LIBRARY_PATH environment variable set on remote nodes as well (espe‐\n",
      "       cially to find the shared libraries required to run user  MPI  applica‐\n",
      "       tions).\n",
      "\n",
      "       However,  it  is not always desirable or possible to edit shell startup\n",
      "       files to set PATH and/or LD_LIBRARY_PATH.  The --prefix option is  pro‐\n",
      "       vided for some simple configurations where this is not possible.\n",
      "\n",
      "       The  --prefix option takes a single argument: the base directory on the\n",
      "       remote node where Open MPI is installed.  Open MPI will use this direc‐\n",
      "       tory  to  set  the remote PATH and LD_LIBRARY_PATH before executing any\n",
      "       Open MPI or user applications.  This allows running Open MPI jobs with‐\n",
      "       out  having  pre-configured  the PATH and LD_LIBRARY_PATH on the remote\n",
      "       nodes.\n",
      "\n",
      "       Open MPI adds the basename of the current node's \"bindir\"  (the  direc‐\n",
      "       tory where Open MPI's executables are installed) to the prefix and uses\n",
      "       that to set the PATH on the remote node.  Similarly, Open MPI adds  the\n",
      "       basename of the current node's \"libdir\" (the directory where Open MPI's\n",
      "       libraries are installed) to the prefix and uses that to set the  LD_LI‐\n",
      "       BRARY_PATH on the remote node.  For example:\n",
      "\n",
      "       Local bindir:  /local/node/directory/bin\n",
      "\n",
      "       Local libdir:  /local/node/directory/lib64\n",
      "\n",
      "       If the following command line is used:\n",
      "\n",
      "           % mpirun --prefix /remote/node/directory\n",
      "\n",
      "       Open  MPI  will  add \"/remote/node/directory/bin\" to the PATH and \"/re‐\n",
      "       mote/node/directory/lib64\" to the LD_LIBRARY_PATH on  the  remote  node\n",
      "       before attempting to execute anything.\n",
      "\n",
      "       The  --prefix option is not sufficient if the installation paths on the\n",
      "       remote node are different than the local node (e.g., if \"/lib\" is  used\n",
      "       on  the local node, but \"/lib64\" is used on the remote node), or if the\n",
      "       installation paths are something other than a subdirectory under a com‐\n",
      "       mon prefix.\n",
      "\n",
      "       Note  that  executing  mpirun via an absolute pathname is equivalent to\n",
      "       specifying --prefix without the last subdirectory in the absolute path‐\n",
      "       name to mpirun.  For example:\n",
      "\n",
      "           % /usr/local/bin/mpirun ...\n",
      "\n",
      "       is equivalent to\n",
      "\n",
      "           % mpirun --prefix /usr/local\n",
      "\n",
      "   Exported Environment Variables\n",
      "       All  environment variables that are named in the form OMPI_* will auto‐\n",
      "       matically be exported to new processes on the local and  remote  nodes.\n",
      "       Environmental parameters can also be set/forwarded to the new processes\n",
      "       using the MCA parameter mca_base_env_list. The -x option to mpirun  has\n",
      "       been deprecated, but the syntax of the MCA param follows that prior ex‐\n",
      "       ample. While the syntax of the -x option and MCA param allows the defi‐\n",
      "       nition  of  new  variables,  note that the parser for these options are\n",
      "       currently not very sophisticated - it does not even  understand  quoted\n",
      "       values.   Users are advised to set variables in the environment and use\n",
      "       the option to export them; not to define them.\n",
      "\n",
      "   Setting MCA Parameters\n",
      "       The -mca switch allows the passing of parameters to various MCA  (Modu‐\n",
      "       lar Component Architecture) modules.  MCA modules have direct impact on\n",
      "       MPI programs because they allow tunable parameters to  be  set  at  run\n",
      "       time (such as which BTL communication device driver to use, what param‐\n",
      "       eters to pass to that BTL, etc.).\n",
      "\n",
      "       The -mca switch takes two arguments: <key> and <value>.  The <key>  ar‐\n",
      "       gument  generally  specifies  which  MCA module will receive the value.\n",
      "       For example, the <key> \"btl\" is used to select which BTL to be used for\n",
      "       transporting  MPI  messages.  The <value> argument is the value that is\n",
      "       passed.  For example:\n",
      "\n",
      "       mpirun -mca btl tcp,self -np 1 foo\n",
      "           Tells Open MPI to use the \"tcp\" and \"self\" BTLs, and to run a  sin‐\n",
      "           gle copy of \"foo\" an allocated node.\n",
      "\n",
      "       mpirun -mca btl self -np 1 foo\n",
      "           Tells  Open  MPI to use the \"self\" BTL, and to run a single copy of\n",
      "           \"foo\" an allocated node.\n",
      "\n",
      "       The -mca switch can be used multiple times to specify  different  <key>\n",
      "       and/or  <value>  arguments.   If  the same <key> is specified more than\n",
      "       once, the <value>s are concatenated with a comma (\",\") separating them.\n",
      "\n",
      "       Note that the -mca switch is simply a shortcut for setting  environment\n",
      "       variables.   The same effect may be accomplished by setting correspond‐\n",
      "       ing environment variables before running mpirun.  The form of the envi‐\n",
      "       ronment variables that Open MPI sets is:\n",
      "\n",
      "             OMPI_MCA_<key>=<value>\n",
      "\n",
      "       Thus,  the  -mca  switch overrides any previously set environment vari‐\n",
      "       ables.  The -mca settings similarly override MCA parameters set in  the\n",
      "       $OPAL_PREFIX/etc/openmpi-mca-params.conf     or     $HOME/.openmpi/mca-\n",
      "       params.conf file.\n",
      "\n",
      "       Unknown <key> arguments are still set as environment variable  --  they\n",
      "       are  not  checked  (by  mpirun)  for correctness.  Illegal or incorrect\n",
      "       <value> arguments may or may not be reported -- it depends on the  spe‐\n",
      "       cific MCA module.\n",
      "\n",
      "       To find the available component types under the MCA architecture, or to\n",
      "       find the  available  parameters  for  a  specific  component,  use  the\n",
      "       ompi_info command.  See the ompi_info(1) man page for detailed informa‐\n",
      "       tion on the command.\n",
      "\n",
      "   Setting MCA parameters and environment variables from file.\n",
      "       The -tune  command  line  option  and  its  synonym  -mca  mca_base_en‐\n",
      "       var_file_prefix  allows  a  user  to set mca parameters and environment\n",
      "       variables with the syntax described below.  This option requires a sin‐\n",
      "       gle file or list of files separated by \",\" to follow.\n",
      "\n",
      "       A  valid  line  in  the  file may contain zero or many \"-x\", \"-mca\", or\n",
      "       “--mca” arguments.  The following patterns are supported: -mca var  val\n",
      "       -mca var \"val\" -x var=val -x var.  If any argument is duplicated in the\n",
      "       file, the last value read will be used.\n",
      "\n",
      "       MCA parameters and environment  specified  on  the  command  line  have\n",
      "       higher precedence than variables specified in the file.\n",
      "\n",
      "   Running as root\n",
      "       The Open MPI team strongly advises against executing mpirun as the root\n",
      "       user.  MPI applications should be run as regular (non-root) users.\n",
      "\n",
      "       Reflecting this advice, mpirun will refuse to run as root  by  default.\n",
      "       To override this default, you can add the --allow-run-as-root option to\n",
      "       the mpirun command line, or you can set  the  environmental  parameters\n",
      "       OMPI_ALLOW_RUN_AS_ROOT=1  and  OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1.   Note\n",
      "       that it takes setting two environment variables to effect the same  be‐\n",
      "       havior  as  --allow-run-as-root  in order to stress the Open MPI team's\n",
      "       strong advice against running as the root user.  After extended discus‐\n",
      "       sions  with  communities  who use containers (where running as the root\n",
      "       user is the default), there was a persistent desire to be able  to  en‐\n",
      "       able root execution of mpirun via an environmental control (vs. the ex‐\n",
      "       isting --allow-run-as-root command line parameter).  The compromise  of\n",
      "       using  two  environment variables was reached: it allows root execution\n",
      "       via an environmental control, but it conveys the Open MPI team's strong\n",
      "       recomendation against this behavior.\n",
      "\n",
      "   Exit status\n",
      "       There  is  no  standard  definition for what mpirun should return as an\n",
      "       exit status. After considerable discussion, we settled on the following\n",
      "       method for assigning the mpirun exit status (note: in the following de‐\n",
      "       scription, the \"primary\" job is  the  initial  application  started  by\n",
      "       mpirun  -  all  jobs  that are spawned by that job are designated \"sec‐\n",
      "       ondary\" jobs):\n",
      "\n",
      "       • if all processes in the primary job normally terminate with exit sta‐\n",
      "         tus 0, we return 0\n",
      "\n",
      "       • if  one  or more processes in the primary job normally terminate with\n",
      "         non-zero exit status, we return the exit status of the  process  with\n",
      "         the lowest MPI_COMM_WORLD rank to have a non-zero status\n",
      "\n",
      "       • if all processes in the primary job normally terminate with exit sta‐\n",
      "         tus 0, and one or more processes in a secondary job  normally  termi‐\n",
      "         nate  with non-zero exit status, we (a) return the exit status of the\n",
      "         process with the lowest MPI_COMM_WORLD rank in the  lowest  jobid  to\n",
      "         have a non-zero status, and (b) output a message summarizing the exit\n",
      "         status of the primary and all secondary jobs.\n",
      "\n",
      "       • if the cmd line option --report-child-jobs-separately is set, we will\n",
      "         return  -only-  the exit status of the primary job. Any non-zero exit\n",
      "         status in secondary jobs will be reported solely in a  summary  print\n",
      "         statement.\n",
      "\n",
      "       By  default, OMPI records and notes that MPI processes exited with non-\n",
      "       zero termination status.  This is generally not considered an \"abnormal\n",
      "       termination\" - i.e., OMPI will not abort an MPI job if one or more pro‐\n",
      "       cesses return a non-zero status. Instead, the default  behavior  simply\n",
      "       reports  the  number of processes terminating with non-zero status upon\n",
      "       completion of the job.\n",
      "\n",
      "       However, in some cases it can be desirable to have the job  abort  when\n",
      "       any process terminates with non-zero status. For example, a non-MPI job\n",
      "       might detect a bad result from a calculation and  want  to  abort,  but\n",
      "       doesn't want to generate a core file. Or an MPI job might continue past\n",
      "       a call to MPI_Finalize, but indicate that all  processes  should  abort\n",
      "       due to some post-MPI result.\n",
      "\n",
      "       It  is  not anticipated that this situation will occur frequently. How‐\n",
      "       ever, in the interest of serving the broader community, OMPI now has  a\n",
      "       means  for  allowing  users  to  direct  that  jobs be aborted upon any\n",
      "       process  exiting  with  non-zero  status.  Setting  the  MCA  parameter\n",
      "       \"orte_abort_on_non_zero_status\"  to 1 will cause OMPI to abort all pro‐\n",
      "       cesses once any process\n",
      "        exits with non-zero status.\n",
      "\n",
      "       Terminations caused in this manner will be reported on the  console  as\n",
      "       an \"abnormal termination\", with the first process to so exit identified\n",
      "       along with its exit status.\n",
      "\n",
      "EXAMPLES\n",
      "       Be sure also to see the examples throughout the sections above.\n",
      "\n",
      "       mpirun -np 4 -mca btl ib,tcp,self prog1\n",
      "           Run 4 copies of prog1 using the \"ib\", \"tcp\", and \"self\"  BTL's  for\n",
      "           the transport of MPI messages.\n",
      "\n",
      "       mpirun -np 4 -mca btl tcp,sm,self\n",
      "           --mca btl_tcp_if_include eth0 prog1\n",
      "           Run 4 copies of prog1 using the \"tcp\", \"sm\" and \"self\" BTLs for the\n",
      "           transport of MPI messages, with TCP using only the  eth0  interface\n",
      "           to  communicate.   Note that other BTLs have similar if_include MCA\n",
      "           parameters.\n",
      "\n",
      "RETURN VALUE\n",
      "       mpirun returns 0 if all processes started by mpirun exit after  calling\n",
      "       MPI_FINALIZE.   A  non-zero  value is returned if an internal error oc‐\n",
      "       curred in mpirun, or  one  or  more  processes  exited  before  calling\n",
      "       MPI_FINALIZE.  If an internal error occurred in mpirun, the correspond‐\n",
      "       ing error code is returned.  In the event that one  or  more  processes\n",
      "       exit   before   calling   MPI_FINALIZE,   the   return   value  of  the\n",
      "       MPI_COMM_WORLD rank of the process that mpirun first notices  died  be‐\n",
      "       fore  calling  MPI_FINALIZE  will  be returned.  Note that, in general,\n",
      "       this will be the first process that died but is not  guaranteed  to  be\n",
      "       so.\n",
      "\n",
      "       If  the  --timeout  command line option is used and the timeout expires\n",
      "       before the job completes (thereby  forcing  mpirun  to  kill  the  job)\n",
      "       mpirun  will return an exit status equivalent to the value of ETIMEDOUT\n",
      "       (which is typically 110 on Linux and OS X systems).\n",
      "\n",
      "SEE ALSO\n",
      "       MPI_Init_thread(3)\n",
      "\n",
      "4.0.3                            Mar 03, 2020                        MPIRUN(1)\n"
     ]
    }
   ],
   "source": [
    "!man mpiexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No manual entry for MPI_Send\n"
     ]
    }
   ],
   "source": [
    "!man MPI_Send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No manual entry for MPI_Init_thread in section 3\n"
     ]
    }
   ],
   "source": [
    "!man 3 MPI_Init_thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例: 梯形法数值积分\n",
    "* [MPI/aipp_3_2.c](./MPI/aipp_3_2.c)\n",
    "\n",
    "$$\n",
    "\\int_{a}^{b} f(x) dx \\\\\n",
    "\n",
    "h = \\frac{b-a}{n} \\\\\n",
    "\n",
    "x_{0} = a, x_{1} = a + h, x_{2} = a + 2h, \\cdots, x_{n-1} = a + (n-1)h, x_{n} = b \\\\\n",
    "\n",
    "Area = h [f(x_{0}) / 2 + f(x_{1}) + f(x_{2}) + \\cdots + f(x_{n-1}) + f(x_{n})/2]\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.000004299928621)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0.0, 3.0, 1024)\n",
    "y = x * x\n",
    "np.trapezoid(y, x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With n=1024 trapezoids, estimated integral from 0.000000 to 3.000000 = 9.000004291534424e+00\n"
     ]
    }
   ],
   "source": [
    "filename = 'aipp_3_2'\n",
    "!mpicc -g -Wall -o {filename} {filename}.c\n",
    "!mpiexec -n 8 {filename}\n",
    "!rm -f {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [MPI/aipp_3_4.c](./MPI/aipp_3_4.c): 每个进程均输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proc 0 of 8 > Does anyone have a toothpick?\n",
      "Proc 5 of 8 > Does anyone have a toothpick?\n",
      "Proc 7 of 8 > Does anyone have a toothpick?\n",
      "Proc 1 of 8 > Does anyone have a toothpick?\n",
      "Proc 3 of 8 > Does anyone have a toothpick?\n",
      "Proc 4 of 8 > Does anyone have a toothpick?\n",
      "Proc 6 of 8 > Does anyone have a toothpick?\n",
      "Proc 2 of 8 > Does anyone have a toothpick?\n"
     ]
    }
   ],
   "source": [
    "filename = 'aipp_3_4'\n",
    "!mpicc -g -Wall -o {filename} {filename}.c\n",
    "!mpiexec -n 8 {filename}\n",
    "!rm -f {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [MPI/aipp_3_5.c](./MPI/aipp_3_5.c): 使用输入获取梯形法数值积分的a,b,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a, b, and n\n",
      "With n=1024 trapezoids, estimated integral from 0.000000 to 3.000000 = 9.000004291534424e+00\n"
     ]
    }
   ],
   "source": [
    "filename = 'aipp_3_5'\n",
    "!mpicc -g -Wall -o {filename} {filename}.c\n",
    "# 使用管道模拟输入\n",
    "!echo  0.0 3.0 1024 | mpiexec -n 8 {filename}\n",
    "!rm -f {filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集体通信\n",
    "Collective communication\n",
    "\n",
    "* tree-structured global sum\n",
    "* butterfly-structured global sum\n",
    "* tree-structured broadcast\n",
    "\n",
    "```c\n",
    "MPI_Reduce()\n",
    "MPI_Allreduce()\n",
    "MPI_Bcast()\n",
    "\n",
    "MPI_Scatter()\n",
    "MPI_Gather()\n",
    "MPI_Allgather()\n",
    "\n",
    "MPI_Barrier()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 衍生的数据类型\n",
    "derived datatype\n",
    "\n",
    "```c\n",
    "MPI_Type_create_struct()\n",
    "MPI_Get_address()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timming\n",
    "\n",
    "```c\n",
    "// wall clock time\n",
    "MPI_Wtime()\n",
    "gettimeofday() // POSIX\n",
    "\n",
    "// CPU time\n",
    "clock() // C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例: 奇偶排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 83 86 77 15 93 35 86 92 49 21 \n",
      "1: 90 19 88 75 61 98 64 77 45 27 \n",
      "2: 46 85 68 40 25 40 72 76 1 64 \n",
      "3: 1 83 74 26 63 37 25 63 28 85 \n",
      "4: 75 65 10 72 76 32 20 49 73 81 \n",
      "5: 41 85 12 65 8 85 86 43 2 78 \n",
      "6: 77 99 99 71 25 43 86 97 0 53 \n",
      "7: 96 44 42 49 11 93 82 21 77 13 \n",
      "Global list: 0 1 1 2 8 10 11 12 13 15 19 20 21 21 25 25 25 26 27 28 32 35 37 40 40 41 42 43 43 44 45 46 49 49 49 53 61 63 63 64 64 65 65 68 71 72 72 73 74 75 75 76 76 77 77 77 77 78 81 82 83 83 85 85 85 85 86 86 86 86 88 90 92 93 93 96 97 98 99 99 \n"
     ]
    }
   ],
   "source": [
    "filename = 'odd_even_sort'\n",
    "!mpicc -g -Wall -o {filename} {filename}.c\n",
    "!mpiexec -n 8 {filename}\n",
    "!rm -f {filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 2, 8, 10, 11, 12, 13, 15, 19, 20, 21, 21, 25, 25, 25, 26, 27, 28, 32, 35, 37, 40, 40, 41, 42, 43, 43, 44, 45, 46, 49, 49, 49, 53, 61, 63, 63, 64, 64, 65, 65, 68, 71, 72, 72, 73, 74, 75, 75, 76, 76, 77, 77, 77, 77, 78, 81, 82, 83, 83, 85, 85, 85, 85, 86, 86, 86, 86, 88, 90, 92, 93, 93, 96, 97, 98, 99, 99]\n"
     ]
    }
   ],
   "source": [
    "a = [int(v) for v in \"0 1 1 2 8 10 11 12 13 15 19 20 21 21 25 25 25 26 27 28 32 35 37 40 40 41 42 43 43 44 45 46 49 49 49 53 61 63 63 64 64 65 65 68 71 72 72 73 74 75 75 76 76 77 77 77 77 78 81 82 83 83 85 85 85 85 86 86 86 86 88 90 92 93 93 96 97 98 99 99\".split(\" \")]\n",
    "print(a)\n",
    "\n",
    "for i in range(len(a)-1):\n",
    "  # print(a[i], a[i+1])\n",
    "  if a[i] > a[i+1]:\n",
    "    print(a[i], a[i+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

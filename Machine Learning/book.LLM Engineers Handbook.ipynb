{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7c1d0b",
   "metadata": {},
   "source": [
    "# LLM Engineer's Handbook\n",
    "- Iusztin, Paul / Labonne, Maxime. **LLM Engineer's Handbook: Master the art of engineering large language models from concept to production**. 2024. Packt. - [code](https://github.com/PacktPublishing/LLM-Engineers-Handbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bda83",
   "metadata": {},
   "source": [
    "# Running example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0c90a",
   "metadata": {},
   "source": [
    "LLM Twin: an AI character that incorporates your writing style, voice, and personality into an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1505472",
   "metadata": {},
   "source": [
    "# Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1a32b",
   "metadata": {},
   "source": [
    "- FTI: feature/training/inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c24b0",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8c7f5",
   "metadata": {},
   "source": [
    "Figure 1.6: LLM Twin high-level architecture\n",
    "<img src=\"https://static.packt-cdn.com/products/9781836200079/graphics/Images/B31105_01_06.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ceda85",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2b023",
   "metadata": {},
   "source": [
    "- Python 3.11.8\n",
    "- pyenv\n",
    "- Poetry: dependency and virtual environment management\n",
    "- Peo the Poet: a plugin on top of Poetry to manage and execute all the CLI commands required to interact with the project.\n",
    "- Docker: 27.1.1+\n",
    "- Huggine Face: model registry\n",
    "- ZenML: orchestrator, artifacts, and metadata\n",
    "- Comet ML: experiment tracker\n",
    "  - Opik: prompt monitoring \n",
    "- MongoDB: NoSQL database\n",
    "- Qdrant: vector database\n",
    "- AWS\n",
    "  - S3: object storage\n",
    "  - ECR: container registry\n",
    "  - SagaMaker: compute for training and inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c009cf3",
   "metadata": {},
   "source": [
    "# Data collection pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147684f",
   "metadata": {},
   "source": [
    "ETL\n",
    "- extract: Medium, Substack, GitHub - crawler\n",
    "  - Scrapy, Crawl4AI\n",
    "- transform\n",
    "- load\n",
    "  - ORM: SQLAlchemy, SQLModel(FastAPI)\n",
    "  - ODM\n",
    "    - Pydantic: generic module\n",
    "\n",
    "Figure 3.1: LLM Twin’s data collection ETL pipeline architecture\n",
    "\n",
    "\n",
    "data: `data/data_warehouse_raw_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bbc97",
   "metadata": {},
   "source": [
    "# Feature pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c906562",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd4a5a",
   "metadata": {},
   "source": [
    "- Retrieval: search for relevant data\n",
    "- Augmented: add the data as context to the prompt\n",
    "- Generation: use the augmented prompt with an LLM for generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859945f",
   "metadata": {},
   "source": [
    "system models:\n",
    "- ingestion pipeline: a batch or streaming pipeplie used to populate the vector DB\n",
    "- retrieval pipeline: a module that queries the vector DB and retrieves relevant entries to the user's input\n",
    "- generation pipeline: the layer that uses the retrieved data to augment the prompt and an LLM to generate answers\n",
    "\n",
    "Figure 4.1: Vanilla RAG architecture\n",
    "\n",
    "<img src=\"./images/Vanilla RAG architecture.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3d854",
   "metadata": {},
   "source": [
    "RAG ingestion pipeline:\n",
    "- data extraction module: 从多个源(例如数据库, API或网页)收集必须奥的数据.\n",
    "- cleaning layer: 标准化提取的数据, 移除不需要的特征.\n",
    "- chunking module: 拆分清洗后的文档, 不超过嵌入模型的最大输入大小, 按语义相关的拆分.\n",
    "- embedding component: 使用chunk的内容(文本, 图片, 语音等)输入一个嵌入模型, 映射到带有语义值的稠密向量上\n",
    "- loading module: 使用嵌入的chunk和元数据文档, 嵌入作为查询相似chunk的索引, 元数据用于访问用以增强提示的信息."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb22b665",
   "metadata": {},
   "source": [
    "RAG retrieval pipeline:\n",
    "- 接收用户输入: 文本, 图片, 语音等\n",
    "- 内嵌\n",
    "- 在向量数据库中查询与用户输入向量相似的向量."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff91ec4",
   "metadata": {},
   "source": [
    "RAG generation pipeline:\n",
    "- 使用用户输入, 检索出的数据, 传入LLM, 生成答案.\n",
    "- 最终的提示依赖于系统, 用户查询生成的提示模板(prompt template)和检索出的上下文."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a88afc6",
   "metadata": {},
   "source": [
    "Embedding\n",
    "- early text data: Word2Vec, Glove\n",
    "- encoder-only transformers: BERT, RoBERTa\n",
    "- Python packages:\n",
    "  - Sentence Transformers\n",
    "  - Hugging Face's transformer\n",
    "- images: CNN(Convolutional Neural Networks) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7504dc",
   "metadata": {},
   "source": [
    "vector DB\n",
    "- algorithms: ANN(approximate nearest neighbor) to find the close neighbors\n",
    "- workflow\n",
    "  - indexing vectors\n",
    "  - querying for similarity\n",
    "  - post-processing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e09560",
   "metadata": {},
   "source": [
    "batch v.s. streaming\n",
    "\n",
    "Figure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\n",
    "\n",
    "<img src=\"./images/Batch vs Streaming tools.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5851d",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a72ad1",
   "metadata": {},
   "source": [
    "## SFT: Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1237d8",
   "metadata": {},
   "source": [
    "- [Differences between Pre-Training and Supervised Fine-Tuning (SFT)](https://techcommunity.microsoft.com/blog/machinelearningblog/differences-between-pre-training-and-supervised-fine-tuning-sft/4220673)\n",
    "- [Understanding and Using Supervised Fine-Tuning (SFT) for Language Models](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised) - 2023-08-11\n",
    "  - SFT: Supervised Fine-Tuning\n",
    "  - ELHF: rEinforcement Learning from Human Feedback\n",
    "\n",
    "<img src=\"./images/Different stages of training for an LLM.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3cc10",
   "metadata": {},
   "source": [
    "SFT: Supervised Fine-Tuning\n",
    "- 在初始的预训练阶段(LLM学习预测序列中下一个token)之后, SFT使用精心准备的指令(instruction)和相应的答案对, 来refine(提炼/完善)模型的能力.\n",
    "- 填充缝隙: 模型的通用语言理解, 模型的实际效用(utility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532402cf",
   "metadata": {},
   "source": [
    "创建指令数据集\n",
    "\n",
    "post-training data pipeline\n",
    "- data curation: (对数字信息的)综合处理，指对数字信息的选择，保存，管理等一系列处理行为，其目的是建立完善的查询机制和提供可靠的参考数据\n",
    "  - task-specific model: 从既有数据库中收集预期任务的示例, 或创建新的\n",
    "  - domain-specific model: 需要领域专家参与\n",
    "- data deduplication\n",
    "- data decontamination: 净化/去污\n",
    "- deta quality evaluation\n",
    "- data exploration\n",
    "- data generation\n",
    "- data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551013e",
   "metadata": {},
   "source": [
    "SFT技术\n",
    "- 需要更多的控制: know your data. 可定制性: the fine-tuned model is unique\n",
    "- 指令数据集格式: Alpaca, ShareGPT, OpenAI\n",
    "- 聊天模板chat templates: ChatML, Llama 3, Mistral, ...\n",
    "- 技术: full fine-tuning, LoRA, QLoRA\n",
    "- tranning parameters: hyperparameters\n",
    "  - learning rate\n",
    "  - batch size\n",
    "  - maximum sequence length, packing\n",
    "  - number of epochs\n",
    "  - optimizers\n",
    "  - weight decay\n",
    "  - gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a306c",
   "metadata": {},
   "source": [
    "## Preference alignment\n",
    "- 引入直接的人类或AI反馈到训练过程中.\n",
    "\n",
    "DPO: Direct Preference Optimization - [paper](https://arxiv.org/abs/2305.18290)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b19586",
   "metadata": {},
   "source": [
    "preference data: a collection of responses to a given instruction, ranked by humans or language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5d37c",
   "metadata": {},
   "source": [
    "## Evaluating LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee8969",
   "metadata": {},
   "source": [
    "LLM evaluation forms:\n",
    "- multiple-choice question answering\n",
    "- open-ended instructions\n",
    "- feedback from real users\n",
    "- general-purpose evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153d91a",
   "metadata": {},
   "source": [
    "model evaluation\n",
    "- without any prompt engineering, RAG pipeline\n",
    "\n",
    "models: \n",
    "- general-purpose\n",
    "- domain-specific\n",
    "- task-specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977ed2d",
   "metadata": {},
   "source": [
    "RAG evaluation\n",
    "- retrieval accuracy\n",
    "- integration quality\n",
    "- factuality and relevance\n",
    "\n",
    "Ragas: RAG Assessment\n",
    "\n",
    "ARES: an automated evaluation framework for RAG systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8493f",
   "metadata": {},
   "source": [
    "# Inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4f0c0",
   "metadata": {},
   "source": [
    "## Inference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35809ad4",
   "metadata": {},
   "source": [
    "inference process metrics:\n",
    "- latency: the time it takes to generate the first token\n",
    "- throughput: the number of tokens generated per second\n",
    "- memory footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be1d08b",
   "metadata": {},
   "source": [
    "optimization methods:\n",
    "- speculative decoding\n",
    "- model parallelism\n",
    "- weight quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e9984b",
   "metadata": {},
   "source": [
    "inference engines:\n",
    "- Text Generation Inference\n",
    "- vLLM\n",
    "- TensorRT-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf012b",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8882c37",
   "metadata": {},
   "source": [
    "<img src=\"./images/RAG inference pipeline architecture.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b92280",
   "metadata": {},
   "source": [
    "- user query\n",
    "- query expansion: 查询扩展, 反应原始用户查询的不同解释的方面\n",
    "- self-quering: 从原始查询中提取有用的元数据, 例如作者名称, 作为向量搜索操作的过滤器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d1c91",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2938dd",
   "metadata": {},
   "source": [
    "types:\n",
    "- online real-time inference\n",
    "- asynchronous inference\n",
    "- offline batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d399a",
   "metadata": {},
   "source": [
    "model serving:\n",
    "- monolith\n",
    "- microservices\n",
    "\n",
    "<img src=\"./images/Microservice deployment architecture of the LLM inference pipeline.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77f8b3",
   "metadata": {},
   "source": [
    "# LLMOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999df539",
   "metadata": {},
   "source": [
    "LLMOps is built on top of MLOps, which is built on DevOps.\n",
    "\n",
    "LLMOps focuses:\n",
    "- prompt monitoring and versioning\n",
    "- input and output guardrails to prevent toxic behavior\n",
    "- feedback loops to gather fine-tuning data\n",
    "- scaling issues:\n",
    "  - collect trillions of tokens for training datasets\n",
    "  - train models on massive GPU clusters\n",
    "  - reduce infrastructure costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72c52d",
   "metadata": {},
   "source": [
    "example:\n",
    "- CI/CD pipeline: GitHub Actions\n",
    "- CT, alerting: ZenML\n",
    "- monitoring: Opik from Comet ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8c6a0",
   "metadata": {},
   "source": [
    "infrastructure flow:\n",
    "\n",
    "<img src=\"./images/Infrastructure flow.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af162e",
   "metadata": {},
   "source": [
    "CT pipeline:\n",
    "\n",
    "<img src=\"./images/CT pipeline.png\" width=\"800\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

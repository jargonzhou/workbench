{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scholarly\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/23/bdf0229ab47fdabb3255cb9709b9af6f63f72150c3498fd9adc3155b1635/scholarly-0.2.5-py2.py3-none-any.whl\n",
      "Collecting arrow (from scholarly)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/39/a8e116084cf4160f0821ca9bb84ec312ccca84caa2b2bffb70d95d47f91f/arrow-0.15.4-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 12.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /anaconda3/lib/python3.7/site-packages (from scholarly) (4.6.3)\n",
      "Requirement already satisfied: requests[security] in /anaconda3/lib/python3.7/site-packages (from scholarly) (2.19.1)\n",
      "Collecting bibtexparser (from scholarly)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/cb/c30ee322d69a569423edbcc9cf544edc5d634f3340f448c918c8a3cbccf0/bibtexparser-1.1.0.tar.gz (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 4.8MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /anaconda3/lib/python3.7/site-packages (from arrow->scholarly) (2.7.3)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests[security]->scholarly) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests[security]->scholarly) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests[security]->scholarly) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests[security]->scholarly) (3.0.4)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14; extra == \"security\" in /anaconda3/lib/python3.7/site-packages (from requests[security]->scholarly) (18.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4; extra == \"security\" in /anaconda3/lib/python3.7/site-packages (from requests[security]->scholarly) (2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /anaconda3/lib/python3.7/site-packages (from bibtexparser->scholarly) (2.2.2)\n",
      "Requirement already satisfied: future>=0.16.0 in /anaconda3/lib/python3.7/site-packages (from bibtexparser->scholarly) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.7/site-packages (from python-dateutil->arrow->scholarly) (1.11.0)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /anaconda3/lib/python3.7/site-packages (from cryptography>=1.3.4; extra == \"security\"->requests[security]->scholarly) (0.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /anaconda3/lib/python3.7/site-packages (from cryptography>=1.3.4; extra == \"security\"->requests[security]->scholarly) (1.11.5)\n",
      "Requirement already satisfied: pycparser in /anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=1.3.4; extra == \"security\"->requests[security]->scholarly) (2.19)\n",
      "Building wheels for collected packages: bibtexparser\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/zhoujiagen/Library/Caches/pip/wheels/aa/4b/a0/424a31590e13ddc0e9cbb6ac5d27addef407b1dd781dbef1b1\n",
      "Successfully built bibtexparser\n",
      "Installing collected packages: arrow, bibtexparser, scholarly\n",
      "Successfully installed arrow-0.15.4 bibtexparser-1.1.0 scholarly-0.2.5\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip install scholarly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/scholarly/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is scholarly\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按文章搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchByPub(pub, result_count = 10):\n",
    "    pubs_query = pub\n",
    "    search_query = scholarly.search_pubs_query(pubs_query)\n",
    "    \n",
    "    if result_count <= 0:\n",
    "        result_count = 1\n",
    "    result_index = 1\n",
    "    for result in search_query:\n",
    "        if result_index == result_count:\n",
    "            break\n",
    "        \n",
    "        result.fill()\n",
    "        print(\"===[\", result_index, \"/\", result_count, \"]\", result.citedby)\n",
    "        pprint.pprint(result.bib)\n",
    "        print(result.url_scholarbib)\n",
    "        print()\n",
    "        result_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 1 / 10 ] 535\n",
      "{'ENTRYTYPE': 'article',\n",
      " 'ID': 'brewer2012cap',\n",
      " 'abstract': 'The CAP theorem asserts that any networked shared-data system '\n",
      "             'can have only two of three desirable properties. However, by '\n",
      "             'explicitly handling partitions, designers can optimize '\n",
      "             'consistency and availability, thereby achieving some trade-off '\n",
      "             'of all three. The featured Web …',\n",
      " 'author': 'Brewer, Eric',\n",
      " 'eprint': 'https://sites.cs.ucsb.edu/~rich/class/cs293b-cloud/papers/brewer-cap.pdf',\n",
      " 'journal': 'Computer',\n",
      " 'number': '2',\n",
      " 'pages': '23--29',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'CAP Twelve years Later: how the',\n",
      " 'url': 'https://www.computer.org/csdl/mags/co/2012/02/mco2012020023-abs.html',\n",
      " 'year': '2012'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:dg5-XLw_QdoJ:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOn0gMLBi_E5ADXdJkpgpJOchV0VgQ&scisf=4&ct=citation&cd=0&hl=en\n",
      "\n",
      "===[ 2 / 10 ] 3\n",
      "{'ENTRYTYPE': 'article',\n",
      " 'ID': 'brewer45cap',\n",
      " 'author': 'Brewer, Eric',\n",
      " 'journal': 'Rules” Have Changed”, Computer',\n",
      " 'title': 'CAP Twelve Years Later: How the',\n",
      " 'volume': '45'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:vfbFAMMimc0J:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOn1DVDubsn4sPH5GJfEeoU-DBMwmu&scisf=4&ct=citation&cd=1&hl=en\n",
      "\n",
      "===[ 3 / 10 ] 3\n",
      "{'ENTRYTYPE': 'article',\n",
      " 'ID': 'eric2012cap',\n",
      " 'author': 'Eric, Brewer',\n",
      " 'journal': 'Have Changed',\n",
      " 'title': 'CAP Twelve Years Later: How the “Rules',\n",
      " 'year': '2012'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:s15SzxlevqsJ:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOn00VK3mHgK35tLiBzAUMSwid23dW&scisf=4&ct=citation&cd=2&hl=en\n",
      "\n",
      "===[ 4 / 10 ] 3\n",
      "{'ENTRYTYPE': 'article',\n",
      " 'ID': 'brewer2012cap',\n",
      " 'author': 'Brewer, Eric',\n",
      " 'journal': 'URL: http://www. infoq. '\n",
      "            'com/articles/cap-twelve-yearslater-how-the-rules-have-changed',\n",
      " 'title': 'CAP twelve years later: How the\" rules\" have changed.(2012)',\n",
      " 'year': '2012'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:-hYQVwmfewEJ:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOny3dzIJ0YA6lH4xQYF4Pg8tZ1jwO&scisf=4&ct=citation&cd=3&hl=en\n",
      "\n",
      "===[ 5 / 10 ] 4\n",
      "{'ENTRYTYPE': 'misc',\n",
      " 'ID': 'payne1931case',\n",
      " 'abstract': '… I show this case in which two replanted teeth were retained, '\n",
      "             'and remained functional for more\\n'\n",
      "             'than twelve years … An impression was taken and a metal cap '\n",
      "             'splint constructed to fit over the lower\\n'\n",
      "             'incisor and canine … Three years later she returned and allowed '\n",
      "             'me to take them out … \\n',\n",
      " 'author': 'Payne, J Lewin',\n",
      " 'publisher': 'SAGE Publications',\n",
      " 'title': 'A case of replantation and the result after twelve years',\n",
      " 'url': 'https://journals.sagepub.com/doi/pdf/10.1177/003591573102400879',\n",
      " 'year': '1931'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:jCtDBfmvmGgJ:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOnxbRB5vbcEzswYa9ds4oyZHlYB7p&scisf=4&ct=citation&cd=4&hl=en\n",
      "\n",
      "===[ 6 / 10 ] 440\n",
      "{'ENTRYTYPE': 'book',\n",
      " 'ID': 'northup1968twelve',\n",
      " 'author': 'Northup, Solomon',\n",
      " 'eprint': 'https://www.penguin.com/static/pdf/teachersguides/twelveyears021314b.pdf',\n",
      " 'publisher': 'LSU Press',\n",
      " 'title': 'Twelve years a slave',\n",
      " 'year': '1968'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:JgELVZQwLn8J:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOnyVzZZky2htftNV-fNvs2lLK62ye&scisf=4&ct=citation&cd=5&hl=en\n",
      "\n",
      "===[ 7 / 10 ] 195\n",
      "{'ENTRYTYPE': 'article',\n",
      " 'ID': 'bailis2013eventual',\n",
      " 'abstract': '… 8. Brewer, E. 2012. CAP twelve years later: how the “rules” '\n",
      "             'have changed. IEEE Computer\\n'\n",
      "             '(February) … http://cs-www.cs.yale.edu/homes/dna/papers/abadi- '\n",
      "             'pacelc.pdf Brewer, E. 2012. CAP\\n'\n",
      "             'twelve years later: how the “rules” have changed. IEEE Computer '\n",
      "             '(February) … \\n',\n",
      " 'author': 'Bailis, Peter and Ghodsi, Ali',\n",
      " 'eprint': 'http://web.eecs.umich.edu/~mozafari/fall2015/eecs584/papers/eventual-acm.pdf',\n",
      " 'journal': 'Queue',\n",
      " 'number': '3',\n",
      " 'pages': '20',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Eventual consistency today: Limitations, extensions, and beyond',\n",
      " 'url': 'http://web.eecs.umich.edu/~mozafari/fall2015/eecs584/papers/eventual-acm.pdf',\n",
      " 'volume': '11',\n",
      " 'year': '2013'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:nP4xJIdnly0J:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOnwIsNJaUdsxcfBw52jnX42VSvmMN&scisf=4&ct=citation&cd=6&hl=en\n",
      "\n",
      "===[ 8 / 10 ] 170\n",
      "{'ENTRYTYPE': 'book',\n",
      " 'ID': 'woolley2009excavations',\n",
      " 'abstract': \"… U A Record of Twelve Year's Work … In the present volume I am \"\n",
      "             'concerned with the whole of the\\n'\n",
      "             'twelve years of excavation and, since it is meant to … them '\n",
      "             'cannot be radically altered, but the\\n'\n",
      "             'conclusions which we formed about them may have been modiﬁed by '\n",
      "             'later discoveries so … \\n',\n",
      " 'author': 'Woolley',\n",
      " 'publisher': 'Routledge',\n",
      " 'title': \"Excavations at Ur: a record of twelve years' work\",\n",
      " 'url': 'https://books.google.com/books?hl=en&lr=&id=7jDzCOgnWxEC&oi=fnd&pg=PP2&dq=CAP+Twelve+years+Later&ots=8h_W89Rb8o&sig=bPLHxeY_8qQ1RoO2cNkHgDWmzbI',\n",
      " 'year': '2009'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:vwbm8DntKdsJ:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOn59qSF9-bV3u5uZbW_8MivclBDWH&scisf=4&ct=citation&cd=7&hl=en\n",
      "\n",
      "===[ 9 / 10 ] 232\n",
      "{'ENTRYTYPE': 'book',\n",
      " 'ID': 'church1892oxford',\n",
      " 'author': 'Church, Richard William',\n",
      " 'publisher': 'Macmillan',\n",
      " 'title': 'The Oxford Movement: Twelve Years, 1833-1845',\n",
      " 'year': '1892'}\n",
      "https://scholar.googleusercontent.com/scholar.bib?q=info:Iy9zZsHbHzwJ:scholar.google.com/&output=citation&scisdr=CgWuNdWXENba3i8Lh2s:AAGBfm0AAAAAXcEOn2v4ZSSStRiujvFMlTCSiN162h7_&scisig=AAGBfm0AAAAAXcEOn_hPsaF-EOK4uHRCjPgZ47hoNr0U&scisf=4&ct=citation&cd=8&hl=en\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SearchByPub(\"CAP Twelve years Later\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　http://redbook.cs.berkeley.edu/bib4.html\n",
    "pubs = [\n",
    "    # Data Models and DBMS Architecture\n",
    "    \"What Goes Around Comes Around\",\n",
    "    \"Anatomy of a Database System\",\n",
    "    \n",
    "    # Query Processing\n",
    "    \"Access Path Selection in a Relational Database Management System\",\n",
    "    \"Join Processing in Database Systems with Large Main Memories\",\n",
    "    \"Parallel Database Systems: The Future of High Performance Database Systems\",\n",
    "    \"Encapsulation of Parallelism in the Volcano Query Processing System\",\n",
    "    \"AlphaSort: A Cache-Sensitive Parallel External Sort\",\n",
    "    \"R* Optimizer Validation and Performance Evaluation for Distributed Queries\",\n",
    "    \"Mariposa: A Wide-Area Distributed Database System\",\n",
    "    \n",
    "    # Data Storage and Access Methods\n",
    "    \"What Goes Around Comes Around\",\n",
    "    \"Anatomy of a Database System\",\n",
    "    \"The R*-Tree: An Efficient and Robust Access Method for Points and Rectangles\",\n",
    "    \"Operating System Support for Database Management\",\n",
    "    \"The Five-Minute Rule Ten Years Later, and Other Computer Storage Rules of Thumb\",\n",
    "    \"A Case for Redundant Arrays of Inexpensive Disks\", \n",
    "]\n",
    "\n",
    "for pub in pubs:\n",
    "    SearchByPub(pub=pub, result_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按作者搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchByAuthor(author, pub_count=100):\n",
    "    author_query = author\n",
    "    search_query = scholarly.search_author(author_query)\n",
    "\n",
    "    if pub_count <= 0:\n",
    "        pub_count = 1\n",
    "    for result in search_query:\n",
    "        result.fill()\n",
    "        print(\"***\")\n",
    "        print(result.id)\n",
    "        print(result.name)\n",
    "        print(result.email)\n",
    "        print(result.interests)\n",
    "        print(result.url_picture)\n",
    "        publications = result.publications\n",
    "        real_pub_count = len(publications)\n",
    "        print(\"total pub count: \", real_pub_count)\n",
    "        pub_index = 1\n",
    "        for pub in publications:\n",
    "            if pub_index == pub_count or pub_index > real_pub_count:\n",
    "                break\n",
    "            \n",
    "            pub.fill()\n",
    "            print(\"===[\", pub_index, \"/\", pub_count, \"]\", pub.citedby)\n",
    "            pprint.pprint(pub.bib)\n",
    "            print()\n",
    "            pub_index += 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "pdDeRScAAAAJ\n",
      "Goetz Graefe\n",
      "@google.com\n",
      "['Database systems', 'queries', 'indexes', 'transactions']\n",
      "https://scholar.google.com/citations?view_op=medium_photo&user=pdDeRScAAAAJ\n",
      "total pub count:  336\n",
      "===[ 1 / 100 ] 1831\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate the problem: In order to manipulate large sets of complex objects as efficiently as today's database systems manipulate simple records, query-processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software.</div><div class=\"gsh_csp\">This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.3178&rep=rep1&type=pdf',\n",
      " 'journal': 'ACM Computing Surveys (CSUR)',\n",
      " 'number': '2',\n",
      " 'pages': '73-169',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Query evaluation techniques for large databases',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=152611',\n",
      " 'volume': '25',\n",
      " 'year': 1993}\n",
      "\n",
      "===[ 2 / 100 ] 555\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Emerging database application domains demand not only new functionality but also high performance. To satisfy these two requirements, the Volcano project provides efficient, extensible tools for query and request processing, particularly for object-oriented and scientific database systems. One of these tools is a new optimizer generator. Data model, logical algebra, physical algebra, and optimization rules are translated by the optimizer generator into optimizer source code. Compared with our earlier EXODUS optimizer generator prototype, the search engine is more extensible and powerful; it provides effective support for non-trivial cost models and for physical properties such as sort order. At the same time, it is much more efficient as it combines dynamic programming, which until now had been used only for relational select-project-join optimization, with goal-directed search and branch-andbound pruning. Compared with other rule-based optimization systems, it provides complete data model independence and more natural extensibility.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and William J McKenna',\n",
      " 'eprint': 'https://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/Papers/Volcano-graefe.pdf',\n",
      " 'journal': 'ICDE',\n",
      " 'pages': '209-218',\n",
      " 'title': 'The volcano optimizer generator: Extensibility and efficient search',\n",
      " 'url': 'https://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/Papers/Volcano-graefe.pdf',\n",
      " 'volume': '93',\n",
      " 'year': 1993}\n",
      "\n",
      "===[ 3 / 100 ] 514\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">In this paper, we present the design, implementation techniques, and initial performance evaluation of Gamma. Gamma is a new relational database machine that exploits dataflow query processing techniques. Gamma is a fully operational prototype consisting of 20 VAX 11/750 computers. The design of Gamma is based on what we learned from building our earlier multiprocessor database machine prototype (DIRECT) and several years of subsequent research on the problems raised by the DIRECT prototype.</div><div class=\"gsh_csp\">In addition to demonstrating that parallelism can really be made to work in a database machine context, the Gamma prototype shows how parallelism can be controlled with minimal control overhead through a combination of the use of algorithms based on hashing and the pipelining of data between processes. Except for 2 messages to initiate each operator of a query tree and 1 message when the operator …</div></div></div>,\n",
      " 'author': 'David J DeWitt and Robert Gerber and Goetz Graefe and Michael '\n",
      "           'Heytens and Krishna Kumar and Murali Muralikrishna',\n",
      " 'eprint': 'https://minds.wisconsin.edu/bitstream/handle/1793/58708/TR635.pdf?sequence=1',\n",
      " 'publisher': 'University of Wisconsin-Madison Department of Computer Sciences',\n",
      " 'title': 'Gamma-a high performance dataflow database machine',\n",
      " 'url': 'https://minds.wisconsin.edu/bitstream/handle/1793/58708/TR635.pdf?sequence=1',\n",
      " 'year': 1986}\n",
      "\n",
      "===[ 4 / 100 ] 508\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\">To investigate the interactions of extensibility and parallelism in database query processing, we have developed a new dataflow query execution system called Volcano. The Volcano effort provides a rich environment for research and education in database systems design, heuristics for query optimization, parallel query execution, and resource allocation. Volcano uses a standard interface between algebra operators, allowing easy addition of new operators and operator implementations. Operations on individual items, e.g., predicates, are imported into the query processing operators using support functions. The semantics of support functions is not prescribed; any data type including complex objects and any operation can be realized. Thus, Volcano is extensible with new operators, algorithms, data types, and type-specific methods. Volcano includes two novel meta-operators. The choose-plan meta-operator …</div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://15721.courses.cs.cmu.edu/spring2016/papers/graefe-ieee1994.pdf',\n",
      " 'journal': 'IEEE Transactions on Knowledge and Data Engineering',\n",
      " 'number': '1',\n",
      " 'pages': '120-135',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'Volcano/spl minus/an extensible and parallel query evaluation '\n",
      "          'system',\n",
      " 'url': 'https://ieeexplore.ieee.org/abstract/document/273032/',\n",
      " 'volume': '6',\n",
      " 'year': 1994}\n",
      "\n",
      "===[ 5 / 100 ] 500\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Volcano is a new dataflow query processing system we have developed for database systems research and education. The uniform interface between operators makes Volcano extensible by new operators. All operators are designed and coded as if they were meant for a single-process system only. When attempting to parallelize Volcano, we had to choose between two models of parallelization, called here the bracket and operator models. We describe the reasons for not choosing the bracket model, introduce the novel operator model, and provide details of Volcano's exchange operator that parallelizes all other operators. It allows intra-operator parallelism on partitioned datasets and both vertical and horizontal inter-operator parallelism. The exchange operator encapsulates all parallelism issues and therefore makes implementation of parallel database algorithms significantly easier and more robust. Included in …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://w6113.github.io/files/papers/volcanoparallelism-89.pdf',\n",
      " 'journal': 'ACM SIGMOD Record',\n",
      " 'number': '2',\n",
      " 'pages': '102-111',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Encapsulation of parallelism in the Volcano query processing system',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=98720',\n",
      " 'volume': '19',\n",
      " 'year': 1990}\n",
      "\n",
      "===[ 6 / 100 ] 456\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">This technical note shows how to combine some well-known techniques to create a method that will efficiently execute common multi-table joins. We concentrate on a commonly occurring type of join known as a star-join, although the method presented will generalize to any type of multi-table join. A star-join consists of a central detail table with large cardinality, such as an orders table (where an order row contains a single purchase) with foreign keys that join to descriptive tables, such as customers, products, and (sales) agents. The method presented in this note uses join indices with compressed bitmap representations, which allow predicates restricting columns of descriptive tables to determine an answer set (or foundset) in the central detail table; the method uses different predicates on different descriptive tables in combination to restrict the detail table through compressed bitmap representations of join indices …</div></div></div>,\n",
      " 'author': \"Patrick O'Neil and Goetz Graefe\",\n",
      " 'eprint': 'https://www.cs.umb.edu/~poneil/bitmjoin.pdf',\n",
      " 'journal': 'ACM SIGMOD Record',\n",
      " 'number': '3',\n",
      " 'pages': '8-11',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Multi-table joins through bitmapped join indices',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=212001',\n",
      " 'volume': '24',\n",
      " 'year': 1995}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 7 / 100 ] 443\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">This paper presents the design and an initial performance evaluation of the query optimizer generator designed for the EXODUS extensible database system. Algebraic transformation rules are translated into an executable query optimizer, which transforms query trees and selects methods for executing operations according to cost functions associated with the methods. The search strategy avoids exhaustive search and it modifies itself to take advantage of past experience. Computational results show that an optimizer generated for a relational system produces access plans almost as good as those produced by exhaustive search, with the search time cut to a small fraction.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and David J DeWitt',\n",
      " 'eprint': 'https://www.seas.upenn.edu/~zives/cis650/papers/exodus.pdf',\n",
      " 'number': '3',\n",
      " 'pages': '160-172',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'The EXODUS optimizer generator',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=38734',\n",
      " 'volume': '16',\n",
      " 'year': 1987}\n",
      "\n",
      "===[ 8 / 100 ] 418\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">With non-traditional application areas such as engineering design, image/voice data management, scientific/statistical applications, and artificial intelligence systems all clamoring for ways to store and efficiently process larger and larger volumes of data, it is clear that traditional database technology has been pushed to its limits. It also seems clear that no single database system will be capable of simultaneously meeting the functionality and performance requirements of such a diverse set of applications. In this paper we describe the preliminary design of EXODUS, an extensible database system that will facilitate the fast development of high-performance, application-specific database systems. EXODUS provides certain kernel facilities, including a versatile storage manager and a type manager. In addition, it provides an architectural framework for building application-specific database systems, tools to partially …</div></div></div>,\n",
      " 'author': 'Michael J Carey and David J DeWitt and Daniel Frank and M '\n",
      "           'Muralikrishna and Goetz Graefe and Joel E Richardson and Eugene J '\n",
      "           'Shekita',\n",
      " 'eprint': 'https://minds.wisconsin.edu/bitstream/handle/1793/58726/TR644.pdf?sequence=1',\n",
      " 'pages': '52-65',\n",
      " 'publisher': 'IEEE Computer Society Press',\n",
      " 'title': 'The architecture of the EXODUS extensible DBMS',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=318839',\n",
      " 'year': 1986}\n",
      "\n",
      "===[ 9 / 100 ] 333\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">This paper describes a new extensible query optimization framework that resolves many of the shortcomings of the EXODUS and Volcano optimizer generators. In addition to extensibility, dynamic programming, and memorization based on and extended from the EXODUS and Volcano prototypes, this new optimizer provides (i) manipulation of operator arguments using rules or functions,(ii) operators that are both logical and physical for predicates etc.,(iii) schema-specific rules for materialized views,(iv) rules to insert” enforcers” or” glue operators,”(v) rule-specific guidance, permitting grouping of rules,(vi) basic facilities that will later permit parallel search, partially ordered cost measures, and dynamic plans,(vii) extensive tracing support, and (viii) a clean interface and implementation making full use of the abstraction mechanisms of C++. We describe and justify our design choices for each of these issues. The optimizer system described here is operational and will serve as the foundation for new query optimizers in Tandem’s NonStop SQL product and in Microsoft’s SQL Server product.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://15721.courses.cs.cmu.edu/spring2018/papers/15-optimizer1/graefe-ieee1995.pdf',\n",
      " 'journal': 'IEEE Data Eng. Bull.',\n",
      " 'number': '3',\n",
      " 'pages': '19-29',\n",
      " 'title': 'The cascades framework for query optimization',\n",
      " 'url': 'https://15721.courses.cs.cmu.edu/spring2018/papers/15-optimizer1/graefe-ieee1995.pdf',\n",
      " 'volume': '18',\n",
      " 'year': 1995}\n",
      "\n",
      "===[ 10 / 100 ] 263\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">This paper presents an overview of EXODUS, an extensible database system project that is addressing data management problems posed by a variety of challenging new applications. The goal of the project is to facilitate the fast development of high-performance, application-specific database systems. EXODUS provides certain kernel facilities, including a versatile storage manager. In addition, it provides an architectural framework for building application-specific database systems; powerful tools to help automate the generation of such systems, including a rule-based query optimizer generator and a persistent programming language; and libraries of generic software components (eg, access methods) that are likely to be useful for many application domains. We briefly describe each of the components of EXODUS in this paper, and we also describe a next-generation DBMS that we are now building using the EXODUS tools.</div></div></div>,\n",
      " 'author': 'Michael J Carey and David J DeWitt and Goetz Graefe and David M '\n",
      "           'Haight and Joel E Richardson and Daniel T Schuh and Eugene J '\n",
      "           'Shekita and Scott L Vandenberg',\n",
      " 'eprint': 'https://minds.wisconsin.edu/bitstream/handle/1793/59050/TR808.pdf?sequence=1',\n",
      " 'journal': 'Readings in object-oriented database systems',\n",
      " 'pages': '474-499',\n",
      " 'publisher': 'Morgan Kaufmann',\n",
      " 'title': 'The EXODUS extensible DBMS project: An overview',\n",
      " 'url': 'http://books.google.com/books?hl=en&lr=&id=vdHATYxBA5sC&oi=fnd&pg=PA474&dq=info:jcGRHvgHsaoJ:scholar.google.com&ots=6_l9a2XC7-&sig=DPEfQxJLZpnFAUHOWc8ys4L1vzA',\n",
      " 'year': 1990}\n",
      "\n",
      "===[ 11 / 100 ] 259\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Traditional query optimizers assume accurate knowledge of run-time parameters such as selectivities and resource availability during plan optimization, ie, at compile time. In reality, however, this assumption is often not justified. Therefore, the “static” plans produced by traditional optimizers may not be optimal for many of their actual run-time invocations. Instead, we propose a novel optimization model that assigns the bulk of the optimization effort to compile-time and delays carefully selected optimization decisions until run-time. Our previous work defined the run-time primitives,“dynamic plans” using “choose-plan” operators, for executing such delayed decisions, but did not solve the problem of constructing dynamic plans at compile-time. The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up …</div></div></div>,\n",
      " 'author': 'Richard L Cole and Goetz Graefe',\n",
      " 'eprint': 'https://apps.dtic.mil/dtic/tr/fulltext/u2/a461520.pdf',\n",
      " 'journal': 'ACM SIGMOD Record',\n",
      " 'number': '2',\n",
      " 'pages': '150-160',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Optimization of dynamic query evaluation plans',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=191872',\n",
      " 'volume': '23',\n",
      " 'year': 1994}\n",
      "\n",
      "===[ 12 / 100 ] 229\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">In most database systems, a query embedded in a program written in a conventional programming language is optimized when the program is compiled. The query optimizer must make assumptions about the values of the program variables that appear as constants in the query, the resources that can be committed to query evaluation, and the data in the database. The optimality of the resulting query evaluation plan depends on the validity of these assumptions. If a query evaluation plan is used repeatedly over an extended period of time, it is important to determine when reoptimization is necessary. Our work aims at developing criteria when reoptimization is required, how these criteria can be implemented efficiently, and how reoptimization can be avoided by using a new technique called dynamic query evaluation plans. We experimentally demonstrate the need for dynamic plans and outline modifications to the …</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Karen Ward',\n",
      " 'eprint': 'https://people.inf.elte.hu/kiss/cikkek/092%20Dynamic%20query%20evaluation%20(9%20oldal).pdf',\n",
      " 'number': '2',\n",
      " 'pages': '358-366',\n",
      " 'publisher': 'Acm',\n",
      " 'title': 'Dynamic query evaluation plans',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=66960',\n",
      " 'volume': '18',\n",
      " 'year': 1989}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 13 / 100 ] 220\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">A system and method for optimizing a database query is herein disclosed. The system consists of a search engine and a database implementor that determines an optimal plan for executing a SQL query. The SQL query is represented as a query tree consisting of a number of nested expressions. The search engine generates a number of plans from which an optimal plan is selected. Plans are generated through the application of a set of rules consisting of implementation and transformation rules. Implementation rules are used to obtain plans. Transformation rules are used to determine equivalent expressions. A plan for the query tree entails finding plans for each expression within the tree where each plan is generated in accordance with a prescribed set of rules. The database implementor selects the set of rules such that more promising plans are generated rather than generating all possible plans. In a preferred …</div></div></div>,\n",
      " 'title': 'System and method for optimizing database queries',\n",
      " 'url': 'https://patents.google.com/patent/US5822747A/en',\n",
      " 'year': 1998}\n",
      "\n",
      "===[ 14 / 100 ] 199\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\">Data compression is widely used in data management to save storage space and network bandwidth. The authors outline the performance improvements that can be achieved by exploiting data compression in query processing. The novel idea is to leave data in compressed state as long as possible, and to only uncompress data when absolutely necessary. They show that many query processing algorithms can manipulate compressed data just as well as decompressed data, and that processing compressed data can speed query processing by a factor much larger than the compression factor.&lt;&gt;</div></div>,\n",
      " 'author': 'Goetz Graefe and Leonard D Shapiro',\n",
      " 'eprint': 'https://cs.brown.edu/courses/cs227/archives/2008/Papers/Compression/GraefeShapiro.pdf',\n",
      " 'pages': '22-27',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'Data compression and database performance',\n",
      " 'url': 'https://ieeexplore.ieee.org/abstract/document/143840/',\n",
      " 'year': 1991}\n",
      "\n",
      "===[ 15 / 100 ] 187\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Today’s databases require database administrators (DBAs) who are responsible for performance tuning. However, as usage of databases becomes pervasive, it is important that the databases are able to automatically tune themselves to application needs and hardware capabilities rather than require external intervention by DBAs or applications. Thus self-tuning databases would result in significant reduction in the cost of ownership of databases and would encourage more widespread use in many nontraditional applications. However, making database management systems self-tuning require significant understanding of DBMS components and relationships among many of the” tuning knobs” that are exposed to the application/user. Microsoft SQL Server is committed to the vision of self-tuning databases. In this short paper, we review some of the recent advances in making Microsoft SQL Server self-tuning. Section 2 describes a physical database design tool, available with SQL Server 7.0, that automatically picks indexes appropriate for a SQL Server database. Section 3 and Section 4 describe the self-tuning advances in the query engine and the storage engine respectively for Microsoft SQL Server 7.0.</div></div></div>,\n",
      " 'author': 'Surajit Chaudhuri and Eric Christensen and Goetz Graefe and Vivek '\n",
      "           'R.  Narasayya and Michael J.  Zwilling',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.654&rep=rep1&type=pdf#page=22',\n",
      " 'journal': 'IEEE Data Eng. Bull.',\n",
      " 'number': '2',\n",
      " 'pages': '20-26',\n",
      " 'title': 'Self-tuning technology in microsoft sql server',\n",
      " 'url': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.47.654&rep=rep1&type=pdf#page=22',\n",
      " 'volume': '22',\n",
      " 'year': 1999}\n",
      "\n",
      "===[ 16 / 100 ] 175\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">This paper reports our experiences building the query optimizer for TI's Open OODB system. To the best of our knowledge, it is the first working object query optimizer to be based on a complete extensible optimization framework including logical algebra, execution algorithms, property enforcers, logical transformation rules, implementation rules, and selectivity and cost estimation. Our algebra incorporates a new materialize operator with its corresponding logical transformation and implementation rules that enable the optimization of path expressions. Initial experiments on queries obtained from the object query optimization literature demonstrate that our optimizer is able to derive plans that are as efficient as, and often substantially more efficient than, the plans generated by other query optimization strategies. These experiments demonstrate that our initial choices for populating each part of our optimization …</div></div></div>,\n",
      " 'author': 'José A Blakeley and William J McKenna and Goetz Graefe',\n",
      " 'eprint': 'https://people.inf.elte.hu/kiss/cikkek/094%20OODB%20query%20optimizer%20(10%20oldal).pdf',\n",
      " 'number': '2',\n",
      " 'pages': '287-296',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Experiences building the Open OODB query optimizer',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=170080',\n",
      " 'volume': '22',\n",
      " 'year': 1993}\n",
      "\n",
      "===[ 17 / 100 ] 168\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Solid state drives perform random reads more than 100x faster than traditional magnetic hard disks, while offering comparable sequential read and write bandwidth. Because of their potential to speed up applications, as well as their reduced power consumption, these new drives are expected to gradually replace hard disks as the primary permanent storage media in large data centers. However, although they may benefit applications that stress random reads immediately, they may not improve database applications, especially those running long data analysis queries. Database query processing engines have been designed around the speed mismatch between random and sequential I/O on hard disks and their algorithms currently emphasize sequential accesses for disk-resident data.</div><div class=\"gsh_csp\">In this paper, we investigate data structures and algorithms that leverage fast random reads to speed up selection, projection …</div></div></div>,\n",
      " 'author': 'Dimitris Tsirogiannis and Stavros Harizopoulos and Mehul A Shah '\n",
      "           'and Janet L Wiener and Goetz Graefe',\n",
      " 'eprint': 'http://www.cs.albany.edu/~jhh/courses/readings/tsirogiannis.sigmod09.solid.pdf',\n",
      " 'pages': '59-72',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Query processing techniques for solid state drives',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1559854',\n",
      " 'year': 2009}\n",
      "\n",
      "===[ 18 / 100 ] 166\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">9 gram (a combination of Csíkszentmihályi’s Flow diagram with Vygotsky’s Zone of Proximal Development conceptualization). The essence of Scalable Game Design is that programming challenges and skills should be balanced and there are different paths, some better suited than others for broadening participation, along which students can advance their skills and tackle more advanced challenges. The four main goals and approaches of the Scalable Game Design framework are described here. Exposure: Develop a highly adoptable middle school CT curriculum integrated into existing computer education and STEM courses so that a potentially very large and diverse group of children is exposed to CT concepts. The small number of middle schools that offer programmingrelated after-school programs attract only a small number of students. A successful computer club at a middle school may attract about 20 …</div></div></div>,\n",
      " 'author': 'Alexander Repenning',\n",
      " 'journal': 'Communications of the ACM',\n",
      " 'number': '5',\n",
      " 'pages': '38-40',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Programming goes back to school',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=2160729',\n",
      " 'volume': '55',\n",
      " 'year': 2012}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 19 / 100 ] 161\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Simple economic and performance arguments suggest appropriate lifetimes for main memory pages and suggest optimal page sizes. The fundamental tradeoffs are the prices and bandwidths of RAMs and disks. The analysis indicates that with today's technology, five minutes is a good lifetime for randomly accessed pages, one minute is a good lifetime for two-pass sequentially accessed pages, and 16 KB is a good size for index pages. These rules-of-thumb change in predictable ways as technology ratios change. They also motivate the importance of the new Kaps, Maps, Scans, and <svg aria-label=\"/Kaps, \" class=\"gs_fsvg\" height=\"14px\" style=\"vertical-align:-4px;\" width=\"45px\"><g transform=\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 10.50000)\"><path d=\"M 115 -471  Q 115 -465 117 -463  L 829 1511  Q 833 1523 843 1529  T 866 1536  Q 884 1536 895 1525  T 907 1495  V 1487  L 195 -487  Q 183 -512 156 -512  Q 139 -512 127 -500  T 115 -471  Z \" transform=\"scale(0.48828, -0.48828)\"></path><path d=\"M 96 0  Q 76 0 76 27  Q 77 32 80 44  T 88 64  T 102 72  Q 227 72 276 86  Q 303 95 315 141  L 596 1266  Q 600 1286 600 1294  Q 600 1316 575 1319  Q 537 1327 428 1327  Q 408 1327 408 1354  Q 415 1380 419 1389  T 442 1399  H 993  Q 1014 1399 1014 1372  Q 1013 1367 1010 1355  T 1002 1335  T 987 1327  Q 862 1327 813 1313  Q 786 1303 774 1257  L 608 594  L 1411 1214  Q 1413 1218 1436 1241  T 1460 1288  Q 1460 1327 1391 1327  Q 1370 1327 1370 1354  Q 1374 1370 1376 1379  T 1385 1393  T 1405 1399  H 1802  Q 1813 1399 1818 1391  T 1823 1372  Q 1822 1367 1818 1354  T 1809 1334  T 1794 1327  Q 1644 1327 1466 1188  Q 1465 1187 1463 1186  T 1459 1185  T 1456 1184  L 1034 856  L 1341 137  Q 1369 92 1401 82  T 1499 72  Q 1520 72 1520 45  Q 1514 21 1508 10  T 1485 0  H 1010  Q 991 0 991 27  Q 994 48 998 60  T 1018 72  Q 1070 72 1106 82  T 1143 131  Q 1143 145 1141 152  L 887 743  L 588 512  L 494 133  Q 489 108 489 104  Q 489 83 514 80  Q 553 72 662 72  Q 682 72 682 45  Q 675 16 671 8  T 647 0  H 96  Z \" transform=\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 500.00000, 0.00000)\"></path><path d=\"M 356 -23  Q 227 -23 152 74  T 78 305  Q 78 436 146 577  T 329 811  T 578 905  Q 639 905 687 872  T 762 782  Q 785 864 852 864  Q 878 864 895 848  T 913 807  Q 913 801 912 798  T 911 791  L 768 219  Q 754 158 754 119  Q 754 31 813 31  Q 877 31 910 112  T 967 301  Q 971 313 983 313  H 1008  Q 1016 313 1021 306  T 1026 293  Q 990 150 947 63  T 809 -23  Q 740 -23 687 17  T 621 125  Q 489 -23 356 -23  Z M 358 31  Q 432 31 501 86  T 621 217  Q 623 219 623 223  L 733 668  L 735 674  Q 723 747 682 799  T 573 852  Q 504 852 444 795  T 344 662  Q 304 580 267 437  T 231 215  Q 231 144 261 87  T 358 31  Z \" transform=\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 1420.85754, 0.00000)\"></path><path d=\"M -51 -397  Q -70 -397 -70 -371  Q -63 -326 -43 -326  Q 21 -326 46 -315  T 86 -256  L 317 666  Q 332 704 332 764  Q 332 852 272 852  Q 208 852 177 775  T 117 582  Q 117 569 100 569  H 76  Q 71 569 65 576  T 59 590  Q 81 679 101 741  T 165 854  T 274 905  Q 344 905 397 865  T 465 758  Q 521 823 589 864  T 729 905  Q 815 905 878 859  T 974 738  T 1006 578  Q 1006 446 938 305  T 755 70  T 508 -23  Q 448 -23 399 11  T 324 100  L 231 -264  Q 226 -294 225 -299  Q 225 -326 360 -326  Q 381 -326 381 -352  Q 374 -379 369 -388  T 346 -397  H -51  Z M 510 31  Q 581 31 642 89  T 741 221  Q 769 276 793 355  T 836 525  T 854 668  Q 854 715 841 756  T 800 824  T 725 852  Q 650 852 583 798  T 463 666  V 659  L 350 207  Q 364 134 404 82  T 510 31  Z \" transform=\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 1949.46753, 0.00000)\"></path><path d=\"M 178 125  Q 233 31 399 31  Q 471 31 536 55  T 643 129  T 686 248  Q 686 301 648 335  T 555 381  L 444 403  Q 368 422 319 474  T 270 600  Q 270 691 319 761  T 450 868  T 618 905  Q 711 905 784 860  T 858 729  Q 858 682 831 646  T 758 610  Q 731 610 711 627  T 692 672  Q 692 696 705 718  T 741 754  T 788 768  Q 770 812 720 832  T 614 852  Q 562 852 510 831  T 426 769  T 395 674  Q 395 637 421 609  T 485 569  L 604 545  Q 661 533 708 502  T 783 425  T 811 319  Q 811 243 768 169  T 664 51  Q 555 -23 397 -23  Q 288 -23 197 27  T 106 176  Q 106 232 138 273  T 227 315  Q 260 315 282 295  T 305 242  Q 305 195 270 160  T 188 125  H 178  Z \" transform=\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 2452.57764, 0.00000)\"></path><path d=\"M 203 -369  Q 203 -360 211 -352  Q 285 -281 326 -188  T 367 8  V 33  Q 334 0 285 0  Q 238 0 205 33  T 172 113  Q 172 161 205 193  T 285 225  Q 358 225 389 157  T 420 8  Q 420 -106 374 -209  T 246 -393  Q 238 -397 233 -397  Q 223 -397 213 -388  T 203 -369  Z \" transform=\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 2921.35767, 0.00000)\"></path></g></svg>/Maps, $/TBscan metrics.</div></div></div>,\n",
      " 'author': 'Jim Gray and Goetz Graefe',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.2158&rep=rep1&type=pdf',\n",
      " 'journal': 'ACM Sigmod Record',\n",
      " 'number': '4',\n",
      " 'pages': '63-68',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'The five-minute rule ten years later, and other computer storage '\n",
      "          'rules of thumb',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=271094',\n",
      " 'volume': '26',\n",
      " 'year': 1997}\n",
      "\n",
      "===[ 20 / 100 ] 145\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">A “pivot” operation rotates the data items in a relational database table so that certain data values in the table become column names of the pivoted table, and the data items of a specified value column appear in corresponding rows in the new columns of the pivoted table. A pivot list specifies that only certain values of the pivot column data items participate in the operation. Additional columns of the input table appear as columns in the output table; the rows of the output table are grouped by equal data-item values in these grouping columns. An “unpivot” operation provides the inverse of the pivot operation. Both operations may be nested in an SQL user query at the algebraic level. The operations occur in the search engine of a relational database management system, and may also be invoked as part of an optimization of another query.</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/bb/45/e8/88e1d8c1103c32/US6298342.pdf',\n",
      " 'title': 'Electronic database operations for perspective transformations on '\n",
      "          'relational tables using pivot and unpivot columns',\n",
      " 'url': 'https://patents.google.com/patent/US6298342B1/en',\n",
      " 'year': 2001}\n",
      "\n",
      "===[ 21 / 100 ] 139\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Partitioning within a B-tree, based on an artificial leading key column and combined with online reorganization, can be exploited during external merge sort for accurate deep read-ahead and dynamic resource allocation, during index creation for a reduced delay until the first query can search the new index, during data loading for streaming integration of new data into a fully indexed database, and for miscellaneous other operations. Despite improving multiple fundamental database operations using a single basic mechanism, the proposal offers these benefits without requiring data structures or algorithms not yet supported in modern relational database management systems. While some of the ideas discussed here have been touched upon elsewhere, the focus here is on re-thinking the relationship between sorting and B-trees more thoroughly, on exploiting this relationship to simplify and unify data structures and algorithms, and on gathering comprehensive lists of issues and benefits.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://www-db.cs.wisc.edu/cidr/cidr2003/program/p1.pdf',\n",
      " 'journal': 'CIDR',\n",
      " 'pages': '5-8',\n",
      " 'title': 'Sorting And Indexing With Partitioned B-Trees.',\n",
      " 'url': 'http://www-db.cs.wisc.edu/cidr/cidr2003/program/p1.pdf',\n",
      " 'volume': '3',\n",
      " 'year': 2003}\n",
      "\n",
      "===[ 22 / 100 ] 137\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Large writes are beneficial both on individual disks and on disk arrays, eg, RAID-5. The presented design enables large writes of internal B-tree nodes and leaves. It supports both in-place updates and large append-only (\" log-structured\") write operations within the same storage volume, within the same B-tree, and even at the same time. The essence of the proposal is to make page migration inexpensive, to migrate pages while writing them, and to make such migration optional rather than mandatory as in log-structured file systems. The inexpensive page migration also aids traditional defragmentation as well as consolidation of free space needed for future large writes. These advantages are achieved with a very limited modification to conventional B-trees that also simplifies other B-tree operations, eg, key range locking and compression.</div><div class=\"gsh_csp\">Prior proposals and prototypes implemented transacted B-tree on top of log …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://www.vldb.org/conf/2004/RS18P2.PDF',\n",
      " 'pages': '672-683',\n",
      " 'publisher': 'VLDB Endowment',\n",
      " 'title': 'Write-optimized B-trees',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1316748',\n",
      " 'year': 2004}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 23 / 100 ] 135\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">PIVOT and UNPIVOT, two operators on tabular data that exchange rows and columns, enable data transformations useful in data modeling, data analysis, and data presentation. They can quite easily be implemented inside a query processor, much like select, project, and join. Such a design provides opportunities for better performance, both during query optimization and query execution. We discuss query optimization and execution implications of this integrated design and evaluate the performance of this approach using a prototype implementation in Microsoft SQL Server.</div></div></div>,\n",
      " 'author': 'Conor Cunningham and César A Galindo-Legaria and Goetz Graefe',\n",
      " 'eprint': 'http://www.vldb.org/conf/2004/IND1P2.PDF',\n",
      " 'pages': '998-1009',\n",
      " 'publisher': 'VLDB Endowment',\n",
      " 'title': 'PIVOT and UNPIVOT: Optimization and Execution Strategies in an '\n",
      "          'RDBMS',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1316775',\n",
      " 'year': 2004}\n",
      "\n",
      "===[ 24 / 100 ] 126\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Invented about 40 years ago and called ubiquitous less than 10 years later, B-tree indexes have been used in a wide variety of computing systems from handheld devices to mainframes and server farms. Over the years, many techniques have been added to the basic design in order to improve efficiency or to add functionality. Examples include separation of updates to structure or contents, utility operations such as non-logged yet transactional index creation, and robust query processing such as graceful degradation during index-to-index navigation.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://www.nowpublishers.com/article/DownloadSummary/DBS-028',\n",
      " 'journal': 'Foundations and Trends® in Databases',\n",
      " 'number': '4',\n",
      " 'pages': '203-402',\n",
      " 'publisher': 'Now Publishers, Inc.',\n",
      " 'title': 'Modern B-tree techniques',\n",
      " 'url': 'http://www.nowpublishers.com/article/Details/DBS-028',\n",
      " 'volume': '3',\n",
      " 'year': 2011}\n",
      "\n",
      "===[ 25 / 100 ] 126\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">For a wide variety of classification algorithms, scalability to large databases can be achieved by observing that most algorithms are driven by a set of sufficient statistics that are significantly smaller than the data. By relying on a SQL backend to compute the sufficient statistics, we leverage the query processing system of SQL databases and avoid the need for moving data to the client. We present a new SQL operator (Unpivot) that enables efficient gathering of statistics with minimal changes to the SQL backend. Our approach results in significant increase in performance without requiring any changes to the physical layout of the data. We show analytically how this approach outperforms an alternative that requires changing in the data layout. We also compare effect of data representation and show that a “dense” representation may be preferred to a “sparse” one, even when the data are fairly sparse.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Usama M Fayyad and Surajit Chaudhuri',\n",
      " 'eprint': 'https://www.aaai.org/Papers/KDD/1998/KDD98-034.pdf',\n",
      " 'pages': '204-208',\n",
      " 'title': 'On the Efficient Gathering of Sufficient Statistics for '\n",
      "          'Classification from Large SQL Databases.',\n",
      " 'url': 'https://www.aaai.org/Papers/KDD/1998/KDD98-034.pdf',\n",
      " 'year': 1998}\n",
      "\n",
      "===[ 26 / 100 ] 125\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Adaptive indexing is characterized by the partial creation and refinement of the index as side effects of query execution. Dynamic or shifting workloads may benefit from preliminary index structures focused on the columns and specific key ranges actually queried---without incurring the cost of full index construction. The costs and benefits of adaptive indexing techniques should therefore be compared in terms of initialization costs, the overhead imposed upon queries, and the rate at which the index converges to a state that is fully-refined for a particular workload component.</div><div class=\"gsh_csp\">Based on an examination of database cracking and adaptive merging, which are two techniques for adaptive indexing, we seek a hybrid technique that has a low initialization cost and also converges rapidly. We find the strengths and weaknesses of database cracking and adaptive merging complementary. One has a relatively high initialization …</div></div></div>,\n",
      " 'author': 'Stratos Idreos and Stefan Manegold and Harumi Kuno and Goetz '\n",
      "           'Graefe',\n",
      " 'eprint': 'https://stratos.seas.harvard.edu/files/AdaptiveIndexingInMainMemoryColumnStores.pdf',\n",
      " 'journal': 'Proceedings of the VLDB Endowment',\n",
      " 'number': '9',\n",
      " 'pages': '586-597',\n",
      " 'publisher': 'VLDB Endowment',\n",
      " 'title': \"Merging what's cracked, cracking what's merged: adaptive indexing \"\n",
      "          'in main-memory column-stores',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=2002944',\n",
      " 'volume': '4',\n",
      " 'year': 2011}\n",
      "\n",
      "===[ 27 / 100 ] 122\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Most commercial database systems do (or should) exploit many sorting techniques that are publicly known, but not readily available in the research literature. These techniques improve both sort performance on modern computer systems and the ability to adapt gracefully to resource fluctuations in multiuser operations. This survey collects many of these techniques for easy reference by students, researchers, and product developers. It covers in-memory sorting, disk-based external sorting, and considerations that apply specifically to sorting in database systems.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://lgis.informatik.uni-kl.de/archiv/wwwdvs.informatik.uni-kl.de/courses/DBSREAL/SS2005/Vorlesungsunterlagen/Implementing_Sorting.pdf',\n",
      " 'journal': 'ACM Computing Surveys (CSUR)',\n",
      " 'number': '3',\n",
      " 'pages': '10',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Implementing sorting in database systems',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1132964',\n",
      " 'volume': '38',\n",
      " 'year': 2006}\n",
      "\n",
      "===[ 28 / 100 ] 122\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Although object-oriented database systems offer advantages over relational or record-oriented database systems, such as modeling facilities for complex objects, they are criticized for poor performance and query capabilities on set-oriented applications. The unacceptable performance is due in part to the object-at-a-time processing typically used by object-oriented database systems. We believe that improved performance of object-oriented database systems depends partially on the efficient and selective retrieval of sets of complex objects from secondary storage. In this report, we present the method of complex object retrieval and assembly used in the Volcano query processing system and the Revelation project. We also present experimental results comparing set-oriented versus object-at-a-time complex object assembly.</div></div></div>,\n",
      " 'author': 'Tom Keller and Goetz Graefe and David Maier',\n",
      " 'eprint': 'https://core.ac.uk/download/pdf/54846508.pdf',\n",
      " 'title': 'Efficient Assembly of Complex Objects; CU',\n",
      " 'url': 'https://core.ac.uk/download/pdf/54846508.pdf',\n",
      " 'year': 1990}\n",
      "\n",
      "===[ 29 / 100 ] 106\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">In a relational data warehouse with many tables, the number of possible and promising indexes exceeds human comprehension and requires automatic index tuning. While monitoring and reactive index tuning have been proposed, adaptive indexing focuses on adapting the physical database layout for and by actual queries.</div><div class=\"gsh_csp\">\" Database cracking\" is one such technique. Only if and when a column is used in query predicates, an index for the column is created; and only if and when a key range is queried, the index is optimized for this key range. The effect is akin to a sort that is adaptive and incremental. This sort is, however, very inefficient, particularly when applied on block-access devices. In contrast, traditional index creation sorts data with an efficient merge sort optimized for block-access devices, but it is neither adaptive nor incremental.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Harumi Kuno',\n",
      " 'eprint': 'http://www.icdt.tu-dortmund.de/proceedings/edbticdt2010proc/edbt/papers/p0371-Graefe.pdf',\n",
      " 'pages': '371-381',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Self-selecting, self-tuning, incrementally optimized indexes',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1739087',\n",
      " 'year': 2010}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 30 / 100 ] 102\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">In 1987, Gray and Putzolo presented the five-minute rule, which was reviewed and renewed ten years later in 1997. With the advent of flash memory in the gap between traditional RAM main memory and traditional disk systems, the five-minute rule now applies to large pages appropriate for today's disks and their fast transfer bandwidths, and it also applies to flash disks holding small pages appropriate for their fast access latency.</div><div class=\"gsh_csp\">Flash memory fills the gap between RAM and disks in terms of many metrics: acquisition cost, access latency, transfer bandwidth, spatial density, and power consumption. Thus, within a few years, flash memory will likely be used heavily in operating systems, file systems, and database systems. Research into appropriate system architectures is urgently needed.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.2708&rep=rep1&type=pdf',\n",
      " 'pages': '6',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'The five-minute rule twenty years later, and how flash memory '\n",
      "          'changes the rules',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1363198',\n",
      " 'year': 2007}\n",
      "\n",
      "===[ 31 / 100 ] 99\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">A linked data structure verification system to verify the integrity of at least one linked data structure simultaneously by way of a verification setup phase and an integrity verification phase. Individual nodes are retrieved from a memory device and examined seriatim in optimal memory device location order. Nodes are retrieved and examined in optimal memory device location order for maximum memory device retrieval performance. Expected and/or actual node information about nodes in a given linked data structure are temporarily stored as records in an integrity verification table for only as much time as is necessary to verify any part of the node information prior to excising one or more unnecessary records from the integrity verification table.</div></div></div>,\n",
      " 'title': 'Linked data structure integrity verification system which verifies '\n",
      "          'actual node information with expected node information stored in a '\n",
      "          'table',\n",
      " 'url': 'https://patents.google.com/patent/US6185569B1/en',\n",
      " 'year': 2001}\n",
      "\n",
      "===[ 32 / 100 ] 95\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">An auto-parameterization process transforms a database query into a parameterized basic query form by replacing any constant values in the query with parameters. The auto-parameterization process attempts to generate a safe execution plan from the basic query form if there is currently no such plan available. A safe execution plan is defined as an execution plan that is optimal over a range of values for the parameters. If a safe execution plan can be generated, it is passed for execution, along with the constant values that were present in the query. If a safe execution plan cannot be generated, the auto-parameterization process passes a specific execution plan for execution. The safe execution plan is cached either at the time it is created or at the time it is executed. The cache is searched each time a parameterized basic query plan is generated by the auto-parameterization process. The auto-parameterization …</div></div></div>,\n",
      " 'title': 'Auto-parameterization of database queries',\n",
      " 'url': 'https://patents.google.com/patent/US6356887B1/en',\n",
      " 'year': 2002}\n",
      "\n",
      "===[ 33 / 100 ] 95\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\">Since many existing techniques for exploiting CPU caches in the implementation of B-tree indexes have not been discussed in the literature, most of them are surveyed. Rather than providing a detailed performance evaluation for one or two of them on some specific contemporary hardware, the purpose is to survey and to make widely available this heretofore-folkloric knowledge in order to enable, structure, and hopefully stimulate future research.</div></div>,\n",
      " 'author': 'Goetz Graefe and P-A Larson',\n",
      " 'eprint': 'http://www.cse.unt.edu/~huangyan/6330/Papers/B-tree-Cache-ICDE2001.pdf',\n",
      " 'pages': '349-358',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'B-tree Indexes and CPU Caches',\n",
      " 'url': 'https://ieeexplore.ieee.org/abstract/document/914847/',\n",
      " 'year': 2001}\n",
      "\n",
      "===[ 34 / 100 ] 93\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">The query execution engine in Microsoft SQL Server employs hash-based algorithms for inner and outer joins, semi-joins, set operations (such as intersection), grouping, and duplicate removal. The implementation combines many techniques proposed individually in the research literature but never combined in a single implementation, neither in a product nor in a research prototype. One of the paper’s contributions is a design that cleanly integrates most existing techniques. One technique, however, which we call hash teams and which has previously been described only in vague terms, has not been implemented in prior research or product work. It realizes in hash-based query processing many of the benefits of interesting orderings in sort-based query processing. Moreover, we describe how memory is managed in complex and bushy query evaluation plans with multiple sort and hash operations. Finally, we report on the effectiveness of hashing using two very typical database queries, including the performance effects of hash teams.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Ross Bunker and Shaun Cooper',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.114.3183&rep=rep1&type=pdf',\n",
      " 'journal': 'VLDB',\n",
      " 'pages': '86-97',\n",
      " 'title': 'Hash joins and hash teams in Microsoft SQL Server',\n",
      " 'url': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.114.3183&rep=rep1&type=pdf',\n",
      " 'volume': '98',\n",
      " 'year': 1998}\n",
      "\n",
      "===[ 35 / 100 ] 81\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\">Efficient algorithms for processing large volumes of data are very important both for relational and new object-oriented database systems. Many query-processing operations can be implemented using sort- or hash-based algorithms, e.g. intersections, joins, and duplicate elimination. In the early relational database systems, only sort-based algorithms were employed. In the last decade, hash-based algorithms have gained acceptance and popularity, and are often considered generally superior to sort-based algorithms such as merge-join. In this article, we compare the concepts behind sort- and hash-based query-processing algorithms and conclude that (1) many dualities exist between the two types of algorithms, (2) their costs differ mostly by percentages rather than by factors, (3) several special cases exist that favor one or the other choice, and (4) there is a strong reason why both hash- and sort-based …</div></div>,\n",
      " 'author': 'Goetz Graefe and Ann Linville and Leonard D.  Shapiro',\n",
      " 'eprint': 'https://pdfs.semanticscholar.org/4411/05feafdcd79bc9b80671649291eed49d707a.pdf',\n",
      " 'journal': 'IEEE Transactions on Knowledge and Data Engineering',\n",
      " 'number': '6',\n",
      " 'pages': '934-944',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'Sort vs. hash revisited',\n",
      " 'url': 'https://ieeexplore.ieee.org/abstract/document/334883/',\n",
      " 'volume': '6',\n",
      " 'year': 1994}\n",
      "\n",
      "===[ 36 / 100 ] 78\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">B-trees have been ubiquitous in database management systems for several decades, and they are used in other storage systems as well. Their basic structure and basic operations are well and widely understood including search, insertion, and deletion. Concurrency control of operations in B-trees, however, is perceived as a difficult subject with many subtleties and special cases. The purpose of this survey is to clarify, simplify, and structure the topic of concurrency control in B-trees by dividing it into two subtopics and exploring each of them in depth.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://www.hpl.hp.com/techreports/2010/HPL-2010-9.pdf',\n",
      " 'journal': 'ACM Transactions on Database Systems (TODS)',\n",
      " 'number': '3',\n",
      " 'pages': '16',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'A survey of B-tree locking techniques',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1806908',\n",
      " 'volume': '35',\n",
      " 'year': 2010}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 37 / 100 ] 78\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\">Matching two sets of data items is a fundamental operation required in relational, extensible, and object-oriented database systems alike. However, the pros and cons of sort- and hash-based query evaluation techniques in modern query processing systems are still not fully understood. After our earlier research clarified strengths and weaknesses of sort- and hash-based query processing techniques and suggested remedies for the shortcomings of hash-based algorithms, the present paper outlines a number of further differences between sort-merge-join and hybrid hash join that traditionally have been ignored in such comparisons and render sort-merge-join mostly obsolete. We consolidate old and raise new issues pertinent to the comparison of sort- and hash-based query evaluation techniques and stir some thought and discussion among both academic and industrial database system builders.&lt;&gt;</div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'pages': '406-417',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'Sort-merge-join: An idea whose time has (h) passed?',\n",
      " 'url': 'https://ieeexplore.ieee.org/abstract/document/283062/',\n",
      " 'year': 1994}\n",
      "\n",
      "===[ 38 / 100 ] 77\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">In some applications, data capture dominates query processing. For example, monitoring moving objects often requires more insertions and updates than queries. Data gathering using automated sensors often exhibits this imbalance. More generally, indexing streams is considered an unsolved problem. For those applications, B-tree indexes are good choices if some trade-off decisions are tilted towards optimization of updates rather than towards optimization of queries. This paper surveys some techniques that let B-trees sustain very high update rates, up to multiple orders of magnitude higher than traditional B-trees, at the expense of query processing performance. Not surprisingly, some of these techniques are reminiscent of those employed during index creation, index rebuild, etc., while other techniques are derived from well known technologies such as differential files and log-structured file systems.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://drops.dagstuhl.de/opus/volltexte/2006/763/pdf/05421.GraefeGoetz.Paper.763.pdf',\n",
      " 'journal': 'ACM Sigmod Record',\n",
      " 'number': '1',\n",
      " 'pages': '39-44',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'B-tree indexes for high update rates',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1122002',\n",
      " 'volume': '35',\n",
      " 'year': 2006}\n",
      "\n",
      "===[ 39 / 100 ] 73\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Various technologies and techniques are disclosed for supporting parallel nested transactions in a transactional memory system. Multiple closed nested transactions are created for a single parent transaction, and the closed nested transactions are executed concurrently as parallel nested transactions. Various techniques are used to ensure effects of the parallel nested transactions are hidden from other transactions outside the parent transaction until the parent transaction commits. For example, versioned write locks are used with parallel nested transactions. When a transactional memory word changes from a write lock to a versioned write lock, an entry is made in a global versioned write lock map to store a pointer to a write log entry that the versioned write lock replaced. When the versioned write lock is encountered during transaction processing, the global versioned write lock map is consulted to translate the …</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/a7/03/68/e8463c7685a408/US7890472.pdf',\n",
      " 'title': 'Parallel nested transactions in transactional memory',\n",
      " 'url': 'https://patents.google.com/patent/US7890472B2/en',\n",
      " 'year': 2011}\n",
      "\n",
      "===[ 40 / 100 ] 72\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">As access times to main memory and disks continue to diverge, faster non-volatile storage technologies become more attractive for speeding up data analysis applications. NAND flash is one such promising substitute for disks. Flash offers faster random reads than disk, consumes less power than disk, and is cheaper than DRAM. In this paper, we investigate alternative data layouts and join algorithms suited for systems that use flash drives as the non-volatile store.</div><div class=\"gsh_csp\">All of our techniques take advantage of the fast random reads of flash. We convert traditional sequential I/O algorithms to ones that use a mixture of sequential and random I/O to process less data in less time. Our measurements on commodity flash drives show that a column-major layout of data pages is faster than a traditional row-based layout for simple scans. We present a new join algorithm, RARE-join, designed for a column-based page layout on …</div></div></div>,\n",
      " 'author': 'Mehul A Shah and Stavros Harizopoulos and Janet L Wiener and Goetz '\n",
      "           'Graefe',\n",
      " 'eprint': 'http://idke.ruc.edu.cn/people/dazhou/Papers/p17-shah.pdf',\n",
      " 'pages': '17-24',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Fast scans and joins using flash drives',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1457154',\n",
      " 'year': 2008}\n",
      "\n",
      "===[ 41 / 100 ] 72\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">The goal of the Volcano project is to provide efficient, extensible tools for algebraic query and request processing in novel application domains, in particular for object-oriented and scientific database systems. At this point, the Volcano project provides data model-independent and architecture-independent tools for optimized parallel query processing over large data sets using multiple operators or data processing steps. This overview describes the new rulebased Volcano Optimizer Generator and Volcano's architecture-independent parallel processing capabilities.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Richard L Cole and Diane L Davison and William J '\n",
      "           'McKenna and Richard H Wolniewicz',\n",
      " 'publisher': 'University of Colorado, Boulder, Department of Computer Science',\n",
      " 'title': 'Extensible query optimization and parallel execution in Volcano',\n",
      " 'url': 'http://scholar.google.com/scholar?cluster=14841949345790812944&hl=en&oi=scholarr',\n",
      " 'year': 1991}\n",
      "\n",
      "===[ 42 / 100 ] 71\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Database software can be tailored for specific application domains and their required functionality, for specific hardware and its characteristics, or for other purposes. This brief paper collects issues and techniques required or desirable for making a server-class database management system energy-efficient. The opportunities go far beyond cost functions and general performance improvements. Topics include additional layers in the memory hierarchy, I/O optimizations, data format, scheduling, adaptive query plan execution, and self management in novel ways.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://www.cs.ovgu.de/inf_media/downloads/forschung/technical_reports_und_preprints/2008/TechReport1-EGOTEC-23e94.pdf#page=32',\n",
      " 'pages': '24-28',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Database servers tailored to improve energy efficiency',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1385494',\n",
      " 'year': 2008}\n",
      "\n",
      "===[ 43 / 100 ] 67\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">A method for implementing adaptive merging in database indexes includes selecting a key range from a database query having a range predicate and searching a database for data matching the key range. The data matching the key range is merged to form a collected dataset which is stored for future retrieval. A method for optimizing B-tree representation of a database using actual queries is also provided.</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/48/84/a0/00096d1c2afb9d/US9298761.pdf',\n",
      " 'title': 'Adaptive merging in database indexes',\n",
      " 'url': 'https://patents.google.com/patent/US9298761B2/en',\n",
      " 'year': 2016}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 44 / 100 ] 64\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"> <div class=\"gsh_csp\">We are exploring a scheme that allows optimizing queries over object-oriented databases with encapsulated behavior. Objects and classes will be able to reveal their behavior in terms of expressions in an algebraic language interpreted by a structural object-oriented database system. An object or class can agree or refuse to reveal its behavior. The structural algebra is richer than relational algebra as it includes operators on complex object collections, and updates and traversals of individual objects. Objects may reveal to the optimizer the structural access paths used by their procedures or cost and other statistics useful for query optimization. The main features of our approach is that the object-oriented user interface language is able to perform general computation and to preserve the encapsulation envelope around classes and types.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and David Maier',\n",
      " 'pages': '358-363',\n",
      " 'publisher': 'Springer, Berlin, Heidelberg',\n",
      " 'title': 'Query optimization in object-oriented database systems: a '\n",
      "          'prospectus',\n",
      " 'url': 'https://link.springer.com/chapter/10.1007/3-540-50345-5_36',\n",
      " 'year': 1988}\n",
      "\n",
      "===[ 45 / 100 ] 63\n",
      "{'author': 'Michael J Carey and David J DeWitt and Goetz Graefe and Dvaid M '\n",
      "           'Haight and Joel E Richardson and Daniel T Schuh and Eugene J '\n",
      "           'Shekita and Scott L Vandenberg',\n",
      " 'publisher': 'Morgan Kaufmann Publishers Inc., San Francisco, CA',\n",
      " 'title': 'The EXODUS extensible DBMS project: an overview, Readings in '\n",
      "          'object-oriented database systems',\n",
      " 'url': 'http://scholar.google.com/scholar?cluster=4224592327467978374&hl=en&oi=scholarr',\n",
      " 'year': 1989}\n",
      "\n",
      "===[ 46 / 100 ] 62\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">A method of satisfying a database query includes evaluating certain joins on a per-segment basis. An expression tree is produced for the query, and the expression tree is evaluated to identify joins whose operands are two instances of the same relation and whose join predicate conjunctively includes an equality comparison between two instances of the same column. When such a join is identified, it may be evaluated by segmenting the operand relation according to the columns that are compared for equality in the predicate. The join is then evaluated by performing the join operation separately on each segment. Segments may be spooled separately, thereby exploiting the efficiencies obtained by spooling even where the entire relation is too large to fit in the spool. Execution iterators are provided for spooling successive segments and for applying the join to the spooled segment.</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/7c/17/9f/7b26088c005443/US7599953.pdf',\n",
      " 'title': 'System and method for segmented evaluation of database queries',\n",
      " 'url': 'https://patents.google.com/patent/US7599953B2/en',\n",
      " 'year': 2009}\n",
      "\n",
      "===[ 47 / 100 ] 59\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">If replacement selection is used in an external mergesort to generate initial runs, individual records are deleted and inserted in the sort operation's workspace. Variable-length records introduce the need for possibly complex memory management and extra copying of records. As a result, few systems employ replacement selection, even though it produces longer runs than commonly used algorithms. We experimentally compared several algorithms and variants for managing this workspace. We found that the simple best fit algorithm achieves memory utilization of 90% or better and run lengths over 1.8 times workspace size, with no extra copying of records and very little other overhead, for widely varying record sizes and for a wide range of memory sizes. Thus, replacement selection is a viable algorithm for commercial database systems, even for variable-length records. Efficient memory management also enables …</div></div></div>,\n",
      " 'author': 'Per-Åke Larson and Goetz Graefe',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.54.687&rep=rep1&type=pdf',\n",
      " 'number': '2',\n",
      " 'pages': '472-483',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Memory management during run generation in external sorting',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=276346',\n",
      " 'volume': '27',\n",
      " 'year': 1998}\n",
      "\n",
      "===[ 48 / 100 ] 57\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Computer-implemented systems and associated operating methods implement a fast join for databases which is adapted for usage with flash storage. A system comprises a processor that performs a join of two tables stored in a storage in pages processed in a column orientation wherein column values for all rows on a page are co-located in mini-pages within the page. The processor reduces input/output operations of the join by accessing only join columns and mini-pages containing join results.</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/9d/53/86/4d1d94ee6cef6b/US9176860.pdf',\n",
      " 'title': 'Database join optimized for flash storage',\n",
      " 'url': 'https://patents.google.com/patent/US9176860B2/en',\n",
      " 'year': 2015}\n",
      "\n",
      "===[ 49 / 100 ] 55\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Universal quantification is not supported directly in most database systems despite the fact that it adds significant power to a system's query processing and inference capabilities, in particular for the analysis of many-to-many relationships and of set-valued attributes. One of the main reasons for this omission has been that universal quantification algorithms and their performance have not been explored for large databases. In this article, we describe and compare three known algorithms and one recently proposed algorithm for relational division, the algebra operator that embodies universal quantification. For each algorithm, we investigate the performance effects of explicit duplicate removal and referential integrity enforcement, variants for inputs larger than memory, and parallel execution strategies. Analytical and experimental performance comparisons illustrate the substantial differences among the algorithms …</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Richard L Cole',\n",
      " 'eprint': 'https://research.tableau.com/sites/default/files/p187-graefe.pdf',\n",
      " 'journal': 'ACM Transactions on Database Systems (TODS)',\n",
      " 'number': '2',\n",
      " 'pages': '187-236',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Fast algorithms for universal quantification in large databases',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=210202',\n",
      " 'volume': '20',\n",
      " 'year': 1995}\n",
      "\n",
      "===[ 50 / 100 ] 54\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\">Adaptive indexing schemes such as database cracking and adaptive merging have been investigated to-date only in the context of range queries. These are typical for non-key columns in relational databases. For complete self-managing indexing, adaptive indexing must also apply to key columns. The present paper proposes a design and offers a first performance evaluation in the context of keys. Adaptive merging for keys also enables further improvements in B-tree indexes. First, partitions can be matched to levels in the memory hierarchy such as a CPU cache and an in-memory buffer pool. Second, adaptive merging in merged B-trees enables automatic master-detail clustering.</div></div>,\n",
      " 'author': 'Goetz Graefe and Harumi Kuno',\n",
      " 'eprint': 'https://www.hpl.hp.com/techreports/2010/HPL-2010-23.pdf',\n",
      " 'pages': '69-74',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'Adaptive indexing for relational keys',\n",
      " 'url': 'https://ieeexplore.ieee.org/abstract/document/5452743/',\n",
      " 'year': 2010}\n",
      "\n",
      "===[ 51 / 100 ] 52\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">1.1. Extensible Database Systems In recent years, a number of new data models have been proposed in the database literature that extend the modelling and query processing power of the relational model (Codd, ısro) For business style applications, the relational data model has been used very successfully. Services provided by relational database systems have significantly enhanced programmer and data processing productivity. Such services include high level query and data manipulation languages, transparent maintenance of secondary search structures, data independence, data sharing, control of access privileges, concurrency control, and recovery from program, media, and system failures. While the relation-tuple paradigm with fixedformat records provides sufficient modelling flexibility for most record-keeping applications, it is not well-suited for other database application areas which are starting to …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://minds.wisconsin.edu/bitstream/handle/1793/58884/TR724.pdf?sequence=1',\n",
      " 'publisher': 'University of Wisconsin-Madison Department of Computer Sciences',\n",
      " 'title': 'Rule-based query optimization in extensible database systems',\n",
      " 'url': 'https://minds.wisconsin.edu/bitstream/handle/1793/58884/TR724.pdf?sequence=1',\n",
      " 'year': 1987}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 52 / 100 ] 51\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">We propose a new framework for resource allocation based on concepts from microeconomics. Specifically, we address the difficult problem of managing resources in a multiple-query environment composed of queries with widely varying resource requirements. The central element of the framework is a resource broker that realizes a profit by\" selling\" resources to competing operators using a performance-based\" currency.\" The guiding principle for brokering resources is profit maximization. In other words, since the currency is derived from the performance objective, the broker can achieve the best performance by making the scheduling and resource allocation decisions that maximize profit. Moreover, the broker employs dynamic techniques and adapts by changing previous allocation decisions while queries are executing. In a first validation study of the framework, we developed a prototype broker that manages …</div></div></div>,\n",
      " 'author': 'Diane L Davison and Goetz Graefe',\n",
      " 'number': '2',\n",
      " 'pages': '281-292',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Dynamic resource brokering for multi-user query execution',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=223845',\n",
      " 'volume': '24',\n",
      " 'year': 1995}\n",
      "\n",
      "===[ 53 / 100 ] 50\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Foster B-trees are a new variant of B-trees that combines advantages of prior B-tree variants optimized for many-core processors and modern memory hierarchies with flash storage and nonvolatile memory. Specific goals include:(i) minimal concurrency control requirements for the data structure,(ii) efficient migration of nodes to new storage locations, and (iii) support for continuous and comprehensive self-testing. Like B link-trees, Foster B-trees optimize latching without imposing restrictions or specific designs on transactional locking, for example, key range locking. Like write-optimized B-trees, and unlike B link-trees, Foster B-trees enable large writes on RAID and flash devices as well as wear leveling and efficient defragmentation. Finally, they support continuous and inexpensive yet comprehensive verification of all invariants, including all cross-node invariants of the B-tree structure. An implementation and a …</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Hideaki Kimura and Harumi Kuno',\n",
      " 'journal': 'ACM Transactions on Database Systems (TODS)',\n",
      " 'number': '3',\n",
      " 'pages': '17',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'Foster B-trees',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=2338630',\n",
      " 'volume': '37',\n",
      " 'year': 2012}\n",
      "\n",
      "===[ 54 / 100 ] 50\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Portions of a B-tree index in a database are locked for concurrency control. In one example, hierarchical lock modes are provided that permit locking a key, a gap between the key and the next key, and a combination of the key and the gap. In another example, key range locking may be applied to the B-tree index using locks on separator keys of index nodes. In another example, key range locking may be applied to the B-tree index using locks on key prefixes.</div></div></div>,\n",
      " 'title': 'Hierarchical locking in B-tree indexes',\n",
      " 'url': 'https://patents.google.com/patent/US7577658B2/en',\n",
      " 'year': 2009}\n",
      "\n",
      "===[ 55 / 100 ] 49\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Adaptive indexing initializes and optimizes indexes incrementally, as a side effect of query processing. The goal is to achieve the benefits of indexes while hiding or minimizing the costs of index creation. However, index-optimizing side effects seem to turn read-only queries into update transactions that might, for example, create lock contention. This paper studies concurrency control in the context of adaptive indexing. We show that the design and implementation of adaptive indexing rigorously separates index structures from index contents; this relaxes the constraints and requirements during adaptive indexing compared to those of traditional index updates. Our design adapts to the fact that an adaptive index is refined continuously, and exploits any concurrency opportunities in a dynamic way. A detailed experimental analysis demonstrates that (a) adaptive indexing maintains its adaptive properties even when running concurrent queries,(b) adaptive indexing can exploit the opportunity for parallelism due to concurrent queries,(c) the number of concurrency conflicts and any concurrency administration overheads follow an adaptive behavior, decreasing as the workload evolves and adapting to the workload needs.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Felix Halim and Stratos Idreos and Harumi Kuno '\n",
      "           'and Stefan Manegold',\n",
      " 'eprint': 'https://arxiv.org/pdf/1203.6405',\n",
      " 'journal': 'arXiv preprint arXiv:1203.6405',\n",
      " 'title': 'Concurrency control for adaptive indexing',\n",
      " 'url': 'https://arxiv.org/abs/1203.6405',\n",
      " 'year': 2012}\n",
      "\n",
      "===[ 56 / 100 ] 48\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\">Emerging database application domains demand not only high functionality, but also high performance. To satisfy these two requirements, the Volcano query execution engine combines the efficient use of parallelism on a wide variety of computer architectures with an extensible set of query processing operators that can be nested into arbitrarily complex query evaluation plans. Volcano's novel exchange operator permits designing, developing, debugging, and tuning data manipulation operators in single-process environments but executing them in various forms of parallelism. The exchange operator shields the data manipulation operators from all parallelism issues. The design and implementation of the generalized exchange operator are examined. The authors justify their decision to support hierarchical architectures and argue that the exchange operator offers a significant advantage for development and …</div></div>,\n",
      " 'author': 'Goetz Graefe and Diane L Davison',\n",
      " 'journal': 'IEEE Transactions on Software Engineering',\n",
      " 'number': '8',\n",
      " 'pages': '749-764',\n",
      " 'publisher': 'IEEE',\n",
      " 'title': 'Encapsulation of parallelism and architecture-independence in '\n",
      "          'extensible database query execution',\n",
      " 'url': 'https://ieeexplore.ieee.org/abstract/document/238579/',\n",
      " 'volume': '19',\n",
      " 'year': 1993}\n",
      "\n",
      "===[ 57 / 100 ] 47\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">When a working set fits into memory, the overhead imposed by the buffer pool renders traditional databases non-competitive with in-memory designs that sacrifice the benefits of a buffer pool. However, despite the large memory available with modern hardware, data skew, shifting workloads, and complex mixed workloads make it difficult to guarantee that a working set will fit in memory. Hence, some recent work has focused on enabling in-memory databases to protect performance when the working data set almost fits in memory. Contrary to those prior efforts, we enable buffer pool designs to match in-memory performance while supporting the\" big data\" workloads that continue to require secondary storage, thus providing the best of both worlds. We introduce here a novel buffer pool design that adapts pointer swizzling for references between system objects (as opposed to application objects), and uses it to …</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Haris Volos and Hideaki Kimura and Harumi Kuno '\n",
      "           'and Joseph Tucek and Mark Lillibridge and Alistair Veitch',\n",
      " 'eprint': 'https://ai.google/research/pubs/pub43985.pdf',\n",
      " 'journal': 'Proceedings of the VLDB Endowment',\n",
      " 'number': '1',\n",
      " 'pages': '37-48',\n",
      " 'publisher': 'VLDB Endowment',\n",
      " 'title': 'In-memory performance for big data',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=2735465',\n",
      " 'volume': '8',\n",
      " 'year': 2014}\n",
      "\n",
      "===[ 58 / 100 ] 46\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Y</div><div class=\"gsh_csp\">50 communications of the acm| july 2009| vol. 52| no. 7 practice latency, transfer bandwidth, spatial density, power consumption, and cooling costs. 13 Table 1 and some derived metrics in Table 2 illustrate this point (all metrics derived on 4/11/2007 from dramexchange. com, dvnation. com, buy. com, seagate. com, and samsung. com). Given the number of CPU instructions possible during the time required for one disk I/O has steadily increased, an intermediate memory in the storage hierarchy is desirable. Flash memory seems to be a highly probable candidate, as has been observed many times by now.</div><div class=\"gsh_csp\">Many architecture details remain to be worked out. For example, in the hardware architecture, will flash memory be accessible via a DIMM slot, a SATA (serial ATA) disk interface, or yet another hardware interface? Given the effort and delay in defining a new hardware interface, adaptations of existing interfaces are likely. A major question is whether flash memory is considered a special part of either main memory or persistent storage. Asked differently: if a system includes 1GB traditional RAM, 8GB flash memory, and 250GB traditional disk, does the software treat it as</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.227.3846&rep=rep1&type=pdf',\n",
      " 'journal': 'Communications of the ACM',\n",
      " 'number': '7',\n",
      " 'pages': '48-59',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'The five-minute rule 20 years later (and how flash memory changes '\n",
      "          'the rules)',\n",
      " 'url': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.227.3846&rep=rep1&type=pdf',\n",
      " 'volume': '52',\n",
      " 'year': 2009}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 59 / 100 ] 45\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Although scientific data analysis increasingly requires access and manipulation of large quantities of data, current database technology fails to meet the needs of scientific processing in a number of areas. To overcome acceptance problems among scientific users, database systems must provide performance and functionality comparable to current combinations of scientific programs and file systems. Therefore, we propose extending the concept of a database query to include numeric computation over scientific databases.</div><div class=\"gsh_csp\">In this paper, we examine the specification of an integrated algebra that includes traditional database operators for pattern matching and search as well as numeric operators for scientific data sets. Through the use of a single integrated algebra, we can perform automatic optimization on scientific computations, realizing all of the traditional benefits of optimization. We have experimented with a prototype optimizer which integrates sets, time series and spectra data types and operators on those types. Our results demonstrate that scientific database computations using numeric operators on multiple data types can’be effectively optimized and permit performance gains that could not be realized without the integration,</div></div></div>,\n",
      " 'author': 'Richard H Wolniewicz and Goetz Graefe',\n",
      " 'eprint': 'http://www.academia.edu/download/30330750/algebraic_optimization_of_computations_o_1200893.pdf',\n",
      " 'journal': 'IEEE Data Eng. Bull.',\n",
      " 'number': '1',\n",
      " 'pages': '48-51',\n",
      " 'title': 'Algebraic optimization of computations over scientific databases',\n",
      " 'url': 'http://www.academia.edu/download/30330750/algebraic_optimization_of_computations_o_1200893.pdf',\n",
      " 'volume': '16',\n",
      " 'year': 1993}\n",
      "\n",
      "===[ 60 / 100 ] 43\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">B-trees have been ubiquitous in database management systems for several decades, and they serve in many other storage systems as well. Their basic structure and their basic operations are well understood including search, insertion, and deletion. However, implementation of transactional guarantees such as all-or-nothing failure atomicity and durability in spite of media and system failures seems to be difficult. High-performance techniques such as pseudo-deleted records, allocation-only logging, and transaction processing during crash recovery are widely used in commercial B-tree implementations but not widely understood. This survey collects many of these techniques as a reference for students, researchers, system architects, and software developers. Central in this discussion are physical data independence, separation of logical database contents and physical representation, and the concepts of user …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'journal': 'ACM Transactions on Database Systems (TODS)',\n",
      " 'number': '1',\n",
      " 'pages': '1',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'A survey of B-tree logging and recovery techniques',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=2109197',\n",
      " 'volume': '37',\n",
      " 'year': 2012}\n",
      "\n",
      "===[ 61 / 100 ] 42\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Various technologies and techniques are disclosed that support buffered writes and enforced serialization order in a software transactional memory system. A buffered write process is provided that performs writes to shadow copies of objects and writes content back to the objects after validating a respective transaction during commit. When a write lock is first obtained for a particular transaction, a shadow copy is made of a particular object. Writes are performed to and reads from the shadow copy. After validating the particular transaction during commit, content is written from the shadow copy to the particular object. A transaction ordering process is provided that ensures that an order in which the transactions are committed matches an abstract serialization order of the transactions. Transactions are not allowed to commit until their ticket number matches a global number that tracks the next transaction that should …</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/e1/10/ac/ea6f936786e3fe/US7908255.pdf',\n",
      " 'title': 'Transactional memory using buffered writes and enforced '\n",
      "          'serialization order',\n",
      " 'url': 'https://patents.google.com/patent/US7908255B2/en',\n",
      " 'year': 2011}\n",
      "\n",
      "===[ 62 / 100 ] 40\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"> <div class=\"gsh_csp\">Ideally, realizing the best physical design for the current and all subsequent workloads would impact neither performance nor storage usage. In reality, workloads and datasets can change dramatically over time and index creation impacts the performance of concurrent user and system activity. We propose a framework that evaluates the key premise of adaptive indexing — a new indexing paradigm where index creation and re-organization take place automatically and incrementally, as a side-effect of query execution. We focus on how the incremental costs and benefits of dynamic reorganization are distributed across the workload’s lifetime. We believe measuring the costs and utility of the stages of adaptation are relevant metrics for evaluating new query processing paradigms and comparing them to traditional approaches.</div></div></div>,\n",
      " 'author': 'Goetz Graefe and Stratos Idreos and Harumi Kuno and Stefan '\n",
      "           'Manegold',\n",
      " 'eprint': 'http://scholar.harvard.edu/files/tpctc2010.pdf',\n",
      " 'pages': '169-184',\n",
      " 'publisher': 'Springer, Berlin, Heidelberg',\n",
      " 'title': 'Benchmarking adaptive indexing',\n",
      " 'url': 'https://link.springer.com/chapter/10.1007/978-3-642-18206-8_13',\n",
      " 'year': 2010}\n",
      "\n",
      "===[ 63 / 100 ] 39\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Recent performance improvements in storage hardware have benefited bandwidth much more than latency. Among other implications, this trend favors large B-tree pages. Recent performance improvements in processor hardware also have benefited processing bandwidth much more than memory latency. Among other implications, this trend favors adding calculations if they save cache faults. With small calculations guiding the search directly to the desired key, interpolation search complements these trends much better than binary search. It performs well if the distribution of key values is perfectly uniform, but it can be useless and even wasteful otherwise. This paper collects and describes more than a dozen techniques for interpolation search in B-tree indexes. Most of them attempt to avoid skew or to detect skew very early and then to avoid its bad effects. Some of these methods are part of the folklore of B-tree …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.2044&rep=rep1&type=pdf',\n",
      " 'pages': '5',\n",
      " 'publisher': 'ACM',\n",
      " 'title': 'B-tree indexes, interpolation search, and skew',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=1140409',\n",
      " 'year': 2006}\n",
      "\n",
      "===[ 64 / 100 ] 38\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Optimization of nested queries, in particular finding equivalent “flattened” queries for queries that employ the SQL sub-query construct, has been researched extensively. In contrast, with the exception of nested loops join, execution of nested plans has found little interest. Nested execution plans may result from a failure to flatten nested SQL expressions but just as likely are created by a query optimizer to exploit all available indexes as effectively as possible. In fact, if materialized views and index tuning perform as expected, few queries should require large operations such as parallel scans, sorts and hash joins, and most actual query plans will rely entirely on navigating indexes on tables and views. Note that only index navigation plans scale truly gracefully, ie, perform equally well on large and on very large databases, whereas sorting and hashing scale at best linearly. Since a typical index navigation plan employs nested iteration, this paper describes techniques to execute such plans efficiently as well as means to cleanly implement these techniques. Taken together, these techniques can improve query performance by orders of magnitude, giving them obvious practical importance.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2004/Papers/graefe-nested-btw2003.pdf',\n",
      " 'journal': 'BTW',\n",
      " 'pages': '58-77',\n",
      " 'title': 'Executing Nested Queries.',\n",
      " 'url': 'http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2004/Papers/graefe-nested-btw2003.pdf',\n",
      " 'volume': '26',\n",
      " 'year': 2003}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 65 / 100 ] 37\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Fluctuations in memory contention during query execution may compromise the effectiveness of previous allocation decisions and result in excessive I/O costs. In order to maximize system per-formance, memory-intensive algorithms such as hash join must gracefully adapt to variations in available memory. Responsiveness to memory contention is particularly important in systems processing mixed workloads due to the erratic frequency and magnitude of ﬂuctuations. Earlier studies on adaptable hash joins have advocated lowering I/O costs by reducing the volume, or number of pages, of I/O performed. In this paper, we present a group of memory-contention responsive hash joins that lower I/O costs by using a large unit of I/O, or cluster, to reduce the amount of time spent on I/() and that dynamically vary the cluster size in response to ﬂuctuations in memory availability. These techniques are effective for static as well …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://scholar.colorado.edu/cgi/viewcontent.cgi?article=1651&context=csci_techreports',\n",
      " 'title': 'Memory-Contention Responsive Hashjoins; CU-CS-682-93',\n",
      " 'url': 'http://scholar.colorado.edu/cgi/viewcontent.cgi?article=1651&context=csci_techreports',\n",
      " 'year': 1994}\n",
      "\n",
      "===[ 66 / 100 ] 37\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">We add yet another paper on parallel sorting to the large body of literature on the topic. We briefly survey a new query evaluation system called Volcano developed for database systems research and education, and then focus on Volcano's sort algorithms. Volcano's single-process sort algorithm has several interesting features that make it quite efficient. Volcano's flexible multi-processing architecture provides efficient mechanisms for single-and multi-input and for single-and multi-output sort operations, in any combination. We report experimental performance results sorting medium size and large files on a shared-memory machine.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://pdfs.semanticscholar.org/e8b2/12763694ec86b85e449d745b6daa92a3630c.pdf',\n",
      " 'publisher': 'University of Colorado, Boulder, Department of Computer Science',\n",
      " 'title': 'Parallel external sorting in Volcano',\n",
      " 'url': 'https://pdfs.semanticscholar.org/e8b2/12763694ec86b85e449d745b6daa92a3630c.pdf',\n",
      " 'year': 1990}\n",
      "\n",
      "===[ 67 / 100 ] 36\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"> <div class=\"gsh_csp\">In previous work, we demonstrated the advantages of encapsulating query evaluation algorithms as ‘iterators’ for sequential and parallel query evaluation. Unfortunately, those earlier models have a severe drawback with respect to resource allocation in distributed‐memory systems. Since threads may be initiated long before they actually perform useful work, thread placement decisions may be suboptimal. In this paper, we briefly review the iterator model and then extend it to support bottom‐up, just‐in‐time activation of appropriate query plan fragments as well as local and global synchronization and communication among sibling threads. Some of the algorithms described here may seem intricate; however, the intricacy is encapsulated entirely in the parallelism or ‘exchange’ iterator, thus freeing developers of data manipulation iterators to focus on the specific algorithms at hand, instead of on mechanisms for …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'journal': 'Software: Practice and Experience',\n",
      " 'number': '4',\n",
      " 'pages': '427-452',\n",
      " 'publisher': 'John Wiley & Sons, Ltd.',\n",
      " 'title': 'Iterators, Schedulers, and Distributed‐memory Parallelism',\n",
      " 'url': 'https://onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1097-024X(199604)26:4%3C427::AID-SPE20%3E3.0.CO;2-H',\n",
      " 'volume': '26',\n",
      " 'year': 1996}\n",
      "\n",
      "===[ 68 / 100 ] 35\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">The memory management system is operational in a database system and functions to gracefully transition data from the allocated memory space to run files on disk only as needed. The memory management system accommodates variable length input records in the workspace of a database sort operation, requires no extra copying of records in memory, and maintains memory utilization at a high level. This memory management system therefore minimizes the amount of data written to disk during run formation and enables the use of the replacement selection algorithm even with variable length input records, which improves performance of sorting and overall operational efficiency of the database system.</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/4e/cf/93/5c264bf1c46576/US6105024.pdf',\n",
      " 'title': 'System for memory management during run formation for external '\n",
      "          'sorting in database system',\n",
      " 'url': 'https://patents.google.com/patent/US6105024A/en',\n",
      " 'year': 2000}\n",
      "\n",
      "===[ 69 / 100 ] 35\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Purpose: In database query processing, optimization is commonly regarded as “the hard” part, whether in relational, post-relational, object-oriented, textual-spatial-temporal, federated or web-based database systems. Query execution, on the other hand, is considered a mostly straightforward exercise in algorithm implementation, with the currently “hot” twist to consider CPU caches. There is, however, a third piece to the puzzle, namely physical database design, eg, the set of useful indexes. The purpose of this brief paper is to tie these three pieces together and to encourage students and researchers to adopt a broader perspective of adaptive query processing. Another purpose is to present some contrarian viewpoints about interesting and worthwhile research topics.</div><div class=\"gsh_csp\">Compilation effort: It is well known that compilation and early binding are good ideas, where they can be applied. Thus, compile-time optimization wins over interpretation, in particular for repetitive query execution, which typically is the bulk of database activity. On the other hand, query optimization based on incomplete information often results in plans that perform poorly in many invocations. The Achilles heel of query optimization is selectivity estimation; thus, missing statistics such as histograms and counts of unique values are a perennial worry for database administrators, unless the DBMS automatically creates, refreshes, and drops statistics as appropriate. The other important source of selectivity estimation errors is lacking compile-time knowledge about run-time parameter values. Traditionally, the only remedy has been run-time optimization. Early research into dynamic or …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://diaswww.epfl.ch/courses/adms07/papers/A00JUN-CD.pdf#page=5',\n",
      " 'journal': 'IEEE Data Eng. Bull.',\n",
      " 'number': '2',\n",
      " 'pages': '3-6',\n",
      " 'title': 'Dynamic query evaluation plans: some course corrections?',\n",
      " 'url': 'http://diaswww.epfl.ch/courses/adms07/papers/A00JUN-CD.pdf#page=5',\n",
      " 'volume': '23',\n",
      " 'year': 2000}\n",
      "\n",
      "===[ 70 / 100 ] 35\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\">Help Design Your New ACM Digital Library. We're upgrading the ACM DL, and would like your\n",
      "input. Please sign up to review new features, functionality and page designs. Leave an email\n",
      "address: or Follow @ACMDL. or [Not interested]. Google, Inc. (search) … \n",
      "</div>,\n",
      " 'author': 'Scott Daniels and Goetz Graefe and Thomas Keller and David Maier '\n",
      "           'and Duri Schmidt and Bennet Vance',\n",
      " 'journal': 'Data Engineering',\n",
      " 'number': '2',\n",
      " 'pages': '58-62',\n",
      " 'publisher': 'IEEE Computer Society Press',\n",
      " 'title': 'Query optimization in revelation, an overview',\n",
      " 'url': 'https://dl.acm.org/citation.cfm?id=108931',\n",
      " 'volume': '14',\n",
      " 'year': 1991}\n",
      "\n",
      "===[ 71 / 100 ] 34\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Computer-implemented systems and associated operating methods use atomic query maps to identify and evaluate database query plan robustness landmarks. The computer-implemented system comprises logic that evaluates robustness of a selected atomic query by measuring performance of atomic query execution in a predetermined range of runtime conditions that include data characteristics. The logic produces a set of measurements that can be displayed as one or more performance maps and analyzes the measured performance to identify landmarks indicative of database atomic query performance degradation greater than a predetermined amount.</div></div></div>,\n",
      " 'eprint': 'https://patentimages.storage.googleapis.com/51/c3/85/ce5825044a434a/US8572068.pdf',\n",
      " 'title': 'Evaluation of set of representative query performance using '\n",
      "          'robustness mapping',\n",
      " 'url': 'https://patents.google.com/patent/US8572068B2/en',\n",
      " 'year': 2013}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[ 72 / 100 ] 34\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"> <div class=\"gsh_csp\">Traditional database query processing relies on three types of algorithms for join and for grouping operations. For joins, index nested loops join exploits an index on its inner input, merge join exploits sorted inputs, and hash join exploits differences in the sizes of the join inputs. For grouping, an index-based algorithm has been used in the past whereas today sort- and hash-based algorithms prevail. Cost-based query optimization chooses the most appropriate algorithm for each query and for each operation. Unfortunately, mistaken algorithm choices during compile-time query optimization are common yet expensive to investigate and to resolve.</div> <div class=\"gsh_csp\">Our goal is to end mistaken choices among join algorithms and among grouping algorithms by replacing the three traditional types of algorithms with a single one. Like merge join, this new join algorithm exploits sorted inputs. Like hash join, it exploits …</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://wwwlgis.informatik.uni-kl.de/cms/fileadmin/users/haerder/2011/JoinAndGrouping.pdf',\n",
      " 'journal': 'Computer Science-Research and Development',\n",
      " 'number': '1',\n",
      " 'pages': '3-27',\n",
      " 'publisher': 'Springer-Verlag',\n",
      " 'title': 'New algorithms for join and grouping operations',\n",
      " 'url': 'https://link.springer.com/article/10.1007/s00450-011-0186-9',\n",
      " 'volume': '27',\n",
      " 'year': 2012}\n",
      "\n",
      "===[ 73 / 100 ] 33\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">Three designs of hierarchical locking suitable for B-tree indexes are explored in detail and their advantages and disadvantages compared. Traditional hierarchies include index, leaf page, and key range or key value. Alternatively, locks on separator keys in interior B-tree pages can protect key ranges of different sizes. Finally, for keys consisting of multiple columns, key prefixes of different sizes permit a third form of hierarchical locking. Each of these approaches requires appropriate implementation techniques. The techniques explored here include node splitting and merging, lock escalation and lock de-escalation, and online changes in the granularity of locking. Those techniques are the first designs permitting introduction and removal of levels in a lock hierarchy on demand and without disrupting transaction or query processing. In addition, a simplification of traditional key range locking is introduced that applies principled hierarchical locking to keys in B-tree leaves. This new method of key range locking avoids counter-intuitive lock modes used in today’s highperformance database systems. Nonetheless, it increases concurrency among operations on individual keys and records beyond that enabled by traditional lock modes.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'https://dl.gi.de/bitstream/handle/20.500.12116/22822/18.pdf?sequence=1',\n",
      " 'journal': 'Datenbanksysteme in Business, Technologie und Web (BTW 2007)–12. '\n",
      "            'Fachtagung des GI-Fachbereichs\" Datenbanken und '\n",
      "            'Informationssysteme\"(DBIS)',\n",
      " 'publisher': 'Gesellschaft für Informatik e. V.',\n",
      " 'title': 'Hierarchical locking in B-tree indexes',\n",
      " 'url': 'https://dl.gi.de/bitstream/handle/20.500.12116/22822/18.pdf?sequence=1',\n",
      " 'year': 2007}\n",
      "\n",
      "===[ 74 / 100 ] 33\n",
      "{'abstract': <div class=\"gsc_vcd_value\" id=\"gsc_vcd_descr\"><div class=\"gsh_small\"><div class=\"gsh_csp\">A recent article introduced partitioned B-trees, in which partitions are defined not in the catalogs but by distinct values in an artificial leading key column. As there usually is only a single value in this column, there usually is only a single partition, and queries and updates perform just like in traditional B-tree indexes. By temporarily permitting multiple values, at the expense of reduced query performance, interesting database usage scenarios become possible, in particular for bulk insert (database load). The present paper guides database administrators to exploiting partitioned B-trees even if they are not implemented by their DBMS vendor.</div></div></div>,\n",
      " 'author': 'Goetz Graefe',\n",
      " 'eprint': 'http://doesen0.informatik.uni-leipzig.de/proceedings/paper/IP11.pdf',\n",
      " 'pages': '668-671',\n",
      " 'title': \"Partitioned B-trees-a user's guide.\",\n",
      " 'url': 'http://doesen0.informatik.uni-leipzig.de/proceedings/paper/IP11.pdf',\n",
      " 'year': 2003}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SearchByAuthor(\"Goetz Graefe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka\n",
    "- [Doc](https://kafka.apache.org/documentation/)\n",
    "- [Code](https://github.com/apache/kafka)\n",
    "  - D:\\workspace\\rtfsc\\kafka\n",
    "- [The Internals of Apache Kafka](https://books.japila.pl/kafka-internals/)\n",
    "\n",
    "> Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\n",
    "\n",
    "Books:\n",
    "- Kafka: a Distributed Messaging System for Log Processing, 2011.\n",
    "- I Heart Logs: Event Data, Stream Processing, and Data Integration, 2014.\n",
    "- Kafka: The Definitive Guide, 2nd Edition, 2021.\n",
    "  - 1st edition book: 0.9.0.1, 0.10.0; version in practice: 3.4.0.\n",
    "  - 2nd edition book: kafka_2.13-2.7.0. (P.23)\n",
    "  - notes: `obsidian\\Data Engineering\\Data Pipelines\\Apache Kafka\\book.Kafka- The Definitive Guide.md`\n",
    "- Kafka Streams in Action, 2018.\n",
    "- Kafka Connect, 2023.\n",
    "\n",
    "actions:\n",
    "- workbench\\DataEngineering\\codes\\data-engineering-java\\kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1 Getting Started:\n",
    "\n",
    "2 APIs:\n",
    "- Admin API\n",
    "- Producer API\n",
    "- Consumer API\n",
    "- Kafka Streams API\n",
    "- Kafka Connect API\n",
    "\n",
    "3 Configuration:\n",
    "- Broker Configs\n",
    "- Topic Configs\n",
    "- Producer Configs\n",
    "- Consumer Configs\n",
    "- Kafka Connect Configs\n",
    "  - Source Connector Configs\n",
    "  - Sink Connector Configs\n",
    "- Kafka Streams Configs\n",
    "- AdminClient Configs\n",
    "- MirrorMaker Configs\n",
    "- System Properties\n",
    "- Tiered Storage Configs\n",
    "\n",
    "4 Design:\n",
    "- 4.3 Efficiency\n",
    "[`sendfile(2)`](https://man7.org/linux/man-pages/man2/sendfile.2.html), [Efficient data transfer through zero copy](https://developer.ibm.com/articles/j-zerocopy/)\n",
    "  - Java: `java.nio.channels.FileChannel#transferTo`\n",
    "- 4.6 Message Delivery Semantics: https://kafka.apache.org/documentation/#semantics\n",
    "Since 0.11.0.0, the Kafka producer also supports **an idempotent delivery option** which guarantees that resending will not result in duplicate entries in the log.\n",
    "Also beginning with 0.11.0.0, the producer supports **the ability to send messages to multiple topic partitions using transaction-like semantics**: i.e. either all messages are successfully written or none of them are.\n",
    "\n",
    "5 Implementation:\n",
    "\n",
    "6 Operations:\n",
    "\n",
    "7 Security:\n",
    "\n",
    "8 Kafka Connect:\n",
    "\n",
    "9 Kafka Streams:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "- topic\n",
    "- partition\n",
    "- replica\n",
    "  - leader\n",
    "  - follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Storage\n",
    "- tired storage\n",
    "- partition allocation\n",
    "- file management\n",
    "- file format\n",
    "- indexes\n",
    "- compaction `procedure`\n",
    "- deleted events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "- broker, controller\n",
    "  - ZooKeeper, KRaft\n",
    "- client\n",
    "  - producer\n",
    "  - consumer\n",
    "  - consumer group\n",
    "  - admin client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request Processing\n",
    "- [Kafka Protocol Guide](https://kafka.apache.org/protocol.html)\n",
    "```\n",
    "- Protocol Primitive Types\n",
    "- grammars\n",
    "- Common Request and Response Structure\n",
    "- Structure\n",
    "- Record Batch\n",
    "- Constants\n",
    "\t- Error Codes\n",
    "\t- Api Keys\n",
    "- The Messages\n",
    "```\n",
    "\n",
    "requests:\n",
    "- produce request\n",
    "- fetch request\n",
    "- admin request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev\n",
    "- Kafka Producers: writing message to Kafka\n",
    "- Kafka Consumers: reading data from Kafka\n",
    "- reliable data delivery\n",
    "- exactly-once semantics\n",
    "  - idempotent producer\n",
    "  - transactions\n",
    "- building data pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ops\n",
    "- installation\n",
    "- managing Kafka programatically\n",
    "- cross-cluster data mirroring\n",
    "- securing Kafka\n",
    "- administering Kafka\n",
    "- monitoring Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strimzi\n",
    "https://strimzi.io/\n",
    "> Strimzi provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.\n",
    "\n",
    "Usage example: \n",
    "- [[book.Vert.x in Action#5.7 Designing a reactive application]]\n",
    "- [[book.Reactive Systems in Java#5.4.2 The Event Bus The Backbone]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confluent\n",
    "- [A Complete Comparison of Apache Kafka vs Confluent](https://www.confluent.io/apache-kafka-vs-confluent/)： Used by over 70% of the Fortune 500, Apache Kafka has become the foundational platform for streaming data, but self-supporting the open source project puts you in the business of managing low-level data infrastructure. With Kafka at its core, Confluent offers *complete, fully managed, cloud-native data streaming* that's available everywhere your data and applications reside.\n",
    "- [Quick Start for Confluent Platform](https://docs.confluent.io/platform/current/platform-quickstart.html#ce-docker-quickstart): In this quick start, you create *Apache Kafka*® topics, use *Kafka Connect* to generate mock data to those topics, and create *ksqlDB* streaming queries on those topics. You then go to *Confluent Control Center* to monitor and analyze the event streaming queries. When you finish, you’ll have a real-time app that consumes and processes data streams by using familiar SQL statements.\n",
    "\t- As of Confluent Platform 7.5, ZooKeeper is deprecated for new deployments\n",
    "- [Confluent Platform All-In-One](https://github.com/confluentinc/cp-all-in-one): docker-compose.yml files for cp-all-in-one , cp-all-in-one-community, cp-all-in-one-cloud, Apache Kafka Confluent Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prometheus\n",
    "- [kafka_exporter](https://github.com/danielqsj/kafka_exporter)\n",
    "- [JMX Exporter](https://github.com/prometheus/jmx_exporter)\n",
    "\n",
    "- Burrow: lag monitoring\n",
    "- Xinfra Monitor/Kafka Monitor: end-to-end monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Streams\n",
    "https://kafka.apache.org/documentation/streams/\n",
    "\n",
    "> Kafka Streams is **a client library for building applications and microservices, where the input and output data are stored in Kafka clusters**. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka's server-side cluster technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Concepts\n",
    "\n",
    "- Stream Processing Topology\n",
    "\t- Stream\n",
    "\t- Processor: source, sink\n",
    "- Time: event time, processing time, ingestion time\n",
    "- Duality of Streams and Tables\n",
    "- Aggregation\n",
    "- Windowing\n",
    "- State: state store\n",
    "- Processing Guarantees\n",
    "\t- exactly one: transactional and idempotent producer (since 0.11.0.0) , KIP-129: Streams Exactly-Once Semantics\n",
    "\t- exactly-once v2: KIP-447: Producer scalability for exactly once semantics\n",
    "\t- out-of-order handling: versioned state store, offset-based semantics/timestamp-based semantics\n",
    "\n",
    "2. Architecture\n",
    "\n",
    "> Kafka Streams simplifies application development by **building on the Kafka producer and consumer libraries** and **leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity**.\n",
    "\n",
    "- Stream Partitions and Tasks\n",
    "- Thread Model\n",
    "- Local State Stores\n",
    "- Fault Tolerance\n",
    "\n",
    "3. Developer Guide\n",
    "\n",
    "- Streams DSL\n",
    "The Kafka Streams DSL (Domain Specific Language) is built on top of the Streams Processor API. It is the recommended for most users, especially beginners. Most data processing operations can be expressed in just a few lines of DSL code.\n",
    "- Processor API\n",
    "The Processor API allows developers to define and connect custom processors and to interact with state stores. With the Processor API, you can define arbitrary stream processors that process one received record at a time, and connect these processors with their associated state stores to compose the processor topology that represents a customized processing logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KIP\n",
    "https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KIP-98 - Exactly Once Delivery and Transactional Messaging\n",
    "- KIP-975: Docker Image for Apache Kafka\n",
    "\n",
    "KRaft:\n",
    "- KIP-500: Replace ZooKeeper with a Self-Managed Metadata Quorum\n",
    "- KIP-595: A Raft Protocol for the Metadata Quorum\n",
    "- KIP-631: The Quorum-based Kafka Controller\n",
    "\n",
    "Read from Follower:\n",
    "- KIP-392: Allow consumers to fetch from closest replica\n",
    "\n",
    "Physical storage:\n",
    "- KIP-405: Kafka Tiered Storage\n",
    "\n",
    "Idempotent/Transactional Producer:\n",
    "- KIP-360: Improve reliability of idempotent/transactional producer\n",
    "- KIP-447: Producer scalability for exactly once semantics\n",
    "\n",
    "MirrorMaker2:\n",
    "- KIP-656: MirrorMaker2 Exactly-once Semantics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

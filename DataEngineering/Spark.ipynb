{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "* https://github.com/apache/spark\n",
    "* 配置: https://spark.apache.org/docs/latest/configuration.html\n",
    "* PySpark: https://spark.apache.org/docs/latest/api/python/getting_started/index.html\n",
    "\n",
    "actions:\n",
    "- https://github.com/jargonzhou/application-store/tree/main/data-engineering/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /home/zhoujiagen/.local/lib/python3.12/site-packages/pjsua2-2.15.1-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyspark\n",
      "  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/317.2 MB\u001b[0m \u001b[31m15.5 kB/s\u001b[0m eta \u001b[36m5:39:20\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL\n",
    "!pip install pyspark[sql]\n",
    "# pandas API on Spark\n",
    "# to plot your data, you can install plotly together.\n",
    "!pip install pyspark[pandas_on_spark] plotly  \n",
    "# Spark Connect\n",
    "!pip install pyspark[connect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# .config(\"spark.some.config.option\", \"some-value\")\n",
    "spark = SparkSession.builder.appName('spike-app').master(\"spark://localhost:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# df = spark.createDataFrame([\n",
    "#     Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "#     Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "#     Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "# ])\n",
    "# df\n",
    "\n",
    "# 带schema\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env\n",
    "\n",
    "# 'SPARK_AUTH_SOCKET_TIMEOUT': '15',\n",
    "#  'SPARK_BUFFER_SIZE': '65536'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "# example: in master\n",
    "/opt/bitnami/spark$ sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.5.5\n",
    "starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to /opt/bitnami/spark/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-724dbdc1d558.out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_REMOTE=\"sc://localhost:15002\"\n"
     ]
    }
   ],
   "source": [
    "# %env SPARK_REMOTE=\"sc://localhost:15002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('spike-connect-app').remote(\"sc://localhost:15002\").getOrCreate()\n",
    "# .config(\"spark.some.config.option\", \"some-value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 04:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 04:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 04:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env\n",
    "\n",
    "#  'SPARK_AUTH_SOCKET_TIMEOUT': '15',\n",
    "#  'SPARK_BUFFER_SIZE': '65536',\n",
    "#  'SPARK_CONNECT_MODE_ENABLED': '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important: clean the environment\n",
    "import os\n",
    "if 'SPARK_CONNECT_MODE_ENABLED' in os.environ:\n",
    "  del os.environ['SPARK_CONNECT_MODE_ENABLED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYARROW_IGNORE_TIMEZONE=1\n"
     ]
    }
   ],
   "source": [
    "%env PYARROW_IGNORE_TIMEZONE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('spike-pandas-app').master(\"spark://localhost:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    3.0\n",
       "2    5.0\n",
       "3    NaN\n",
       "4    6.0\n",
       "5    8.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Wanna to do ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read MySQL Table\n",
    "- read Kafka\n",
    "- read Redis\n",
    "- read Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# https://github.com/apache/spark/blob/master/examples/src/main/python/sql/datasource.py#L380\n",
    "#\n",
    "# datasource options\n",
    "# https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# .config('spark.jars', \"C:/Users/zhouj/.m2/repository/com/mysql/mysql-connector-j/8.0.32/mysql-connector-j-8.0.32.jar\") \\\n",
    "    # .config('spark.driver.extraClassPath', \"/opt/bitnami/spark/jars/mysql-connector-j-8.0.32.jar\") \\\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('spike-mysql-app') \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .config('spark.jars', \"/opt/bitnami/spark/jars/mysql-connector-j-8.0.32.jar\") \\\n",
    "    .config('spark.driver.extraClassPath', \"mysql-connector-j-8.0.32.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------+\n",
      "| id|           email|password|\n",
      "+---+----------------+--------+\n",
      "|  1|xxx1@example.com|    pwd1|\n",
      "|  2|xxx2@example.com|    pwd2|\n",
      "|  3|xxx3@example.com|    pwd3|\n",
      "|  4|xxx4@example.com|    pwd4|\n",
      "+---+----------------+--------+\n",
      "\n",
      "+---+----------------+--------+\n",
      "| id|           email|password|\n",
      "+---+----------------+--------+\n",
      "|  3|xxx3@example.com|    pwd3|\n",
      "|  4|xxx4@example.com|    pwd4|\n",
      "+---+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql.SparkSession.read\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.read.html\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://192.168.3.178:3306/devops\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"users\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"change_me\") \\\n",
    "    .load()\n",
    "jdbcDF.show()\n",
    "\n",
    "spark.read \\\n",
    "    .jdbc(url=\"jdbc:mysql://192.168.3.178:3306/devops\", \n",
    "        table='users', \n",
    "        predicates=['id > 2'],\n",
    "        properties = {'user': 'root', 'password': 'change_me'}) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------+\n",
      "| id|           email|password|\n",
      "+---+----------------+--------+\n",
      "|  3|xxx3@example.com|    pwd3|\n",
      "|  4|xxx4@example.com|    pwd4|\n",
      "+---+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame上过滤\n",
    "jdbcDF.filter(jdbcDF['id'] > 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# DataFrame.write\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.write.html#pyspark.sql.DataFrame.write\n",
    "df = spark.createDataFrame([\n",
    "    Row(id=4, email='xxx@example.com', password='pwd')\n",
    "])\n",
    "\n",
    "# 追加: 存在主键重复\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://192.168.3.178:3306/devops\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"users\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"change_me\") \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "  Row(id=1, email='xxx1@example.com', password='pwd1'),\n",
    "  Row(id=2, email='xxx2@example.com', password='pwd2'),\n",
    "  Row(id=3, email='xxx3@example.com', password='pwd3'),\n",
    "  Row(id=4, email='xxx4@example.com', password='pwd4')\n",
    "])\n",
    "\n",
    "df.write \\\n",
    "  .jdbc(url=\"jdbc:mysql://192.168.3.178:3306/devops\", \n",
    "        table='users', \n",
    "        # mode='ignore', \n",
    "        mode='overwrite', # 全部覆盖\n",
    "        # mode='append', # 追加: 存在主键重复\n",
    "        properties = {'user': 'root', 'password': 'change_me'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------+\n",
      "| id|           email|password|\n",
      "+---+----------------+--------+\n",
      "|  1|xxx1@example.com|    pwd1|\n",
      "|  2|xxx2@example.com|    pwd2|\n",
      "|  3|xxx3@example.com|    pwd3|\n",
      "|  4|xxx4@example.com|    pwd4|\n",
      "+---+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read \\\n",
    "    .jdbc(url=\"jdbc:mysql://192.168.3.178:3306/devops\", \n",
    "        table='users', \n",
    "        properties = {'user': 'root', 'password': 'change_me'}) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_HOME=.\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_HOME=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql.SparkSession.sql\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html\n",
    "#\n",
    "# SQL syntax\n",
    "# https://spark.apache.org/docs/latest/sql-ref-syntax.html\n",
    "\n",
    "# Why we need this???\n",
    "# : java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
    "# https://sparkbyexamples.com/spark/spark-hadoop-exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io-nativeio-nativeiowindows-access0ljava-lang-stringiz/\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# CREATE TABLE s_users(id BIGINT, email STRING, password STRING) USING JDBC\n",
    "# OPTIONS (\n",
    "#     'url'='jdbc:mysql://192.168.3.178:3306/devops',\n",
    "#     'driver'='com.mysql.cj.jdbc.Driver',\n",
    "#     'dbtable'='users',\n",
    "#     'user'='root',\n",
    "#     'password'='change_me' \n",
    "# )\n",
    "# COMMENT 'table from devops.users'\n",
    "# \"\"\").show()\n",
    "\n",
    "# play in spark-sql\n",
    "# \n",
    "# spark-sql (default)> CREATE TABLE s_users(id BIGINT, email STRING, password STRING) USING JDBC\n",
    "#                    > OPTIONS (\n",
    "#                    >     'url'='jdbc:mysql://192.168.3.178:3306/devops',\n",
    "#                    >     'driver'='com.mysql.cj.jdbc.Driver',\n",
    "#                    >     'dbtable'='users',\n",
    "#                    >     'user'='root',\n",
    "#                    >     'password'='change_me' \n",
    "#                    > )\n",
    "#                    > COMMENT 'table from devops.users';\n",
    "\n",
    "# spark-sql (default)> select * from s_users;\n",
    "# 1       xxx1@example.com        pwd1\n",
    "# 2       xxx2@example.com        pwd2\n",
    "# 3       xxx3@example.com        pwd3\n",
    "# 4       xxx4@example.com        pwd4\n",
    "# Time taken: 1.079 seconds, Fetched 4 row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT * FROM s_users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis\n",
    "* https://github.com/RedisLabs/spark-redis/blob/master/doc/python.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "* https://www.machinelearningplus.com/pyspark/what-is-sparksession/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387533d3",
   "metadata": {},
   "source": [
    "# Flink\n",
    "\n",
    "- [Apache Flink® — Stateful Computations over Data Streams](https://flink.apache.org/)\n",
    "\n",
    "<img src=\"https://flink.apache.org/img/flink-home-graphic.png\" width=\"800\"/>\n",
    "\n",
    "books:\n",
    "- Hueske, Fabian / Kalavri, Vasiliki. **Stream Processing with Apache Flink**. 2019. O’Reilly.\n",
    "\n",
    "\n",
    "actions:\n",
    "- workbench\\DataEngineering\\codes\\data-engineering-java\\streaming\\flink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac04aa",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94953b29",
   "metadata": {},
   "source": [
    "version: LTS 1.20\n",
    "\n",
    "<img src=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/processes.svg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85c1b1",
   "metadata": {},
   "source": [
    "## Flink Program/Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e0d10",
   "metadata": {},
   "source": [
    "Client: 准备和发送数据流到JobManager.\n",
    "- detached mode: 断开与JobManager的连接\n",
    "- attached mode: 保持连接接收进展报告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d7fba",
   "metadata": {},
   "source": [
    "## JobManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3152d83",
   "metadata": {},
   "source": [
    "JobManager负责协调Flink应用的分布式执行:\n",
    "- 决定合适调度下一个/下一批任务\n",
    "- 处理结束的任务或任务执行失败\n",
    "- 协调checkpoint\n",
    "- 失败时协调恢复\n",
    "\n",
    "构成组件:\n",
    "- ResourceManager: 负责资源分配/回收, 制备Flink集群\n",
    "  - 例: YARN, Kubernetes, 单独部署. 在单独部署中, ResourceManager只能分布可用的slot, 不能启动新的TaskManager.\n",
    "  - 负责管理task slot: Flink集群中资源调度的单元\n",
    "- Dispatcher: 提供了REST接口\n",
    "  - 提交Flink应用, 并为每个提交的job启动一个新的JobMaster.\n",
    "  - 同时运行Flink Web UI提供job执行信息.\n",
    "- JobMaster: 负责管理单个JobGraph的执行.\n",
    "  - Flink集群中可以同时执行多个job, 每个job有自己的JobMaster.\n",
    "\n",
    "至少有一个JobManager.\n",
    "- HA部署中, 一个是leader, 其他是standby."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980bd62",
   "metadata": {},
   "source": [
    "## TaskManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27898981",
   "metadata": {},
   "source": [
    "TaskManager/worker执行数据流中任务, 缓存和交换数据流.\n",
    "\n",
    "最少有一个TaskManager.\n",
    "\n",
    "TaskManager中资源调度的最小单位是task slot.\n",
    "- TaskManager中task slot的数量: 并发执行任务的数量\n",
    "- 多个操作符可以在一个task slot中执行: task and operator chain\n",
    "\n",
    "在分布式执行中, Flink将操作符子任务链成任务task. 每个task由一个线程执行.\n",
    "- 链接chainning操作符: 优化, 减少线程之间通信和缓存的成本, 增加整体吞吐量的同时减少延迟.\n",
    "\n",
    "每个TaskManager是一个JVM进程, 使用task slot抽象控制TaskManager可以接受的任务数量.\n",
    "- 每个task slot表示TaskManager中的一个固定的资源子集: 隔离的内存资源.\n",
    "\n",
    "slot sharing\n",
    "- 默认情况下, Flink允许subtask共享slot, 甚至是不同task的subtask, 只要这些subtask属于同一个job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976f7a4",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b28432",
   "metadata": {},
   "source": [
    "<img src=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/fig/levels_of_abstraction.svg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eba757",
   "metadata": {},
   "source": [
    "- stateful and timely stream processing: 在DataStream中以Process Function方式内嵌\n",
    "  - 处理流中事件, 提供一致的和容错的状态抽象.\n",
    "  - 注册事件时间/处理时间回调: 支持复杂的计算\n",
    "- DataStream API: 有界/无界的流\n",
    "  - 流式风格的API, 提供数据处理的基本构建块: 转换, 联接, 聚合, 窗口, 状态等.\n",
    "  - 数据类型表示为相应编程语言中的类.\n",
    "- DataSet API: 有界数据集\n",
    "  - 提供额外的构建块: 循环, 迭代等.\n",
    "- Table API\n",
    "  - 围绕表的声明式DSL, 可以是动态变化的表.\n",
    "    - 定义所需完成的逻辑操作, 而不是描述如何实现操作.\n",
    "  - 扩展的关系模型: schema, 操作(select, project, join, group-by, aggregate等)\n",
    "    - 可以通过用户自定义函数扩展\n",
    "    - 在执行前通过优化器处理\n",
    "- SQL: 在语义和表达能力上与Table API相同, 但使用SQL查询表达式.\n",
    "  - 可以在Table API定义的表上执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e754ab8",
   "metadata": {},
   "source": [
    "example: \n",
    "- https://github.com/apache/flink/tree/master/flink-walkthroughs\n",
    "- https://github.com/apache/flink/tree/master/flink-examples\n",
    "- https://github.com/apache/flink-playgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec57fb",
   "metadata": {},
   "source": [
    "## DataStream API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086cd2e",
   "metadata": {},
   "source": [
    "- `StreamExecutionEnvironment`\n",
    "- `org.apache.flink.streaming.api`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548422f0",
   "metadata": {},
   "source": [
    "## Table API & SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3655116b",
   "metadata": {},
   "source": [
    "- `TableEnvironment`: `StreamTableEnvironment`\n",
    "\n",
    "refs:\n",
    "- [Table API](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/tableapi/)\n",
    "- [Data Types](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/types/)\n",
    "- [SQL](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/sql/overview/)\n",
    "- [Functions](https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/table/functions/overview/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2c383",
   "metadata": {},
   "source": [
    "## Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be7545",
   "metadata": {},
   "source": [
    "## Library: CEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d36ac",
   "metadata": {},
   "source": [
    "## Library: State Processor API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9785e67",
   "metadata": {},
   "source": [
    "# Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d87da",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f67b8",
   "metadata": {},
   "source": [
    "例:\n",
    "- 搜索特定的事件模式: 存储当前遇到的事件序列\n",
    "- 按分钟/小时/天聚合事件: 存储相应的聚合量\n",
    "- 在数据点流上训练机器学习模型: 存储模型参数的当前版本\n",
    "- 需要管理历史性数据: 存储提供高效访问之前出现的事件的数据结构\n",
    "\n",
    "相关概念:\n",
    "- 状态容错: checkpoint, savepoint\n",
    "- 应用的可扩展性: 在多个实例上分布状态\n",
    "- 状态存储后端"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e45425",
   "metadata": {},
   "source": [
    "Keyed State: 用内嵌的键/值存储维护\n",
    "- 在按键/分区数据交换之后, 只能在keyed stream上访问键/值状态\n",
    "- 按键对齐流和状态确保: 状态更新是局部操作, 不需要事务即可保证一致性\n",
    "\n",
    "Key Group: 由Keyed State组成\n",
    "- 是分布Keyed State的原子单元, 与定义的最大并行度数量相同.\n",
    "- 在执行时, 每个键操作符的实例在一个或多个Key Group中的键上操作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257c97d",
   "metadata": {},
   "source": [
    "### State Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f47b7",
   "metadata": {},
   "source": [
    "Flink使用流重放(stream replay)和检查点(checkpoint)的组合实现容错.\n",
    "- checkpoint用每个操作符的相应状态, 标记输入流中的特定点.\n",
    "  - checkpint默认关闭\n",
    "  - 状态通常使用分布式文件系统存储.\n",
    "  - 通过分布式快照(distributed snapshot)实现: checkpoint, snapshot同义.\n",
    "- 通过恢复操作符的状态和重放checkpoint之后的事件记录, 从检查点处恢复流数据流的执行, 同时确保一致性(精确一次的处理语义).\n",
    "  - 重放时, 数据流的源(source)需要能够恢复到特定的事件记录: 例Apache Kafka.\n",
    "\n",
    "checkpoint的周期间隔是执行时的容错消耗和恢复时间(需要重放的事件记录数量)的权衡."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eedcd0",
   "metadata": {},
   "source": [
    "checkpoint实现\n",
    "- [Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/abs/1506.08603)\n",
    "  - ABS: Asynchronous Barrier Snapshotting. 只持久化无环执行拓扑中操作符状态, 在环数据流中保持最少的记录日志.\n",
    "\n",
    "> aligned checkpintint\n",
    "\n",
    "stream barrier: 注入到数据流中, 随数据流中数据一起流动\n",
    "- 将数据流中记录划分为两部分: 当前快照中的记录, 下一次快照中的记录.\n",
    "- barrier中待snapshot ID, 表示它之前的记录所属的快照ID.\n",
    "- barrier不会终端流的流动, 是轻量级的.\n",
    "- 不同snapshot的barrier可以同时在流中出现: 支持并发的多个快照.\n",
    "- 事件处理\n",
    "  - 在哪里输入数据流中?: 在数据流源(source)处, 报告给checkpoint协调者JobManager. \n",
    "  - 中间的操作符遇到barrier时, 在所有输出流中注入barrier\n",
    "  - sink操作符遇到barrier, 给checkpoint协调者确认该snapshot.\n",
    "  - checkpoint协调者收到所有sink的确认后, 认为snapshot完成.\n",
    "- 对齐: 接收多个输入流的操作符需要在snapshot barrier上对立输入流\n",
    "  - 一旦操作符从一个输入流中接收到snapshot barrier n, 不能处理该流中的后续事件记录, 记录在输入缓冲中, 直到收到所有输入流的barrier n.\n",
    "  - 一旦收到所有输入流的barrier n, 开始输出所有pending的事件, 并注入barrier n.\n",
    "  - 快照状态, 恢复处理流中记录, 先处理输入缓冲中的记录, 再处理流中记录.\n",
    "  - 最终, 异步写状态到状态后端.\n",
    "  - 接收多个输入流的操作符: 所有有多个输入的操作符, shuffle后从多个上游子任务消费其输出流的操作符.\n",
    "- 执行状态快照\n",
    "  - 何时执行?: 在接收到输入流的所有snapshot barrier时, 在输出流中注入barrier之前.\n",
    "  - 存储在状态存储后端: JobManager的内存, 或分布式的可靠存储, 例如HDFS.\n",
    "  - 快照内容?\n",
    "    - 每个并行流数据源: 快照开始的偏移量/位置.\n",
    "    - 每个操作符: 快照中存储的状态的指针.\n",
    "- 恢复中使用快照\n",
    "  - 使用最近完成的checkpoint k\n",
    "  - 重新部署整个分布式数据流: 源从位置Sk处开始读取, 每个操作符设置为checkpoint k中的相应状态.\n",
    "- savepoint总是对齐的.\n",
    "  - more: Operations > State & Fault Tolerance > Task Failure Recovery\n",
    "\n",
    "unaligend ckeckpointing\n",
    "- 思路: checkpoint中可以包含是操作符状态的in-flight事件记录.\n",
    "- 事件处理\n",
    "  - 在源中注入barrier: 避免checkpoint协调者过载\n",
    "  - 操作符对其输入缓冲中的第一个barrier开始处理, 立即在输出缓冲末尾注入barrier; 操作符标记并异步存储所有涉及的记录, 并创建它的状态的snapshot.\n",
    "- 适用场景: barrier需要尽快的到达sink, 缓慢移动的数据路径. \n",
    "- 恢复\n",
    "  - 除了在开始处理上游操作符的数据之前恢复in-fligth数据之外, 与对齐的checkpint恢复一致.\n",
    "\n",
    "批处理程序中的状态:\n",
    "- 不使用checkpoint, 使用重放这个有界的流执行恢复.\n",
    "- 使用简化的内存/核外数据结构存储状态, 而不是键值索引."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ff303",
   "metadata": {},
   "source": [
    "状态存储后端:\n",
    "- 内存中hash map\n",
    "- RocksDB\n",
    "- checkpoint支持: point-in-time snapshot\n",
    "  - 由JobManager触发TaskManager的checkpoint执行\n",
    "  - TaskManager存储状态的snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa07836",
   "metadata": {},
   "source": [
    "savepoint: 手动触发的checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1282d5",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce6128",
   "metadata": {},
   "source": [
    "notions of time:\n",
    "- processing time: 执行特定操作的机器的系统时间\n",
    "- event time: 每个独立的事件在产生设备上出现的时间.\n",
    "\n",
    "水位线watermark\n",
    "- 参考: [Dataflow Model](https://research.google.com/pubs/archive/43864.pdf)\n",
    "- 度量事件时间的进展. watermark随数据流流动, 携带了一个时间戳t.\n",
    "- Watermark(t): 事件时间已经到达时刻t, 带时间戳t'<=t的事件不应该出现.\n",
    "- 流: 有序流, 乱序流\n",
    "\n",
    "并行流中的watermark\n",
    "- 源: 源函数的并行子任务独立的产生各自的watermark.\n",
    "- 随着watermark在流程序中流动, 推进到达的操作符的事件时间. 当操作符推进事件时间时, 为后继的操作符生成新的watermark.\n",
    "- 有多个输入流的操作符的当前事件时间是输入流事件时间的最小值.\n",
    "\n",
    "lateness\n",
    "- 迟到的元素: 系统的事件时间时钟(由watermark表示)中, 元素的时间戳已过.\n",
    "\n",
    "windowing\n",
    "- 数据流上的聚合: 窗口window\n",
    "  - 例: 最近5分钟的数量(time-driven), 最近100个元素的和(data-driven)\n",
    "- 窗口分类:\n",
    "  - 滚动tumbling窗口: 没有重叠\n",
    "  - 滑动sliding窗口: 有重叠\n",
    "  - 会话session窗口: 由无活动的间隙分隔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0777263",
   "metadata": {},
   "source": [
    "# Connectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e6be09",
   "metadata": {},
   "source": [
    "DataStream Connectors\n",
    "- Kafka\n",
    "\n",
    "Table API Connectors\n",
    "- Kafka\n",
    "- JDBC: MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff79ffd",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe9190",
   "metadata": {},
   "source": [
    "- Overview\n",
    "- Java Compatibility\n",
    "- Resource Providers\n",
    "- Configuration\n",
    "- Memory Configuration\n",
    "- Command-Line Interface\n",
    "- Elastic Scaling\n",
    "- Fine-Grained Resource Management\n",
    "- Speculative Execution\n",
    "- File Systems\n",
    "- High Availability\n",
    "- Metric Reporters\n",
    "- Trace Reporters\n",
    "- REPLs\n",
    "- Security\n",
    "- Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f3d5a",
   "metadata": {},
   "source": [
    "Deployment modes\n",
    "- **application mode**: runs on JobManager then exit\n",
    "  - 例: 在Kubernetes上部署Flink应用\n",
    "- **per-job mode**: YARN, deprecated in v1.15.\n",
    "- **session mode**: multiple jobs share one JobManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714a45d",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515a5f8",
   "metadata": {},
   "source": [
    "- State & Fault Tolerance\n",
    "- Metrics\n",
    "- Traces\n",
    "- REST API\n",
    "- Batch\n",
    "- Debugging\n",
    "- Monitoring: [example](https://flink.apache.org/2019/02/21/monitoring-apache-flink-applications-101/)\n",
    "- Upgrading Applications and Flink Versions\n",
    "- Production Readiness Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74844fc",
   "metadata": {},
   "source": [
    "# Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd13ca3",
   "metadata": {},
   "source": [
    "## Job and Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a2204",
   "metadata": {},
   "source": [
    "## Task Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d7a65",
   "metadata": {},
   "source": [
    "## File Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcdffb4",
   "metadata": {},
   "source": [
    "# Flink CDC\n",
    "* https://github.com/apache/flink-cdc\n",
    "\n",
    "\n",
    "Flink CDC is a streaming data integration tool that aims to provide users with a more robust API. It allows users to describe their ETL pipeline logic via YAML elegantly and help users automatically generating customized Flink operators and submitting job. Flink CDC prioritizes optimizing the task submission process and offers enhanced functionalities such as schema evolution, data transformation, full database synchronization and exactly-once semantic.\n",
    "\n",
    "Deeply integrated with and powered by Apache Flink, Flink CDC provides:\n",
    "\n",
    "- ✅ End-to-end data integration framework- \n",
    "- ✅ API for data integration users to build jobs easily\n",
    "- ✅ Multi-table support in Source / Sink\n",
    "- ✅ Synchronization of entire databases\n",
    "- ✅ Schema evolution capability\n",
    "\n",
    "actions:\n",
    "- application-store\\data-engineering\\flink\\cdc: standalone Flink cluster, local submit job(MySQL to Doris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb936563",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d3038",
   "metadata": {},
   "source": [
    "- data pipeline: source, sink, transform(optional), route(optional), pipeline\n",
    "- data source\n",
    "- data sink\n",
    "- table id: mapping to the storage objects of external system\n",
    "- transform: dalelte/expand data columns, filter data\n",
    "- route: source-table, sink-table, repliace-symbol, description\n",
    "- schema evolution: synchronize upstream schema DDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b12c0",
   "metadata": {},
   "source": [
    "## Connectors\n",
    "* https://nightlies.apache.org/flink/flink-cdc-docs-release-3.3/docs/connectors/pipeline-connectors/overview/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da48f7",
   "metadata": {},
   "source": [
    "- Apache Doris\n",
    "- Elasticsearch\n",
    "- Kafka\n",
    "- MySQL\n",
    "- OceanBase\n",
    "- Paimon\n",
    "- StarRocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf4301",
   "metadata": {},
   "source": [
    "# Stream Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798fed12",
   "metadata": {},
   "source": [
    "- [yahoo/streaming-benchmarks](https://github.com/yahoo/streaming-benchmarks)\n",
    "\n",
    "Sanket Chintapalli, Derek Dagit, Bobby Evans, et al. \"Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming. \" First Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware. IEEE, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab251f6",
   "metadata": {},
   "source": [
    "# More"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074b96d",
   "metadata": {},
   "source": [
    "- flink learning blog. [http://www.54tianzhisheng.cn/](http://www.54tianzhisheng.cn/) 含 Flink 入门、概念、原理、实战、性能调优、源码解析等内容。涉及 Flink Connector、Metrics、Library、DataStream API、Table API & SQL 等内容的学习案例，还有 Flink 落地应用的大型项目案例（PVUV、日志存储、百亿数据实时去…: https://github.com/zhisheng17/flink-learning\n",
    "- Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文: http://www.54tianzhisheng.cn/2019/06/13/flink-book-paper/\n",
    "- Ververica Platform: The Enterprise Stream Processing Platform by the Original Creators of Apache Flink®: https://www.ververica.com/platform"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

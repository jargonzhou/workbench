{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubernetes in Action\n",
    "\n",
    "- kiada: Kubernetes in Action Demo Application\n",
    "- [code](https://github.com/luksa/kubernetes-in-action-2nd-edition)\n",
    "- VSCode extension: Kubernetes\n",
    "\n",
    "\n",
    "tools:\n",
    "- [jq](https://github.com/jqlang/jq): Command-line JSON processor\n",
    "```shell\n",
    "$ which jq\n",
    "/c/Users/zhouj/bin/jq\n",
    "```\n",
    "- [yq](https://github.com/mikefarah/yq): yq is a portable command-line YAML, JSON, XML, CSV, TOML and properties processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "- built-in Kubernetes cluster in Docker Desktop: selected in Windows 1.27.2\n",
    "- Minukube\n",
    "- kind: Kubernetes in Docker\n",
    "- GKE: Goodle Kubernetes Engine\n",
    "- Amazon EKS: Elastic Kubernetes Service\n",
    "- kubeadm\n",
    "- kubectl: kubeconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Windows IP Configuration\n",
      "\n",
      "\n",
      "Unknown adapter 本地连接:\n",
      "\n",
      "   Media State . . . . . . . . . . . : Media disconnected\n",
      "   Connection-specific DNS Suffix  . : \n",
      "\n",
      "Ethernet adapter 以太网 2:\n",
      "\n",
      "   Media State . . . . . . . . . . . : Media disconnected\n",
      "   Connection-specific DNS Suffix  . : \n",
      "\n",
      "Unknown adapter OpenVPN Connect DCO Adapter:\n",
      "\n",
      "   Media State . . . . . . . . . . . : Media disconnected\n",
      "   Connection-specific DNS Suffix  . : \n",
      "\n",
      "Wireless LAN adapter 本地连接* 1:\n",
      "\n",
      "   Media State . . . . . . . . . . . : Media disconnected\n",
      "   Connection-specific DNS Suffix  . : \n",
      "\n",
      "Wireless LAN adapter 本地连接* 2:\n",
      "\n",
      "   Media State . . . . . . . . . . . : Media disconnected\n",
      "   Connection-specific DNS Suffix  . : \n",
      "\n",
      "Wireless LAN adapter WLAN:\n",
      "\n",
      "   Connection-specific DNS Suffix  . : \n",
      "   Link-local IPv6 Address . . . . . : fe80::3155:5954:f582:f646%20\n",
      "   IPv4 Address. . . . . . . . . . . : 192.168.0.105\n",
      "   Subnet Mask . . . . . . . . . . . : 255.255.255.0\n",
      "   Default Gateway . . . . . . . . . : 192.168.0.1\n",
      "\n",
      "Ethernet adapter vEthernet (WSL (Hyper-V firewall)):\n",
      "\n",
      "   Connection-specific DNS Suffix  . : \n",
      "   Link-local IPv6 Address . . . . . : fe80::6892:b634:56b6:f766%45\n",
      "   IPv4 Address. . . . . . . . . . . : 172.22.144.1\n",
      "   Subnet Mask . . . . . . . . . . . : 255.255.240.0\n",
      "   Default Gateway . . . . . . . . . : \n"
     ]
    }
   ],
   "source": [
    "!ipconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOCALHOST=192.168.0.105\n"
     ]
    }
   ],
   "source": [
    "# 环境变量\n",
    "%env LOCALHOST=192.168.0.105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Version: v1.27.2\n",
      "Kustomize Version: v5.0.1\n",
      "Server Version: v1.27.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flag --short has been deprecated, and will be removed in the future. The --short output will become the default.\n"
     ]
    }
   ],
   "source": [
    "# 查看版本\n",
    "!kubectl version --short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubectl controls the Kubernetes cluster manager.\n",
      "\n",
      " Find more information at: https://kubernetes.io/docs/reference/kubectl/\n",
      "\n",
      "Basic Commands (Beginner):\n",
      "  create          Create a resource from a file or from stdin\n",
      "  expose          Take a replication controller, service, deployment or pod and expose it as a new Kubernetes service\n",
      "  run             Run a particular image on the cluster\n",
      "  set             Set specific features on objects\n",
      "\n",
      "Basic Commands (Intermediate):\n",
      "  explain         Get documentation for a resource\n",
      "  get             Display one or many resources\n",
      "  edit            Edit a resource on the server\n",
      "  delete          Delete resources by file names, stdin, resources and names, or by resources and label selector\n",
      "\n",
      "Deploy Commands:\n",
      "  rollout         Manage the rollout of a resource\n",
      "  scale           Set a new size for a deployment, replica set, or replication controller\n",
      "  autoscale       Auto-scale a deployment, replica set, stateful set, or replication controller\n",
      "\n",
      "Cluster Management Commands:\n",
      "  certificate     Modify certificate resources.\n",
      "  cluster-info    Display cluster information\n",
      "  top             Display resource (CPU/memory) usage\n",
      "  cordon          Mark node as unschedulable\n",
      "  uncordon        Mark node as schedulable\n",
      "  drain           Drain node in preparation for maintenance\n",
      "  taint           Update the taints on one or more nodes\n",
      "\n",
      "Troubleshooting and Debugging Commands:\n",
      "  describe        Show details of a specific resource or group of resources\n",
      "  logs            Print the logs for a container in a pod\n",
      "  attach          Attach to a running container\n",
      "  exec            Execute a command in a container\n",
      "  port-forward    Forward one or more local ports to a pod\n",
      "  proxy           Run a proxy to the Kubernetes API server\n",
      "  cp              Copy files and directories to and from containers\n",
      "  auth            Inspect authorization\n",
      "  debug           Create debugging sessions for troubleshooting workloads and nodes\n",
      "  events          List events\n",
      "\n",
      "Advanced Commands:\n",
      "  diff            Diff the live version against a would-be applied version\n",
      "  apply           Apply a configuration to a resource by file name or stdin\n",
      "  patch           Update fields of a resource\n",
      "  replace         Replace a resource by file name or stdin\n",
      "  wait            Experimental: Wait for a specific condition on one or many resources\n",
      "  kustomize       Build a kustomization target from a directory or URL\n",
      "\n",
      "Settings Commands:\n",
      "  label           Update the labels on a resource\n",
      "  annotate        Update the annotations on a resource\n",
      "  completion      Output shell completion code for the specified shell (bash, zsh, fish, or powershell)\n",
      "\n",
      "Other Commands:\n",
      "  api-resources   Print the supported API resources on the server\n",
      "  api-versions    Print the supported API versions on the server, in the form of \"group/version\"\n",
      "  config          Modify kubeconfig files\n",
      "  plugin          Provides utilities for interacting with plugins\n",
      "  version         Print the client and server version information\n",
      "\n",
      "Usage:\n",
      "  kubectl [flags] [options]\n",
      "\n",
      "Use \"kubectl <command> --help\" for more information about a given command.\n",
      "Use \"kubectl options\" for a list of global command-line options (applies to all commands).\n"
     ]
    }
   ],
   "source": [
    "# 查看命令\n",
    "!kubectl --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "clusters:\n",
      "- cluster:\n",
      "    certificate-authority-data: DATA+OMITTED\n",
      "    server: https://127.0.0.1:6443\n",
      "  name: docker-desktop\n",
      "contexts:\n",
      "- context:\n",
      "    cluster: docker-desktop\n",
      "    user: docker-desktop\n",
      "  name: docker-desktop\n",
      "current-context: docker-desktop\n",
      "kind: Config\n",
      "preferences: {}\n",
      "users:\n",
      "- name: docker-desktop\n",
      "  user:\n",
      "    client-certificate-data: DATA+OMITTED\n",
      "    client-key-data: DATA+OMITTED\n"
     ]
    }
   ],
   "source": [
    "# 查看配置\n",
    "# !kubectl config --help\n",
    "!kubectl config view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes control plane is running at https://127.0.0.1:6443\n",
      "CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n",
      "\n",
      "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n",
      "NAME             STATUS   ROLES           AGE   VERSION\n",
      "docker-desktop   Ready    control-plane   42d   v1.27.2\n"
     ]
    }
   ],
   "source": [
    "# 查看集群信息\n",
    "!kubectl cluster-info\n",
    "# 查看节点信息\n",
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               docker-desktop\n",
      "Roles:              control-plane\n",
      "Labels:             beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    kubernetes.io/arch=amd64\n",
      "                    kubernetes.io/hostname=docker-desktop\n",
      "                    kubernetes.io/os=linux\n",
      "                    node-role.kubernetes.io/control-plane=\n",
      "                    node.kubernetes.io/exclude-from-external-load-balancers=\n",
      "Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock\n",
      "                    node.alpha.kubernetes.io/ttl: 0\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
      "CreationTimestamp:  Tue, 21 Jan 2025 16:52:18 +0800\n",
      "Taints:             <none>\n",
      "Unschedulable:      false\n",
      "Lease:\n",
      "  HolderIdentity:  docker-desktop\n",
      "  AcquireTime:     <unset>\n",
      "  RenewTime:       Tue, 04 Mar 2025 21:56:24 +0800\n",
      "Conditions:\n",
      "  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----             ------  -----------------                 ------------------                ------                       -------\n",
      "  MemoryPressure   False   Tue, 04 Mar 2025 21:56:12 +0800   Tue, 21 Jan 2025 16:52:17 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure     False   Tue, 04 Mar 2025 21:56:12 +0800   Tue, 21 Jan 2025 16:52:17 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  PIDPressure      False   Tue, 04 Mar 2025 21:56:12 +0800   Tue, 21 Jan 2025 16:52:17 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
      "  Ready            True    Tue, 04 Mar 2025 21:56:12 +0800   Tue, 21 Jan 2025 16:52:19 +0800   KubeletReady                 kubelet is posting ready status\n",
      "Addresses:\n",
      "  InternalIP:  192.168.65.4\n",
      "  Hostname:    docker-desktop\n",
      "Capacity:\n",
      "  cpu:                20\n",
      "  ephemeral-storage:  1055762868Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16238968Ki\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                20\n",
      "  ephemeral-storage:  972991057538\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16136568Ki\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                 0e8508e7-1e29-48ae-b552-e6fc4ac31499\n",
      "  System UUID:                0e8508e7-1e29-48ae-b552-e6fc4ac31499\n",
      "  Boot ID:                    cccb2e6c-3c6f-4b6c-8498-85a99243f2ce\n",
      "  Kernel Version:             5.15.153.1-microsoft-standard-WSL2\n",
      "  OS Image:                   Docker Desktop\n",
      "  Operating System:           linux\n",
      "  Architecture:               amd64\n",
      "  Container Runtime Version:  docker://24.0.5\n",
      "  Kubelet Version:            v1.27.2\n",
      "  Kube-Proxy Version:         v1.27.2\n",
      "Non-terminated Pods:          (9 in total)\n",
      "  Namespace                   Name                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n",
      "  ---------                   ----                                      ------------  ----------  ---------------  -------------  ---\n",
      "  kube-system                 coredns-5d78c9869d-rlgnm                  100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     42d\n",
      "  kube-system                 coredns-5d78c9869d-zkbjr                  100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     42d\n",
      "  kube-system                 etcd-docker-desktop                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         42d\n",
      "  kube-system                 kube-apiserver-docker-desktop             250m (1%)     0 (0%)      0 (0%)           0 (0%)         42d\n",
      "  kube-system                 kube-controller-manager-docker-desktop    200m (1%)     0 (0%)      0 (0%)           0 (0%)         42d\n",
      "  kube-system                 kube-proxy-plbl4                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         42d\n",
      "  kube-system                 kube-scheduler-docker-desktop             100m (0%)     0 (0%)      0 (0%)           0 (0%)         42d\n",
      "  kube-system                 storage-provisioner                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         42d\n",
      "  kube-system                 vpnkit-controller                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         42d\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  Resource           Requests    Limits\n",
      "  --------           --------    ------\n",
      "  cpu                850m (4%)   0 (0%)\n",
      "  memory             240Mi (1%)  340Mi (2%)\n",
      "  ephemeral-storage  0 (0%)      0 (0%)\n",
      "  hugepages-1Gi      0 (0%)      0 (0%)\n",
      "  hugepages-2Mi      0 (0%)      0 (0%)\n",
      "Events:\n",
      "  Type     Reason                   Age                From             Message\n",
      "  ----     ------                   ----               ----             -------\n",
      "  Normal   Starting                 25m                kube-proxy       \n",
      "  Normal   Starting                 25m                kubelet          Starting kubelet.\n",
      "  Warning  InvalidDiskCapacity      25m                kubelet          invalid capacity 0 on image filesystem\n",
      "  Normal   NodeHasSufficientMemory  25m (x8 over 25m)  kubelet          Node docker-desktop status is now: NodeHasSufficientMemory\n",
      "  Normal   NodeHasNoDiskPressure    25m (x7 over 25m)  kubelet          Node docker-desktop status is now: NodeHasNoDiskPressure\n",
      "  Normal   NodeHasSufficientPID     25m (x7 over 25m)  kubelet          Node docker-desktop status is now: NodeHasSufficientPID\n",
      "  Normal   NodeAllocatableEnforced  25m                kubelet          Updated Node Allocatable limit across pods\n",
      "  Normal   RegisteredNode           25m                node-controller  Node docker-desktop event: Registered Node docker-desktop in Controller\n"
     ]
    }
   ],
   "source": [
    "# 描述节点信息\n",
    "!kubectl describe node docker-desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard\n",
    "* https://github.com/kubernetes/dashboard\n",
    "\n",
    "As of version 7.0.0, we have dropped support for Manifest-based installation. Only Helm-based installation is supported now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version.BuildInfo{Version:\"v3.14.2\", GitCommit:\"c309b6f0ff63856811846ce18f3bdc93d2b4d54b\", GitTreeState:\"clean\", GoVersion:\"go1.21.7\"}\n"
     ]
    }
   ],
   "source": [
    "!helm version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"kubernetes-dashboard\" already exists with the same configuration, skipping\n",
      "Release \"kubernetes-dashboard\" has been upgraded. Happy Helming!\n",
      "NAME: kubernetes-dashboard\n",
      "LAST DEPLOYED: Tue Mar  4 22:07:10 2025\n",
      "NAMESPACE: kubernetes-dashboard\n",
      "STATUS: deployed\n",
      "REVISION: 2\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "*************************************************************************************************\n",
      "*** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ***\n",
      "*************************************************************************************************\n",
      "\n",
      "Congratulations! You have just installed Kubernetes Dashboard in your cluster.\n",
      "\n",
      "To access Dashboard run:\n",
      "  kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443\n",
      "\n",
      "NOTE: In case port-forward command does not work, make sure that kong service name is correct.\n",
      "      Check the services in Kubernetes Dashboard namespace using:\n",
      "        kubectl -n kubernetes-dashboard get svc\n",
      "\n",
      "Dashboard will be available at:\n",
      "  https://localhost:8443\n"
     ]
    }
   ],
   "source": [
    "# Add kubernetes-dashboard repository\n",
    "!helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\n",
    "# Deploy a Helm Release named \"kubernetes-dashboard\" using the kubernetes-dashboard chart\n",
    "# !helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard\n",
    "# https://github.com/kubernetes/dashboard/issues/8765\n",
    "!helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard --set kong.admin.tls.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                  READY   STATUS    RESTARTS   AGE\n",
      "kubernetes-dashboard-api-77c5f55788-ghr2p             1/1     Running   0          23s\n",
      "kubernetes-dashboard-auth-79894c8dcc-llxl9            1/1     Running   0          23s\n",
      "kubernetes-dashboard-kong-74c75b5b97-xzzhq            1/1     Running   0          23s\n",
      "kubernetes-dashboard-metrics-scraper-6bbf6cc8-rc4sh   1/1     Running   0          8m6s\n",
      "kubernetes-dashboard-web-84cd78db55-k8rjr             1/1     Running   0          8m6s\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubernetes-dashboard get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !kubectl proxy\n",
    "\n",
    "# run and interupted\n",
    "!kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\n",
      "kubernetes-dashboard-api               ClusterIP   10.103.112.219   <none>        8000/TCP   10m\n",
      "kubernetes-dashboard-auth              ClusterIP   10.98.240.95     <none>        8000/TCP   10m\n",
      "kubernetes-dashboard-kong-proxy        ClusterIP   10.99.124.124    <none>        443/TCP    10m\n",
      "kubernetes-dashboard-metrics-scraper   ClusterIP   10.97.224.240    <none>        8000/TCP   10m\n",
      "kubernetes-dashboard-web               ClusterIP   10.111.100.23    <none>        8000/TCP   10m\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubernetes-dashboard get svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhbGciOiJSUzI1NiIsImtpZCI6IlJ5TDNxVHN2QzN2SHMyLTR6azlZQVpEQXdYTzFNN2Z0NXJoUVYxd1JWdGsifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQxMTAxMDQ2LCJpYXQiOjE3NDEwOTc0NDYsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC13ZWIiLCJ1aWQiOiIyNGNlNmE1MS02ZTFjLTRjMmYtOTIzNS1lYTM4YzQzNDIxYjMifX0sIm5iZiI6MTc0MTA5NzQ0Niwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmt1YmVybmV0ZXMtZGFzaGJvYXJkLXdlYiJ9.ii2hxWX3eZf6LuDxGpaOM4uv_5v15MV35Y47X2zPYC2aj83PBStqse_wGFRWg6e4n8Lk1Go30_m8QxrFZatv7QXUMurHX2Fds8kzhiaUfPrsQ2N0y6KiTkyjoB1WtdZtB5nJ5PCOEpss99KLAi51YZUI37-4PzrKYZkTg1-IV3Akd-_B_K9ByeDrkBUaL_ZLJeRTvL_Jnd6fbpUEIlDE479Fv_5tddlpA_C2sDX93w5yGl0MuMFsVFC3hQ0Tq_Fv84P8plmaXFJ6gcAlDRs0E33HQf_qHCgSHG653BgQLAlSQEwZ1pbjO-PIM6H-eQtDHqyxVbEf0rLiI73L2nRv0A\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubernetes-dashboard create token kubernetes-dashboard-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"kubernetes-dashboard\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "# 清理\n",
    "!helm uninstall -n kubernetes-dashboard kubernetes-dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in kubernetes-dashboard namespace.\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubernetes-dashboard get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: luksa/kiada:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1: Pulling from luksa/kiada\n",
      "07471e81507f: Pulling fs layer\n",
      "c6cef1aa2170: Pulling fs layer\n",
      "13a51f13be8e: Pulling fs layer\n",
      "def39d67a1a7: Pulling fs layer\n",
      "a8367252e08e: Pulling fs layer\n",
      "5402d6c1eb0a: Pulling fs layer\n",
      "c018bad89663: Pulling fs layer\n",
      "ba9fd0742332: Pulling fs layer\n",
      "9d775c430e09: Pulling fs layer\n",
      "c39854f8e166: Pulling fs layer\n",
      "2dd5f1bea0e7: Pulling fs layer\n",
      "5402d6c1eb0a: Waiting\n",
      "c018bad89663: Waiting\n",
      "ba9fd0742332: Waiting\n",
      "9d775c430e09: Waiting\n",
      "c39854f8e166: Waiting\n",
      "2dd5f1bea0e7: Waiting\n",
      "a8367252e08e: Waiting\n",
      "def39d67a1a7: Waiting\n",
      "13a51f13be8e: Verifying Checksum\n",
      "13a51f13be8e: Download complete\n",
      "c6cef1aa2170: Download complete\n",
      "07471e81507f: Verifying Checksum\n",
      "07471e81507f: Download complete\n",
      "07471e81507f: Pull complete\n",
      "c6cef1aa2170: Pull complete\n",
      "13a51f13be8e: Pull complete\n",
      "5402d6c1eb0a: Verifying Checksum\n",
      "5402d6c1eb0a: Download complete\n",
      "def39d67a1a7: Verifying Checksum\n",
      "def39d67a1a7: Download complete\n",
      "def39d67a1a7: Pull complete\n",
      "ba9fd0742332: Download complete\n",
      "c018bad89663: Download complete\n",
      "9d775c430e09: Download complete\n",
      "c39854f8e166: Download complete\n",
      "2dd5f1bea0e7: Download complete\n",
      "a8367252e08e: Verifying Checksum\n",
      "a8367252e08e: Download complete\n",
      "a8367252e08e: Pull complete\n",
      "5402d6c1eb0a: Pull complete\n",
      "c018bad89663: Pull complete\n",
      "ba9fd0742332: Pull complete\n",
      "9d775c430e09: Pull complete\n",
      "c39854f8e166: Pull complete\n",
      "2dd5f1bea0e7: Pull complete\n",
      "Digest: sha256:f47dac8541512e04c75c2a620bbaa29070425e13b1d9213813da525dd38ee3ef\n",
      "Status: Downloaded newer image for luksa/kiada:0.1\n",
      "docker.io/luksa/kiada:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "What's Next?\n",
      "  View summary of image vulnerabilities and recommendations → docker scout quickview luksa/kiada:0.1\n"
     ]
    }
   ],
   "source": [
    "# quay.io/luksa/kiada:0.1\n",
    "!docker pull luksa/kiada:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/kiada created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create deployment kiada --image=luksa/kiada:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "kiada   1/1     1            1           8s\n"
     ]
    }
   ],
   "source": [
    "# 查看deployment\n",
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                    READY   STATUS    RESTARTS   AGE\n",
      "kiada-845878c8f-g8m6n   1/1     Running   0          14s\n"
     ]
    }
   ],
   "source": [
    "# 查看pod\n",
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   TYPE      REASON                    OBJECT                       MESSAGE\n",
      "46m         Normal    Starting                  node/docker-desktop          Starting kubelet.\n",
      "46m         Warning   InvalidDiskCapacity       node/docker-desktop          invalid capacity 0 on image filesystem\n",
      "46m         Normal    NodeHasSufficientMemory   node/docker-desktop          Node docker-desktop status is now: NodeHasSufficientMemory\n",
      "46m         Normal    NodeHasNoDiskPressure     node/docker-desktop          Node docker-desktop status is now: NodeHasNoDiskPressure\n",
      "46m         Normal    NodeHasSufficientPID      node/docker-desktop          Node docker-desktop status is now: NodeHasSufficientPID\n",
      "46m         Normal    NodeAllocatableEnforced   node/docker-desktop          Updated Node Allocatable limit across pods\n",
      "46m         Normal    Starting                  node/docker-desktop          \n",
      "46m         Normal    RegisteredNode            node/docker-desktop          Node docker-desktop event: Registered Node docker-desktop in Controller\n",
      "2m13s       Normal    Scheduled                 pod/kiada-845878c8f-g8m6n    Successfully assigned default/kiada-845878c8f-g8m6n to docker-desktop\n",
      "2m12s       Normal    Pulled                    pod/kiada-845878c8f-g8m6n    Container image \"luksa/kiada:0.1\" already present on machine\n",
      "2m12s       Normal    Created                   pod/kiada-845878c8f-g8m6n    Created container kiada\n",
      "2m12s       Normal    Started                   pod/kiada-845878c8f-g8m6n    Started container kiada\n",
      "2m13s       Normal    SuccessfulCreate          replicaset/kiada-845878c8f   Created pod: kiada-845878c8f-g8m6n\n",
      "2m13s       Normal    ScalingReplicaSet         deployment/kiada             Scaled up replica set kiada-845878c8f to 1\n"
     ]
    }
   ],
   "source": [
    "# 查看事件\n",
    "!kubectl get events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada - Kubernetes in Action Demo Application\n",
      "---------------------------------------------\n",
      "Kiada 0.1 starting...\n",
      "Local hostname is kiada-845878c8f-g8m6n\n",
      "Listening on port 8080\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs kiada-845878c8f-g8m6n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/kiada exposed\n"
     ]
    }
   ],
   "source": [
    "# 创建服务\n",
    "!kubectl expose deployment kiada --type=LoadBalancer --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n",
      "kiada        LoadBalancer   10.99.102.164   localhost     8080:31104/TCP   19s\n",
      "kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP          42d\n"
     ]
    }
   ],
   "source": [
    "# 查看服务\n",
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n",
      "kiada   LoadBalancer   10.99.102.164   localhost     8080:31104/TCP   50s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada version 0.1. Request processed by \"kiada-845878c8f-g8m6n\". Client IP: ::ffff:192.168.65.4\n"
     ]
    }
   ],
   "source": [
    "# Windows\n",
    "!curl -s %LOCALHOST%:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/kiada scaled\n"
     ]
    }
   ],
   "source": [
    "# 扩容\n",
    "!kubectl scale deployment kiada --replicas=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "kiada   3/3     3            3           18m\n",
      "NAME                    READY   STATUS    RESTARTS   AGE     IP          NODE             NOMINATED NODE   READINESS GATES\n",
      "kiada-845878c8f-5gjpd   1/1     Running   0          2m22s   10.1.0.96   docker-desktop   <none>           <none>\n",
      "kiada-845878c8f-g8m6n   1/1     Running   0          18m     10.1.0.94   docker-desktop   <none>           <none>\n",
      "kiada-845878c8f-js6xh   1/1     Running   0          2m22s   10.1.0.95   docker-desktop   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments\n",
    "!kubectl get pods -o wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada version 0.1. Request processed by \"kiada-845878c8f-5gjpd\". Client IP: ::ffff:192.168.65.4\n",
      "Kiada version 0.1. Request processed by \"kiada-845878c8f-g8m6n\". Client IP: ::ffff:192.168.65.4\n",
      "Kiada version 0.1. Request processed by \"kiada-845878c8f-js6xh\". Client IP: ::ffff:192.168.65.4\n"
     ]
    }
   ],
   "source": [
    "# 展示负载均衡\n",
    "!curl -s %LOCALHOST%:8080\n",
    "!curl -s %LOCALHOST%:8080\n",
    "!curl -s %LOCALHOST%:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"kiada\" deleted\n"
     ]
    }
   ],
   "source": [
    "# 清理\n",
    "!kubectl delete deployment kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service \"kiada\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete service kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No resources found in default namespace.\n",
      "No resources found in default namespace.\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments\n",
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada created\n"
     ]
    }
   ],
   "source": [
    "# 部署pod\n",
    "!kubectl apply -f pod.kiada.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    READY   STATUS    RESTARTS   AGE\n",
      "kiada   1/1     Running   0          2m23s\n"
     ]
    }
   ],
   "source": [
    "# 查看pod状态\n",
    "!kubectl get pods kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  annotations:\n",
      "    kubectl.kubernetes.io/last-applied-configuration: |\n",
      "      {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"kiada\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"luksa/kiada:0.1\",\"imagePullPolicy\":\"Always\",\"name\":\"kinda\",\"ports\":[{\"containerPort\":8080}]}]}}\n",
      "  creationTimestamp: \"2025-03-07T09:33:08Z\"\n",
      "  name: kiada\n",
      "  namespace: default\n",
      "  resourceVersion: \"633606\"\n",
      "  uid: c5d5cec1-6414-453c-ab03-48a9ee40e1b4\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/kiada:0.1\n",
      "    imagePullPolicy: Always\n",
      "    name: kinda\n",
      "    ports:\n",
      "    - containerPort: 8080\n",
      "      protocol: TCP\n",
      "    resources: {}\n",
      "    terminationMessagePath: /dev/termination-log\n",
      "    terminationMessagePolicy: File\n",
      "    volumeMounts:\n",
      "    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n",
      "      name: kube-api-access-tw4zl\n",
      "      readOnly: true\n",
      "  dnsPolicy: ClusterFirst\n",
      "  enableServiceLinks: true\n",
      "  nodeName: docker-desktop\n",
      "  preemptionPolicy: PreemptLowerPriority\n",
      "  priority: 0\n",
      "  restartPolicy: Always\n",
      "  schedulerName: default-scheduler\n",
      "  securityContext: {}\n",
      "  serviceAccount: default\n",
      "  serviceAccountName: default\n",
      "  terminationGracePeriodSeconds: 30\n",
      "  tolerations:\n",
      "  - effect: NoExecute\n",
      "    key: node.kubernetes.io/not-ready\n",
      "    operator: Exists\n",
      "    tolerationSeconds: 300\n",
      "  - effect: NoExecute\n",
      "    key: node.kubernetes.io/unreachable\n",
      "    operator: Exists\n",
      "    tolerationSeconds: 300\n",
      "  volumes:\n",
      "  - name: kube-api-access-tw4zl\n",
      "    projected:\n",
      "      defaultMode: 420\n",
      "      sources:\n",
      "      - serviceAccountToken:\n",
      "          expirationSeconds: 3607\n",
      "          path: token\n",
      "      - configMap:\n",
      "          items:\n",
      "          - key: ca.crt\n",
      "            path: ca.crt\n",
      "          name: kube-root-ca.crt\n",
      "      - downwardAPI:\n",
      "          items:\n",
      "          - fieldRef:\n",
      "              apiVersion: v1\n",
      "              fieldPath: metadata.namespace\n",
      "            path: namespace\n",
      "status:\n",
      "  conditions:\n",
      "  - lastProbeTime: null\n",
      "    lastTransitionTime: \"2025-03-07T09:33:08Z\"\n",
      "    status: \"True\"\n",
      "    type: Initialized\n",
      "  - lastProbeTime: null\n",
      "    lastTransitionTime: \"2025-03-07T09:33:22Z\"\n",
      "    status: \"True\"\n",
      "    type: Ready\n",
      "  - lastProbeTime: null\n",
      "    lastTransitionTime: \"2025-03-07T09:33:22Z\"\n",
      "    status: \"True\"\n",
      "    type: ContainersReady\n",
      "  - lastProbeTime: null\n",
      "    lastTransitionTime: \"2025-03-07T09:33:08Z\"\n",
      "    status: \"True\"\n",
      "    type: PodScheduled\n",
      "  containerStatuses:\n",
      "  - containerID: docker://63a8c4dee12c88f54f82b25fa730788a443d0518a5c30d91d80de4f5dfb4a494\n",
      "    image: luksa/kiada:0.1\n",
      "    imageID: docker-pullable://luksa/kiada@sha256:f47dac8541512e04c75c2a620bbaa29070425e13b1d9213813da525dd38ee3ef\n",
      "    lastState: {}\n",
      "    name: kinda\n",
      "    ready: true\n",
      "    restartCount: 0\n",
      "    started: true\n",
      "    state:\n",
      "      running:\n",
      "        startedAt: \"2025-03-07T09:33:21Z\"\n",
      "  hostIP: 192.168.65.4\n",
      "  phase: Running\n",
      "  podIP: 10.1.0.112\n",
      "  podIPs:\n",
      "  - ip: 10.1.0.112\n",
      "  qosClass: BestEffort\n",
      "  startTime: \"2025-03-07T09:33:08Z\"\n"
     ]
    }
   ],
   "source": [
    "# 查看pod详情\n",
    "!kubectl get pod kiada -o yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             kiada\n",
      "Namespace:        default\n",
      "Priority:         0\n",
      "Service Account:  default\n",
      "Node:             docker-desktop/192.168.65.4\n",
      "Start Time:       Fri, 07 Mar 2025 17:33:08 +0800\n",
      "Labels:           <none>\n",
      "Annotations:      <none>\n",
      "Status:           Running\n",
      "IP:               10.1.0.112\n",
      "IPs:\n",
      "  IP:  10.1.0.112\n",
      "Containers:\n",
      "  kinda:\n",
      "    Container ID:   docker://63a8c4dee12c88f54f82b25fa730788a443d0518a5c30d91d80de4f5dfb4a494\n",
      "    Image:          luksa/kiada:0.1\n",
      "    Image ID:       docker-pullable://luksa/kiada@sha256:f47dac8541512e04c75c2a620bbaa29070425e13b1d9213813da525dd38ee3ef\n",
      "    Port:           8080/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Running\n",
      "      Started:      Fri, 07 Mar 2025 17:33:21 +0800\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:    <none>\n",
      "    Mounts:\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tw4zl (ro)\n",
      "Conditions:\n",
      "  Type              Status\n",
      "  Initialized       True \n",
      "  Ready             True \n",
      "  ContainersReady   True \n",
      "  PodScheduled      True \n",
      "Volumes:\n",
      "  kube-api-access-tw4zl:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   BestEffort\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type    Reason     Age    From               Message\n",
      "  ----    ------     ----   ----               -------\n",
      "  Normal  Scheduled  2m27s  default-scheduler  Successfully assigned default/kiada to docker-desktop\n",
      "  Normal  Pulling    2m27s  kubelet            Pulling image \"luksa/kiada:0.1\"\n",
      "  Normal  Pulled     2m15s  kubelet            Successfully pulled image \"luksa/kiada:0.1\" in 12.585247931s (12.585266825s including waiting)\n",
      "  Normal  Created    2m15s  kubelet            Created container kinda\n",
      "  Normal  Started    2m15s  kubelet            Started container kinda\n"
     ]
    }
   ],
   "source": [
    "# 描述pod详情\n",
    "!kubectl describe pod kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   TYPE     REASON      OBJECT           MESSAGE\n",
      "13m         Normal   Scheduled   pod/client-pod   Successfully assigned default/client-pod to docker-desktop\n",
      "13m         Normal   Pulling     pod/client-pod   Pulling image \"curlimages/curl\"\n",
      "13m         Normal   Pulled      pod/client-pod   Successfully pulled image \"curlimages/curl\" in 7.036316877s (7.036380715s including waiting)\n",
      "13m         Normal   Created     pod/client-pod   Created container client-pod\n",
      "13m         Normal   Started     pod/client-pod   Started container client-pod\n",
      "2m30s       Normal   Scheduled   pod/kiada        Successfully assigned default/kiada to docker-desktop\n",
      "2m30s       Normal   Pulling     pod/kiada        Pulling image \"luksa/kiada:0.1\"\n",
      "2m18s       Normal   Pulled      pod/kiada        Successfully pulled image \"luksa/kiada:0.1\" in 12.585247931s (12.585266825s including waiting)\n",
      "2m18s       Normal   Created     pod/kiada        Created container kinda\n",
      "2m18s       Normal   Started     pod/kiada        Started container kinda\n",
      "21m         Normal   Scheduled   pod/kinda        Successfully assigned default/kinda to docker-desktop\n",
      "21m         Normal   Pulling     pod/kinda        Pulling image \"luksa/kiada:0.1\"\n",
      "21m         Normal   Pulled      pod/kinda        Successfully pulled image \"luksa/kiada:0.1\" in 9.933330124s (9.933398444s including waiting)\n",
      "21m         Normal   Created     pod/kinda        Created container kinda\n",
      "21m         Normal   Started     pod/kinda        Started container kinda\n",
      "4m49s       Normal   Killing     pod/kinda        Stopping container kinda\n"
     ]
    }
   ],
   "source": [
    "# 查看事件\n",
    "!kubectl get events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    READY   STATUS    RESTARTS   AGE     IP           NODE             NOMINATED NODE   READINESS GATES\n",
      "kiada   1/1     Running   0          2m42s   10.1.0.112   docker-desktop   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "# 获取pod的IP\n",
    "!kubectl get pod kiada -o wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fail!\n",
    "!curl -s 10.1.0.112:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada version 0.1. Request processed by \"kiada\". Client IP: ::ffff:10.1.0.113\n",
      "pod \"client-pod\" deleted\n"
     ]
    }
   ],
   "source": [
    "# 使用一次性客户端pod\n",
    "# or: tutum/curl\n",
    "!kubectl run --image=curlimages/curl -it --restart=Never --rm client-pod curl 10.1.0.112:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用端口转发: 在另一个console中运行\n",
    "!kubectl port-forward kiada 18080:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada version 0.1. Request processed by \"kiada\". Client IP: ::ffff:127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "!curl -s localhost:18080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received request for /stylesheet.css from ::ffff:127.0.0.1\n",
      "Received request for /html from ::ffff:127.0.0.1\n",
      "Received request for /stylesheet.css from ::ffff:127.0.0.1\n",
      "Received request for /cover.png from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:127.0.0.1\n",
      "Received request for /html from ::ffff:127.0.0.1\n",
      "Received request for /stylesheet.css from ::ffff:127.0.0.1\n",
      "Received request for /cover.png from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:10.1.0.113\n",
      "Received request for / from ::ffff:127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "# 查看pod日志\n",
    "# !kubectl logs kiada\n",
    "# 末尾\n",
    "# !kubectl logs kiada -f\n",
    "# 时间戳\n",
    "# !kubectl logs kiada --timestamps=true\n",
    "# 查看最近的日志\n",
    "# !kubectl logs kiada --since=2m\n",
    "# 查看最后几行日志\n",
    "!kubectl logs kiada --tail=10\n",
    "\n",
    "# Windows日志文件路径\n",
    "# \\\\wsl$\\docker-desktop-data\\data\\docker\\containers\\63a8c4dee12c88f54f82b25fa730788a443d0518a5c30d91d80de4f5dfb4a494\\63a8c4dee12c88f54f82b25fa730788a443d0518a5c30d91d80de4f5dfb4a494-json.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从容器中拷贝文件\n",
    "!kubectl cp kiada:html/index.html index.html\n",
    "# 拷贝文件到容器\n",
    "!kubectl cp index.html kiada:html/index2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
      "root           1  0.0  0.2 583492 36244 ?        Ssl  09:33   0:00 node app.js\n",
      "root          19  0.0  0.0   3872  3176 pts/0    Ss+  09:33   0:00 bash\n",
      "root          50  0.0  0.0   7644  2684 ?        Rs   09:52   0:00 ps aux\n"
     ]
    }
   ],
   "source": [
    "# 在容器中执行命令\n",
    "!kubectl exec kiada -- ps aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada version 0.1. Request processed by \"kiada\". Client IP: ::ffff:127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec kiada -- curl -s localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行交互式shell\n",
    "# !kubectl exec -it kiada -- bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# 查看标准输出\n",
    "# -i: 传入标准输入, example kiada-stdin\n",
    "# !kubectl attach kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you don't see a command prompt, try pressing enter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada\" deleted\n"
     ]
    }
   ],
   "source": [
    "# 删除pod\n",
    "!kubectl delete -f pod.kiada.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple containers in a pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-ssl created\n"
     ]
    }
   ],
   "source": [
    "# pod中运行多个容器\n",
    "!kubectl apply -f pod.kiada-ssl.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl port-forward kiada-ssl 8080 8443 9901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada version 0.2. Request processed by \"kiada-ssl\". Client IP: ::ffff:127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "!curl -k -s https://localhost:8443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada version 0.2. Request processed by \"kiada-ssl\". Client IP: ::ffff:127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "!curl -s -k https://example.spike.com:8443 --resolve example.spike.com:8443:127.0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada - Kubernetes in Action Demo Application\n",
      "---------------------------------------------\n",
      "Kiada 0.2 starting...\n",
      "Local hostname is kiada-ssl\n",
      "Listening on port 8080\n",
      "Received request for / from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "# 查看容器日志\n",
    "!kubectl logs kiada-ssl -c kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      {}\n",
      "[2025-03-07 10:10:30.406][1][info][config] [source/server/configuration_impl.cc:103] loading tracing configuration\n",
      "[2025-03-07 10:10:30.406][1][info][config] [source/server/configuration_impl.cc:69] loading 0 static secret(s)\n",
      "[2025-03-07 10:10:30.406][1][info][config] [source/server/configuration_impl.cc:75] loading 1 cluster(s)\n",
      "[2025-03-07 10:10:30.407][1][info][upstream] [source/common/upstream/cluster_manager_impl.cc:171] cm init: all clusters initialized\n",
      "[2025-03-07 10:10:30.407][1][info][config] [source/server/configuration_impl.cc:79] loading 1 listener(s)\n",
      "[2025-03-07 10:10:30.411][1][info][config] [source/server/configuration_impl.cc:129] loading stats sink configuration\n",
      "[2025-03-07 10:10:30.411][1][info][main] [source/server/server.cc:533] all clusters initialized. initializing init manager\n",
      "[2025-03-07 10:10:30.411][1][info][config] [source/server/listener_manager_impl.cc:725] all dependencies initialized. starting workers\n",
      "[2025-03-07 10:10:30.425][1][info][main] [source/server/server.cc:554] starting main dispatch loop\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs kiada-ssl -c envoy --tail 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      {}\n",
      "[2025-03-07 10:10:30.406][1][info][config] [source/server/configuration_impl.cc:103] loading tracing configuration\n",
      "[2025-03-07 10:10:30.406][1][info][config] [source/server/configuration_impl.cc:69] loading 0 static secret(s)\n",
      "[2025-03-07 10:10:30.406][1][info][config] [source/server/configuration_impl.cc:75] loading 1 cluster(s)\n",
      "[2025-03-07 10:10:30.407][1][info][upstream] [source/common/upstream/cluster_manager_impl.cc:171] cm init: all clusters initialized\n",
      "[2025-03-07 10:10:30.407][1][info][config] [source/server/configuration_impl.cc:79] loading 1 listener(s)\n",
      "[2025-03-07 10:10:30.411][1][info][config] [source/server/configuration_impl.cc:129] loading stats sink configuration\n",
      "[2025-03-07 10:10:30.411][1][info][main] [source/server/server.cc:533] all clusters initialized. initializing init manager\n",
      "[2025-03-07 10:10:30.411][1][info][config] [source/server/listener_manager_impl.cc:725] all dependencies initialized. starting workers\n",
      "[2025-03-07 10:10:30.425][1][info][main] [source/server/server.cc:554] starting main dispatch loop\n",
      "Kiada - Kubernetes in Action Demo Application\n",
      "---------------------------------------------\n",
      "Kiada 0.2 starting...\n",
      "Local hostname is kiada-ssl\n",
      "Listening on port 8080\n",
      "Received request for / from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs kiada-ssl --all-containers --tail 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada - Kubernetes in Action Demo Application\n",
      "---------------------------------------------\n",
      "Kiada 0.2 starting...\n",
      "Local hostname is kiada-ssl\n",
      "Listening on port 8080\n",
      "Received request for / from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:127.0.0.1\n",
      "Received request for / from ::ffff:127.0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulted container \"kiada\" out of: kiada, envoy\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs kiada-ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\n",
      "boot\n",
      "dev\n",
      "docker-entrypoint.sh\n",
      "etc\n",
      "home\n",
      "lib\n",
      "lib64\n",
      "media\n",
      "mnt\n",
      "opt\n",
      "proc\n",
      "root\n",
      "run\n",
      "sbin\n",
      "srv\n",
      "sys\n",
      "tmp\n",
      "usr\n",
      "var\n"
     ]
    }
   ],
   "source": [
    "# 在容器中运行命令\n",
    "# -it\n",
    "!kubectl exec kiada-ssl -c envoy -- ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-ssl\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod kiada-ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-init created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada-init.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   TYPE      REASON      OBJECT           MESSAGE\n",
      "60m         Normal    Scheduled   pod/client-pod   Successfully assigned default/client-pod to docker-desktop\n",
      "60m         Normal    Pulling     pod/client-pod   Pulling image \"curlimages/curl\"\n",
      "60m         Normal    Pulled      pod/client-pod   Successfully pulled image \"curlimages/curl\" in 7.036316877s (7.036380715s including waiting)\n",
      "60m         Normal    Created     pod/client-pod   Created container client-pod\n",
      "60m         Normal    Started     pod/client-pod   Started container client-pod\n",
      "46m         Normal    Scheduled   pod/client-pod   Successfully assigned default/client-pod to docker-desktop\n",
      "45m         Normal    Pulling     pod/client-pod   Pulling image \"curlimages/curl\"\n",
      "46m         Warning   Failed      pod/client-pod   Failed to pull image \"curlimages/curl\": rpc error: code = Unknown desc = Error response from daemon: Head \"https://registry-1.docker.io/v2/curlimages/curl/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%3Acurlimages%2Fcurl%3Apull&service=registry.docker.io\": net/http: TLS handshake timeout\n",
      "46m         Warning   Failed      pod/client-pod   Error: ErrImagePull\n",
      "46m         Normal    BackOff     pod/client-pod   Back-off pulling image \"curlimages/curl\"\n",
      "46m         Warning   Failed      pod/client-pod   Error: ImagePullBackOff\n",
      "45m         Normal    Pulled      pod/client-pod   Successfully pulled image \"curlimages/curl\" in 3.922023395s (3.922068049s including waiting)\n",
      "45m         Normal    Created     pod/client-pod   Created container client-pod\n",
      "45m         Normal    Started     pod/client-pod   Started container client-pod\n",
      "39s         Normal    Scheduled   pod/kiada-init   Successfully assigned default/kiada-init to docker-desktop\n",
      "39s         Normal    Pulled      pod/kiada-init   Container image \"luksa/init-demo:0.1\" already present on machine\n",
      "39s         Normal    Created     pod/kiada-init   Created container init-demo\n",
      "39s         Normal    Started     pod/kiada-init   Started container init-demo\n",
      "34s         Normal    Pulled      pod/kiada-init   Container image \"luksa/network-connectivity-checker:0.1\" already present on machine\n",
      "34s         Normal    Created     pod/kiada-init   Created container network-check\n",
      "34s         Normal    Started     pod/kiada-init   Started container network-check\n",
      "33s         Normal    Pulled      pod/kiada-init   Container image \"luksa/kiada:0.2\" already present on machine\n",
      "33s         Normal    Created     pod/kiada-init   Created container kiada\n",
      "33s         Normal    Started     pod/kiada-init   Started container kiada\n",
      "33s         Normal    Pulled      pod/kiada-init   Container image \"luksa/kiada-ssl-proxy:0.1\" already present on machine\n",
      "33s         Normal    Created     pod/kiada-init   Created container envoy\n",
      "33s         Normal    Started     pod/kiada-init   Started container envoy\n",
      "11m         Normal    Scheduled   pod/kiada-ssl    Successfully assigned default/kiada-ssl to docker-desktop\n",
      "11m         Normal    Pulled      pod/kiada-ssl    Container image \"luksa/kiada:0.2\" already present on machine\n",
      "11m         Normal    Created     pod/kiada-ssl    Created container kiada\n",
      "11m         Normal    Started     pod/kiada-ssl    Started container kiada\n",
      "11m         Normal    Pulled      pod/kiada-ssl    Container image \"luksa/kiada-ssl-proxy:0.1\" already present on machine\n",
      "11m         Normal    Created     pod/kiada-ssl    Created container envoy\n",
      "11m         Normal    Started     pod/kiada-ssl    Started container envoy\n",
      "79s         Normal    Killing     pod/kiada-ssl    Stopping container kiada\n",
      "79s         Normal    Killing     pod/kiada-ssl    Stopping container envoy\n",
      "49m         Normal    Scheduled   pod/kiada        Successfully assigned default/kiada to docker-desktop\n",
      "49m         Normal    Pulling     pod/kiada        Pulling image \"luksa/kiada:0.1\"\n",
      "49m         Normal    Pulled      pod/kiada        Successfully pulled image \"luksa/kiada:0.1\" in 12.585247931s (12.585266825s including waiting)\n",
      "49m         Normal    Created     pod/kiada        Created container kinda\n",
      "49m         Normal    Started     pod/kiada        Started container kinda\n",
      "16m         Normal    Killing     pod/kiada        Stopping container kinda\n",
      "51m         Normal    Killing     pod/kinda        Stopping container kinda\n"
     ]
    }
   ],
   "source": [
    "# -w: watch\n",
    "!kubectl get events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         READY   STATUS    RESTARTS   AGE\n",
      "kiada-init   2/2     Running   0          96s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod kiada-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization started...\n",
      "Performing initialization procedure 1/5\n",
      "Performing initialization procedure 2/5\n",
      "Performing initialization procedure 3/5\n",
      "Performing initialization procedure 4/5\n",
      "Performing initialization procedure 5/5\n",
      "Initialization complete!\n",
      "Checking network connectivity to 1.1.1.1 ...\n",
      "Host appears to be reachable\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs kiada-init -c init-demo\n",
    "!kubectl logs kiada-init -c network-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-init\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod kiada-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"kiada-init\" not found\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods kiada-init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pod lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phases:\n",
    "- **Pending**: After you create the Pod object, this is its initial phase. Until the pod is scheduled to a node and the images of its containers are pulled and started, it remains in this phase.\n",
    "- **Running**: At least one of the pod’s containers is running.\n",
    "- **Succeeded**: Pods that aren’t intended to run indefinitely are marked as Succeeded when all their containers complete successfully.\n",
    "- **Failed**: When a pod is not configured to run indefinitely and at least one of its containers terminates unsuccessfully, the pod is marked as Failed.\n",
    "- **Unknown**: The state of the pod is unknown because the Kubelet has stopped reporting communicating with the API server. Possibly the worker node has failed or has disconnected from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada unchanged\n",
      "  phase: Running\n",
      "NAME    READY   STATUS    RESTARTS        AGE\n",
      "kiada   1/1     Running   1 (5m24s ago)   14h\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada.yaml\n",
    "!kubectl get pod kiada -o yaml | findstr phase\n",
    "!kubectl get pod kiada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pod conditions: A pod’s conditions indicate whether a pod has reached a certain state or not, and why that’s the case.\n",
    "- **PodScheduled**:\tIndicates whether or not the pod has been scheduled to a node.\n",
    "- **Initialized**: The pod’s init containers have all completed successfully.\n",
    "- **ContainersReady**: All containers in the pod indicate that they are ready. This is a necessary but not sufficient condition for the entire pod to be ready.\n",
    "- **Ready**: The pod is ready to provide services to its clients. The containers in the pod and the pod’s readiness gates are all reporting that they are ready. Note: this is explained in chapter 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe pod kiada | grep Conditions: -A5\n",
    "# Conditions:\n",
    "#   Type              Status\n",
    "#   Initialized       True\n",
    "#   Ready             True\n",
    "#   ContainersReady   True\n",
    "#   PodScheduled      True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"lastProbeTime\": null,\n",
      "    \"lastTransitionTime\": \"2025-03-07T10:28:20Z\",\n",
      "    \"status\": \"True\",\n",
      "    \"type\": \"Initialized\"\n",
      "  },\n",
      "  {\n",
      "    \"lastProbeTime\": null,\n",
      "    \"lastTransitionTime\": \"2025-03-08T01:00:49Z\",\n",
      "    \"status\": \"True\",\n",
      "    \"type\": \"Ready\"\n",
      "  },\n",
      "  {\n",
      "    \"lastProbeTime\": null,\n",
      "    \"lastTransitionTime\": \"2025-03-08T01:00:49Z\",\n",
      "    \"status\": \"True\",\n",
      "    \"type\": \"ContainersReady\"\n",
      "  },\n",
      "  {\n",
      "    \"lastProbeTime\": null,\n",
      "    \"lastTransitionTime\": \"2025-03-07T10:28:20Z\",\n",
      "    \"status\": \"True\",\n",
      "    \"type\": \"PodScheduled\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods kiada -o json | jq .status.conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "container states:\n",
    "- **Waiting**: The container is waiting to be started. The reason and message fields indicate why the container is in this state.\n",
    "- **Running**: The container has been created and processes are running in it. The startedAt field indicates the time at which this container was started.\n",
    "- **Terminated**: The processes that had been running in the container have terminated. The startedAt and finishedAt fields indicate when the container was started and when it terminated. The exit code with which the main process terminated is in the exitCode field.\n",
    "- **Unknown**: The state of the container couldn’t be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl describe pod kiada | grep Containers: -A25\n",
    "# Containers:\n",
    "#   kinda:\n",
    "#     Container ID:   docker://6434998c4c4480f55f03d485260ef32fc7cb53a7eaa724c220769808a5660c4b\n",
    "#     Image:          luksa/kiada:0.1\n",
    "#     Image ID:       docker-pullable://luksa/kiada@sha256:f47dac8541512e04c75c2a620bbaa29070425e13b1d9213813da525dd38ee3ef\n",
    "#     Port:           8080/TCP\n",
    "#     Host Port:      0/TCP\n",
    "#     State:          Running\n",
    "#       Started:      Sat, 08 Mar 2025 09:00:49 +0800\n",
    "#     Last State:     Terminated\n",
    "#       Reason:       Error\n",
    "#       Exit Code:    255\n",
    "#       Started:      Fri, 07 Mar 2025 18:28:26 +0800\n",
    "#       Finished:     Sat, 08 Mar 2025 09:00:37 +0800\n",
    "#     Ready:          True\n",
    "#     Restart Count:  1\n",
    "#     Environment:    <none>\n",
    "#     Mounts:\n",
    "#       /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pcwn6 (ro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"containerID\": \"docker://6434998c4c4480f55f03d485260ef32fc7cb53a7eaa724c220769808a5660c4b\",\n",
      "    \"image\": \"luksa/kiada:0.1\",\n",
      "    \"imageID\": \"docker-pullable://luksa/kiada@sha256:f47dac8541512e04c75c2a620bbaa29070425e13b1d9213813da525dd38ee3ef\",\n",
      "    \"lastState\": {\n",
      "      \"terminated\": {\n",
      "        \"containerID\": \"docker://801a93c691b5dc82b5a180b3b1a05ea4ddbefdd4ad73cc73c9facde6af258267\",\n",
      "        \"exitCode\": 255,\n",
      "        \"finishedAt\": \"2025-03-08T01:00:37Z\",\n",
      "        \"reason\": \"Error\",\n",
      "        \"startedAt\": \"2025-03-07T10:28:26Z\"\n",
      "      }\n",
      "    },\n",
      "    \"name\": \"kinda\",\n",
      "    \"ready\": true,\n",
      "    \"restartCount\": 1,\n",
      "    \"started\": true,\n",
      "    \"state\": {\n",
      "      \"running\": {\n",
      "        \"startedAt\": \"2025-03-08T01:00:49Z\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods kiada -o json | jq .status.containerStatuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f pod.kiada.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## container health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-ssl created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada-ssl.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        READY   STATUS    RESTARTS   AGE\n",
      "kiada-ssl   2/2     Running   0          4s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod kiada-ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl port-forward kiada-ssl 18080:8080 8443 9901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# 结束envoy容器\n",
    "!curl -s -X POST http://localhost:9901/quitquitquit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        READY   STATUS    RESTARTS      AGE\n",
      "kiada-ssl   2/2     Running   2 (52s ago)   6m20s\n"
     ]
    }
   ],
   "source": [
    "# 重新查看直到处于Running状态\n",
    "!kubectl get pods kiada-ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "container restart policy: `restartPolicy`\n",
    "- **Always**: Container is restarted regardless of the exit code the process in the container terminates with. This is the *default* restart policy.\n",
    "- **OnFailure**: The container is restarted only if the process terminates with a non-zero exit code, which by convention indicates failure.\n",
    "- **Never**: The container is never restarted - not even when it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-ssl\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f pod.kiada-ssl.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## liveness probe\n",
    "- http GET: 返回不是错误响应时(即2xx, 3xx)认为成功\n",
    "- tcpSocket: 连接建立时认为成功\n",
    "- exec: 在容器中执行命令, exit code是0时认为成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-liveness created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada-liveness.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             READY   STATUS    RESTARTS   AGE\n",
      "kiada-liveness   2/2     Running   0          2m15s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods kiada-liveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada - Kubernetes in Action Demo Application\n",
      "---------------------------------------------\n",
      "Kiada 1.0 starting...\n",
      "Pod name is unknown\n",
      "Local hostname is kiada-liveness\n",
      "Local IP is unknown\n",
      "Running on node unknown\n",
      "Node IP is unknown\n",
      "Status message is \n",
      "Internal URL of the quotes service is localhost:8080/quote\n",
      "External URL of the quotes service is dummy/quote\n",
      "Internal URL of the quiz service is localhost:8080dummy/quiz\n",
      "External URL of the quiz service is dummy/quiz\n",
      "Listening on port 8080\n",
      "Received request for / from ::ffff:10.1.0.1\n",
      "Received request for / from ::ffff:10.1.0.1\n",
      "Received request for / from ::ffff:10.1.0.1\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs kiada-liveness -c kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-08T01:45:53.034Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:45:58.034Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:03.034Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:08.033Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:13.033Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:18.034Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:23.033Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:28.033Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:33.033Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n",
      "[2025-03-08T01:46:38.033Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.1.0.1\" \"kube-probe/1.27\" \"-\" \"10.1.0.124:9901\" \"-\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec kiada-liveness -c envoy -- tail -n 10 /var/log/envoy.admin.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# 模拟Envoy健康检查失败\n",
    "# !kubectl port-forward kiada-liveness 18080:8080 8443 9901\n",
    "!curl -s -X POST http://localhost:9901/healthcheck/fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl get envets -w\n",
    "# 0s          Warning   Unhealthy                 pod/kiada-liveness    Liveness probe failed: HTTP probe failed with statuscode: 503\n",
    "# 0s          Warning   Unhealthy                 pod/kiada-liveness    Liveness probe failed: HTTP probe failed with statuscode: 503\n",
    "# 0s          Warning   Unhealthy                 pod/kiada-liveness    Liveness probe failed: HTTP probe failed with statuscode: 503\n",
    "# 0s          Normal    Killing                   pod/kiada-liveness    Container envoy failed liveness probe, will be restarted\n",
    "# 0s          Normal    Pulled                    pod/kiada-liveness    Container image \"luksa/kiada-ssl-proxy:1.0\" already present on machine\n",
    "# 0s          Normal    Created                   pod/kiada-liveness    Created container envoy\n",
    "# 0s          Normal    Started                   pod/kiada-liveness    Started container envoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "!curl -s -X POST http://localhost:9901/healthcheck/ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"containerID\": \"docker://7739d3e9c4fdd600e7c08ca084178d732e2ad40f0a824c8a010e9fae8495dbf0\",\n",
      "    \"image\": \"luksa/kiada-ssl-proxy:1.0\",\n",
      "    \"imageID\": \"docker-pullable://luksa/kiada-ssl-proxy@sha256:4b7def942298ca4e412bd05d3466cba8cbf6a775ba87310c023ed79ffa8e3c52\",\n",
      "    \"lastState\": {\n",
      "      \"terminated\": {\n",
      "        \"containerID\": \"docker://7ada4c24300ab6cb702d4947e3e91b92bf1fe21da5d105ec8ad167fb1f6cf341\",\n",
      "        \"exitCode\": 0,\n",
      "        \"finishedAt\": \"2025-03-08T01:50:18Z\",\n",
      "        \"reason\": \"Completed\",\n",
      "        \"startedAt\": \"2025-03-08T01:45:34Z\"\n",
      "      }\n",
      "    },\n",
      "    \"name\": \"envoy\",\n",
      "    \"ready\": true,\n",
      "    \"restartCount\": 1,\n",
      "    \"started\": true,\n",
      "    \"state\": {\n",
      "      \"running\": {\n",
      "        \"startedAt\": \"2025-03-08T01:50:18Z\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"containerID\": \"docker://71f9992d104db07f56c32df95fd188c82e4a0d889067366757d15acfcf9a6433\",\n",
      "    \"image\": \"luksa/kiada:1.0\",\n",
      "    \"imageID\": \"docker-pullable://luksa/kiada@sha256:80af238cccfb3df0e2f7cc154525c57a3d268d4b0675efde283423b936c19596\",\n",
      "    \"lastState\": {},\n",
      "    \"name\": \"kiada\",\n",
      "    \"ready\": true,\n",
      "    \"restartCount\": 0,\n",
      "    \"started\": true,\n",
      "    \"state\": {\n",
      "      \"running\": {\n",
      "        \"startedAt\": \"2025-03-08T01:45:19Z\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod kiada-liveness -o json | jq .status.containerStatuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-08 01:45:34.661][1][info][upstream] [source/common/upstream/cluster_manager_impl.cc:171] cm init: all clusters initialized\n",
      "[2025-03-08 01:45:34.661][1][info][config] [source/server/configuration_impl.cc:79] loading 1 listener(s)\n",
      "[2025-03-08 01:45:34.666][1][info][config] [source/server/configuration_impl.cc:129] loading stats sink configuration\n",
      "[2025-03-08 01:45:34.666][1][info][main] [source/server/server.cc:533] all clusters initialized. initializing init manager\n",
      "[2025-03-08 01:45:34.666][1][info][config] [source/server/listener_manager_impl.cc:725] all dependencies initialized. starting workers\n",
      "[2025-03-08 01:45:34.675][1][info][main] [source/server/server.cc:554] starting main dispatch loop\n",
      "[2025-03-08 01:50:18.060][1][warning][main] [source/server/server.cc:493] caught SIGTERM\n",
      "[2025-03-08 01:50:18.060][1][info][main] [source/server/server.cc:613] shutting down server instance\n",
      "[2025-03-08 01:50:18.060][1][info][main] [source/server/server.cc:560] main dispatch loop exited\n",
      "[2025-03-08 01:50:18.064][1][info][main] [source/server/server.cc:606] exiting\n"
     ]
    }
   ],
   "source": [
    "# 查看上一个envoy容器的日志\n",
    "!kubectl logs kiada-liveness -c envoy -p --tail 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## startup probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-liveness\" deleted\n"
     ]
    }
   ],
   "source": [
    "# 添加启动探针后执行:\n",
    "!kubectl delete pod kiada-liveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-liveness created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada-liveness.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             READY   STATUS    RESTARTS   AGE\n",
      "kiada-liveness   2/2     Running   0          11s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod kiada-liveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-liveness\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod kiada-liveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## postStart, preStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/fortune created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.fortune-poststart-prestop.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      READY   STATUS    RESTARTS   AGE\n",
      "fortune   1/1     Running   0          3m20s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod fortune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those who in quarrels interpose, must often wipe a bloody nose.\n"
     ]
    }
   ],
   "source": [
    "# !kubectl port-forward fortune 18000:80\n",
    "!curl -s http://localhost:18000/quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"fortune\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod fortune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "容器接收TERM信号\n",
    "\n",
    "Dockerfile中`ENTRYPOINT`, `CMD`:\n",
    "- `ENTRYPOINT` exec形式: `ENTRYPOINT [\"/myexecutable\", \"1st-arg\", \"2nd-arg\"]`\n",
    "  - 直接调用执行文件, 启动的进程成为容器的根进程\n",
    "- `ENTRYPOINT` shell形式: `ENTRYPOINT /myexecutable 1st-arg 2nd-arg`\n",
    "  - shell作为容器的根进程, 并不传递信号给子进程\n",
    "\n",
    "[more](https://www.docker.com/blog/docker-best-practices-choosing-between-run-cmd-and-entrypoint/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three stages of the pod’s lifecycle are:\n",
    "- The **initialization** stage, during which the pod’s init containers run. `initContainers`\n",
    "- The **run** stage, in which the regular containers of the pod run. `spec`\n",
    "- The **termination** stage, in which the pod’s containers are terminated.\n",
    "  - `spec.terminationGracePeriodSeconds`: default 30s, 结束\n",
    "  - `metadata.deletionGracePeriodSeconds`: 删除, 默认是`spec.terminationGracePeriodSeconds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    READY   STATUS    RESTARTS   AGE\n",
      "kiada   1/1     Running   0          8s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada\" deleted\n"
     ]
    }
   ],
   "source": [
    "# more fast than 30s!!!\n",
    "# or with `--grace-period 10`\n",
    "!kubectl delete -f pod.kiada.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wangwei1237.github.io/Kubernetes-in-Action-Second-Edition/images/6.16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PV, PVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker pull luksa/quiz-api:0.1\n",
    "# !docker pull mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volume types:\n",
    "- emptyDir: pod存在时有效\n",
    "- hostPath: 系统级的pod, 访问node的文件系统\n",
    "- nfs\n",
    "- gcePersistentDisk, awsElasticBlockStore, azureFile\n",
    "- cephfs, cinder, fc, flexVolume, flocker, glusterfs, iscsi, portworxVolume, quobyte, rbd, scaleIO, storageos,photonPersistentDisk, vsphereVolume\n",
    "- configMap, secret, downwardAPI, projected\n",
    "- persistentVolumeClaim\n",
    "- csi: Container Storage Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emptyDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout\n"
     ]
    }
   ],
   "source": [
    "!docker pull luksa/quiz-initdb-script-installer:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull luksa/quote-writer:0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gcePersistentDisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://wangwei1237.github.io/Kubernetes-in-Action-Second-Edition/images/8.7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StorageClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `storageClassName`\n",
    "- cases: dynamic provisiong of persisten volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\n",
      "hostpath (default)   docker.io/hostpath   Delete          Immediate           false                  46d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get storageclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: storage.k8s.io/v1\n",
      "kind: StorageClass\n",
      "metadata:\n",
      "  annotations:\n",
      "    kubectl.kubernetes.io/last-applied-configuration: |\n",
      "      {\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"},\"name\":\"hostpath\"},\"provisioner\":\"docker.io/hostpath\",\"reclaimPolicy\":\"Delete\",\"volumeBindingMode\":\"Immediate\"}\n",
      "    storageclass.kubernetes.io/is-default-class: \"true\"\n",
      "  creationTimestamp: \"2025-01-21T08:52:27Z\"\n",
      "  name: hostpath\n",
      "  resourceVersion: \"382\"\n",
      "  uid: 38ddba04-2780-4c0d-81c3-cd7570caf4d8\n",
      "provisioner: docker.io/hostpath\n",
      "reclaimPolicy: Delete\n",
      "volumeBindingMode: Immediate\n"
     ]
    }
   ],
   "source": [
    "!kubectl get sc hostpath -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hostPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node-local perisistent volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConfigMap, Secret, Downward API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## command, args, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker pull luksa/kiada:0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-args created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada-args.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiada - Kubernetes in Action Demo Application\n",
      "---------------------------------------------\n",
      "Kiada 0.4 starting...\n",
      "Pod name is kiada\n",
      "Local hostname is kiada-args\n",
      "Local IP is 0.0.0.0\n",
      "Running on node unknown-node\n",
      "Node IP is 0.0.0.0\n",
      "Status message is This status message is set in the pod spec. My name is kiada. I run NodeJS version $(NODE_VERSION). - hostname is $HOSTNAME.\n",
      "Listening on port 9090\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs kiada-args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "HOSTNAME=kiada-args\n",
      "POD_NAME=kiada\n",
      "INITIAL_STATUS_MESSAGE=This status message is set in the pod spec. My name is kiada. I run NodeJS version $(NODE_VERSION). - hostname is $HOSTNAME.\n",
      "LISTEN_PORT=9090\n",
      "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
      "KUBERNETES_PORT_443_TCP_PORT=443\n",
      "KIADA_PORT_8080_TCP=tcp://10.99.102.164:8080\n",
      "KIADA_PORT_8080_TCP_PROTO=tcp\n",
      "KUBERNETES_SERVICE_HOST=10.96.0.1\n",
      "KUBERNETES_SERVICE_PORT=443\n",
      "KIADA_SERVICE_PORT=8080\n",
      "KIADA_PORT_8080_TCP_PORT=8080\n",
      "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
      "KIADA_SERVICE_HOST=10.99.102.164\n",
      "KIADA_PORT=tcp://10.99.102.164:8080\n",
      "KIADA_PORT_8080_TCP_ADDR=10.99.102.164\n",
      "KUBERNETES_PORT=tcp://10.96.0.1:443\n",
      "KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443\n",
      "KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1\n",
      "NODE_VERSION=16.11.1\n",
      "YARN_VERSION=1.22.15\n",
      "HOME=/root\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec kiada-args -- env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-args\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod kiada-args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConfigMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a config map with the kubectl create configmap command:\n",
    "- `--from-literal`\n",
    "- `--from-file`\n",
    "- `--from-env-file`\n",
    "\n",
    "Creating a config map from a YAML manifest\n",
    "\n",
    "Injecting config map values into environment variables\n",
    "- Injecting a single config map entry: `env.valueFrom.configMapKeyRef`\n",
    "- Injecting the entire config map: `envFrom.configMapRef`\n",
    "- Injecting multiple config maps\n",
    "\n",
    "Injecting config map entries into containers as files: \n",
    "- `volumes.configMap`, `volumeMounts`\n",
    "- Projecting only specific config map entries: `volumes.configMap.items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/kiada-config created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f cm.kiada-config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "data:\n",
      "  status-message: This status message is set in the kiada-config config map\n",
      "kind: ConfigMap\n",
      "metadata:\n",
      "  annotations:\n",
      "    kubectl.kubernetes.io/last-applied-configuration: |\n",
      "      {\"apiVersion\":\"v1\",\"data\":{\"status-message\":\"This status message is set in the kiada-config config map\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"name\":\"kiada-config\",\"namespace\":\"default\"}}\n",
      "  creationTimestamp: \"2025-03-09T01:46:12Z\"\n",
      "  name: kiada-config\n",
      "  namespace: default\n",
      "  resourceVersion: \"655063\"\n",
      "  uid: 4e830906-13a2-40d8-81be-59db8a6f5cf6\n"
     ]
    }
   ],
   "source": [
    "!kubectl get cm kiada-config -o yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-cm created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada-cm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL_STATUS_MESSAGE=This status message is set in the kiada-config config map\n"
     ]
    }
   ],
   "source": [
    "!kubectl exec kiada-cm -- env | findstr INITIAL_STATUS_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-cm\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f pod.kiada-cm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap \"kiada-config\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f cm.kiada-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "types of secrets:\n",
    "- Opaque\n",
    "- bootstrap.kubernetes.io/token\t\n",
    "- kubernetes.io/basic-auth\t\n",
    "- kubernetes.io/dockercfg\t\n",
    "- kubernetes.io/dockerconfigjson\t\n",
    "- kubernetes.io/service-account-token\t\n",
    "- kubernetes.io/ssh-auth\t\n",
    "- kubernetes.io/tls\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't open \"C:\\Program Files\\Common Files\\ssl\\/openssl.cnf\" for reading, No such file or directory\n",
      "E40D0000:error:80000003:system library:BIO_new_file:No such process:crypto\\bio\\bss_file.c:67:calling fopen(C:\\Program Files\\Common Files\\ssl\\/openssl.cnf, r)\n",
      "E40D0000:error:10000080:BIO routines:BIO_new_file:no such file:crypto\\bio\\bss_file.c:75:\n"
     ]
    }
   ],
   "source": [
    "# 123456\n",
    "# !openssl req -newkey rsa:2048 -x509 -keyout example-com.key -out example-com.cert -days 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic/opaque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/kiada-tls created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create secret generic kiada-tls \\\n",
    "  --from-file tls.crt=example-com.cert \\\n",
    "  --from-file tls.key=example-com.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "data:\n",
      "  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tDQpNSUlEbHpDQ0FuK2dBd0lCQWdJVUlJYVQwdEpyUFpNZnc5bldSbXVNYi95YnhiQXdEUVlKS29aSWh2Y05BUUVMDQpCUUF3V3pFTE1Ba0dBMVVFQmhNQ1FWVXhFekFSQmdOVkJBZ01DbE52YldVdFUzUmhkR1V4SVRBZkJnTlZCQW9NDQpHRWx1ZEdWeWJtVjBJRmRwWkdkcGRITWdVSFI1SUV4MFpERVVNQklHQTFVRUF3d0xaWGhoYlhCc1pTNWpiMjB3DQpIaGNOTWpVd016QTVNREl4TURBNFdoY05Nall3TXpBNU1ESXhNREE0V2pCYk1Rc3dDUVlEVlFRR0V3SkJWVEVUDQpNQkVHQTFVRUNBd0tVMjl0WlMxVGRHRjBaVEVoTUI4R0ExVUVDZ3dZU1c1MFpYSnVaWFFnVjJsa1oybDBjeUJRDQpkSGtnVEhSa01SUXdFZ1lEVlFRRERBdGxlR0Z0Y0d4bExtTnZiVENDQVNJd0RRWUpLb1pJaHZjTkFRRUJCUUFEDQpnZ0VQQURDQ0FRb0NnZ0VCQUw4aFB4UWVTdldFdWF3ZzEyNFErb0w0YVFQL3RFdkZSVmUzVko3dXN0V3FicG50DQpYV2d0QS9La25iMC9id0ZxaXEzMHBqV2YyQ0F6anptK1hoMEx0ekJNSldsT09NL2x3bm9ibk5WcGIrbGZzcVhSDQpEMC9zc01hT2w5K0NlaFh5SHQ1Tno3YUNIU0F5TmNiQkgrUUJjUDhtQkFGMFdkc0d0empnZGVzZUx1UytJOFlVDQo4TWJBTW9lVWtXa3poeVBEU3IwTm5kRmV2TUU0V21zTk8ybUlpc2pBVk4rci96VEJIZWdUc1RZNDJLVWdZNm8yDQo3YkIwd2FKS0hiZ2ZxZGpmeWhMZW9Ha2xEWGk3eTFyeERnb0NxUWwwYTF6T3dyd1BIN0VPejdXUmo3ZUx0T0MrDQpkUFlPRnJTSDZHZmJCQUR3UzZaZFpjN0FpQzFuM3VEUGxVWFZKRmNDQXdFQUFhTlRNRkV3SFFZRFZSME9CQllFDQpGR2QzWnoxb29oTjNxQmFvUW9UNVY2NkxPZ1F1TUI4R0ExVWRJd1FZTUJhQUZHZDNaejFvb2hOM3FCYW9Rb1Q1DQpWNjZMT2dRdU1BOEdBMVVkRXdFQi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFBd29CNkt2DQo3TlJiL3MrUlh2dkFaNkxEVVh1UW5SamlxMkh6Y2F0T1RWMDdGekFQT0pGak5vYVA4Y3Noa2piaTEvQ1h4UWh0DQp4UUllclJUQkZEYXhBaDNUOFpYVHJvejV2MlhoWXlJNkRsZGpCUlBKUnM5dm5UTVUzL1hUL0QzQ3dSRk0yblRPDQpseXhvUTJyQmFTeDYvdEdzZWZVRytiKzJFeWVQSjBWMkFJN2loM2dCMEtLbHZJaGVYZWhkZ2dWbi83VkZUamhWDQpJN3pwekJ6QmtOT2J2MUh1dWpTMWpXOVQ2Wm91cXNEaExyMndPOExLT3Bqb0ZhaVArUGh6TFpjeml6RTlQMW1XDQpQRHNtZ0ErNEZkcmNpVXFHWW9na3lOZHpBZjJ5RG91TWlLK0pFaW12U2hRc1ZKZzd1L01vUTA4ZmdsdElUaHhlDQp2YlFmTnBjLzBocWlPN3c9DQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tDQo=\n",
      "  tls.key: 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\n",
      "kind: Secret\n",
      "metadata:\n",
      "  creationTimestamp: \"2025-03-09T02:12:00Z\"\n",
      "  name: kiada-tls\n",
      "  namespace: default\n",
      "  resourceVersion: \"657144\"\n",
      "  uid: 2383bcc7-fa32-4973-8ed7-c65d5f123f4c\n",
      "type: Opaque\n"
     ]
    }
   ],
   "source": [
    "!kubectl get secret kiada-tls -o yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "data:\n",
      "  pass: bXktcGFzc3dvcmQ=\n",
      "  user: bXktdXNlcm5hbWU=\n",
      "kind: Secret\n",
      "metadata:\n",
      "  creationTimestamp: null\n",
      "  name: my-credentials\n"
     ]
    }
   ],
   "source": [
    "# 创建YAML\n",
    "!kubectl create secret generic my-credentials \\\n",
    "  --from-literal user=my-username \\\n",
    "  --from-literal pass=my-password \\\n",
    "  --dry-run=client -o yaml\n",
    "# or using `stringData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret \"kiada-tls\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete secret kiada-tls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: tls: failed to parse private key\n"
     ]
    }
   ],
   "source": [
    "!kubectl create secret tls kiada-tls \\\n",
    "  --cert example-com.cert \\\n",
    "  --key example-com.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有密码时, 使用解密后的key\n",
    "# $ openssl rsa -in example-com.key -out un-example-com.key\n",
    "!ren example-com.key example-com.key.bak\n",
    "!ren un-example-com.key example-com.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret \"kiada-tls\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete secret kiada-tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/kiada-tls created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create secret tls kiada-tls \\\n",
    "  --cert example-com.cert \\\n",
    "  --key example-com.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker pull envoyproxy/envoy:v1.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/kiada-envoy-config created\n"
     ]
    }
   ],
   "source": [
    "# 创建Envoy ConfigMap\n",
    "!kubectl create configmap kiada-envoy-config \\\n",
    "  --from-file envoy.yaml=./envoy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada-secret created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f pod.kiada-secret.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl port-forward kiada-secret 18000:8080 8443 9901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada-secret\" deleted\n",
      "configmap \"kiada-envoy-config\" deleted\n",
      "secret \"kiada-tls\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f pod.kiada-secret.yaml\n",
    "!kubectl delete cm kiada-envoy-config \n",
    "!kubectl delete secret kiada-tls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DownWard API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Namespace, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   STATUS   AGE\n",
      "default                Active   46d\n",
      "kube-node-lease        Active   46d\n",
      "kube-public            Active   46d\n",
      "kube-system            Active   46d\n",
      "kubernetes-dashboard   Active   4d12h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS    RESTARTS       AGE\n",
      "coredns-5d78c9869d-rlgnm                 1/1     Running   23 (25h ago)   46d\n",
      "coredns-5d78c9869d-zkbjr                 1/1     Running   23 (25h ago)   46d\n",
      "etcd-docker-desktop                      1/1     Running   23 (25h ago)   46d\n",
      "kube-apiserver-docker-desktop            1/1     Running   23 (25h ago)   46d\n",
      "kube-controller-manager-docker-desktop   1/1     Running   23 (25h ago)   46d\n",
      "kube-proxy-plbl4                         1/1     Running   23 (25h ago)   46d\n",
      "kube-scheduler-docker-desktop            1/1     Running   23 (25h ago)   46d\n",
      "storage-provisioner                      1/1     Running   45 (25h ago)   46d\n",
      "vpnkit-controller                        1/1     Running   23 (25h ago)   46d\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kube-system get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE              NAME                                                   DATA   AGE\n",
      "default                kube-root-ca.crt                                       1      46d\n",
      "kube-node-lease        kube-root-ca.crt                                       1      46d\n",
      "kube-public            cluster-info                                           1      46d\n",
      "kube-public            kube-root-ca.crt                                       1      46d\n",
      "kube-system            coredns                                                1      46d\n",
      "kube-system            extension-apiserver-authentication                     6      46d\n",
      "kube-system            kube-apiserver-legacy-service-account-token-tracking   1      46d\n",
      "kube-system            kube-proxy                                             2      46d\n",
      "kube-system            kube-root-ca.crt                                       1      46d\n",
      "kube-system            kubeadm-config                                         1      46d\n",
      "kube-system            kubelet-config                                         1      46d\n",
      "kubernetes-dashboard   kube-root-ca.crt                                       1      4d12h\n"
     ]
    }
   ],
   "source": [
    "# 多个namespace中对象\n",
    "!kubectl get cm --all-namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/kiada created\n"
     ]
    }
   ],
   "source": [
    "# 创建namespace\n",
    "!kubectl create namespace kiada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/kiada created\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kiada apply -f pod.kiada.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME    READY   STATUS    RESTARTS   AGE\n",
      "kiada   1/1     Running   0          18s\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kiada get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"kiada\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kiada delete pod kiada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `metadata.labels`\n",
    "- `kubectl label`\n",
    "\n",
    "label selectors:\n",
    "- quality-based selectors\n",
    "```\n",
    "app=quote\n",
    "app != quote\n",
    "```\n",
    "- set-based selectors\n",
    "```\n",
    "app in (quiz, quote)\n",
    "app notin (kiada)\n",
    "app\n",
    "!app\n",
    "app=quote,rel=canary\n",
    "```\n",
    "\n",
    "maximum length of label value: 63 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching labels to nodes: `kubectl label node`\n",
    "\n",
    "Scheduling pods to nodes with specific labels: `spec.nodeSelector`\n",
    "\n",
    "Using label selectors in persistent volume claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## object annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to 256 KB: \n",
    "- NOT store identifying information\n",
    "- NOT used to filter objects\n",
    "\n",
    "- `kubectl annotate`\n",
    "- `metadata.annotations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "types of services:\n",
    "- ClusterIP\n",
    "- NodePort\n",
    "- LoadBalaner\n",
    "- ExternalName\n",
    "\n",
    "`Service.spec`:\n",
    "- `type`\n",
    "- `selector`\n",
    "- `ports`:\n",
    "  - `name`: The name of this port within the service.\n",
    "  - `port`: The port that will be exposed by this service.\n",
    "  - `targetPort`: Number or name of the port to access on the pods targeted by the service\n",
    "  - `nodePort`: The port on each node on which this service is exposed when type is NodePort or LoadBalancer.\n",
    "  - `protocol`: The IP protocol for this port.\n",
    "  - `appProtocol`: The application protocol for this port. This is used as a hint for implementations to offer richer behavior for protocols that they understand.\n",
    "- `sessionAffinity`: `None`, `ClusterIP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNS:\n",
    "- `<service-name>`\n",
    "- `<service-name>.<service-namespace>`\n",
    "- `<service-name>.<service-namespace>.svc`\n",
    "- `<service-name>.<service-namespace>.svc.cluster.local`\n",
    "\n",
    "文件: `/etc/resolv.conf`\n",
    "\n",
    "组件: `kube-dns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archived 2023-10-27\n",
    "# TODO: need another image to use nslookup, host, dig\n",
    "# !docker pull giantswarm/tiny-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NodePort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoadBalancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例: MetalLB: cluster runs on bare meta\n",
    "\n",
    "配置:\n",
    "- `loadBalancerClass`\n",
    "- `loadBalancerIP`\n",
    "- `loadBalancerSourceRanges`\n",
    "- `allocateLoadBalancerNodePorts`\n",
    "- `externalTrafficPolicy`: `Local`, `Cluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ecah endpoints object contains a list of IP and port combinations the represent the endpoint for the service.\n",
    "- `addresses`\n",
    "- `ports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         ENDPOINTS           AGE\n",
      "kiada        <none>              4d13h\n",
      "kubernetes   192.168.65.4:6443   46d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EndpointSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constraints:\n",
    "- an EndpointSlice object supports maximum 1000 endpoints.\n",
    "- by default, add up to 100 endponts to each slice, and limit to 100 port in a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          ADDRESSTYPE   PORTS     ENDPOINTS      AGE\n",
      "kiada-c7bs8   IPv4          <unset>   <unset>        4d13h\n",
      "kubernetes    IPv4          6443      192.168.65.4   46d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get endpointslices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl expose pod quiz --name quiz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR\n",
      "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   47d   <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expose sevice to the outside world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- assign an additional IP to a node, `externalIP`\n",
    "- NodePort\n",
    "- LoadBalancer: MetalLB, or cloud provider support\n",
    "- Ingress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## headless service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configure the internal DNS to return the pod IPs instead of the service's cluster IP: `ClusterIP: None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExternalName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a service that serve as an alias for an existing service: add CNAME record to the cluster DNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traffic policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `internalTrafficPolicy`: `Local`\n",
    "- topology aware hints\n",
    "  - node: with label `kubernetes.io/zone`\n",
    "  - service: annotation `service.kubernetes.io/topology-aware-hints: Auto`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## readiness probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`readinessProbe`:\n",
    "- exec\n",
    "- httpGet\n",
    "- tcpSocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ingress function:\n",
    "- the Ingress API object\n",
    "- L7 load balancer or reverse proxy: route traffic to the backend services\n",
    "  - Nginx\n",
    "  - HAProxy\n",
    "  - Envoy\n",
    "- ingress controller: monitor Ingress objects, deploy and configure the load balaner or reverse proxy\n",
    "  - GLBC: Google Cloud Platform's L7 load balancer\n",
    "  - AGIC: Azure Application Gateway Ingress Controller\n",
    "  - Nginx ingress controller: https://kubernetes.github.io/ingress-nginx/deploy/\n",
    "  - Ambassador\n",
    "  - Contour\n",
    "  - Traefik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nginx Ingress Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"kubernetes-dashboard\" chart repository\n",
      "...Successfully got an update from the \"bitnami\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n"
     ]
    }
   ],
   "source": [
    "!helm repo update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"ingress-nginx\" does not exist. Installing it now.\n",
      "NAME: ingress-nginx\n",
      "LAST DEPLOYED: Sun Mar  9 20:59:02 2025\n",
      "NAMESPACE: ingress-nginx\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n",
      "NOTES:\n",
      "The ingress-nginx controller has been installed.\n",
      "It may take a few minutes for the load balancer IP to be available.\n",
      "You can watch the status by running 'kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watch'\n",
      "\n",
      "An example Ingress that makes use of the controller:\n",
      "  apiVersion: networking.k8s.io/v1\n",
      "  kind: Ingress\n",
      "  metadata:\n",
      "    name: example\n",
      "    namespace: foo\n",
      "  spec:\n",
      "    ingressClassName: nginx\n",
      "    rules:\n",
      "      - host: www.example.com\n",
      "        http:\n",
      "          paths:\n",
      "            - pathType: Prefix\n",
      "              backend:\n",
      "                service:\n",
      "                  name: exampleService\n",
      "                  port:\n",
      "                    number: 80\n",
      "              path: /\n",
      "    # This section is only required if TLS is to be enabled for the Ingress\n",
      "    tls:\n",
      "      - hosts:\n",
      "        - www.example.com\n",
      "        secretName: example-tls\n",
      "\n",
      "If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:\n",
      "\n",
      "  apiVersion: v1\n",
      "  kind: Secret\n",
      "  metadata:\n",
      "    name: example-tls\n",
      "    namespace: foo\n",
      "  data:\n",
      "    tls.crt: <base64 encoded cert>\n",
      "    tls.key: <base64 encoded key>\n",
      "  type: kubernetes.io/tls\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install ingress-nginx ingress-nginx \\\n",
    "  --repo https://kubernetes.github.io/ingress-nginx \\\n",
    "  --namespace ingress-nginx --create-namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or\n",
    "# !kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.0/deploy/static/provider/cloud/deploy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "items:\n",
      "- apiVersion: v1\n",
      "  kind: Pod\n",
      "  metadata:\n",
      "    creationTimestamp: \"2025-03-09T13:00:41Z\"\n",
      "    generateName: ingress-nginx-controller-56d8f56b79-\n",
      "    labels:\n",
      "      app.kubernetes.io/component: controller\n",
      "      app.kubernetes.io/instance: ingress-nginx\n",
      "      app.kubernetes.io/managed-by: Helm\n",
      "      app.kubernetes.io/name: ingress-nginx\n",
      "      app.kubernetes.io/part-of: ingress-nginx\n",
      "      app.kubernetes.io/version: 1.12.0\n",
      "      helm.sh/chart: ingress-nginx-4.12.0\n",
      "      pod-template-hash: 56d8f56b79\n",
      "    name: ingress-nginx-controller-56d8f56b79-qjpmx\n",
      "    namespace: ingress-nginx\n",
      "    ownerReferences:\n",
      "    - apiVersion: apps/v1\n",
      "      blockOwnerDeletion: true\n",
      "      controller: true\n",
      "      kind: ReplicaSet\n",
      "      name: ingress-nginx-controller-56d8f56b79\n",
      "      uid: e2fc91d2-36d9-4fa8-a8d1-3e667057b680\n",
      "    resourceVersion: \"670518\"\n",
      "    uid: a4fe62d2-3796-40c6-8598-d43f3ad479db\n",
      "  spec:\n",
      "    containers:\n",
      "    - args:\n",
      "      - /nginx-ingress-controller\n",
      "      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n",
      "      - --election-id=ingress-nginx-leader\n",
      "      - --controller-class=k8s.io/ingress-nginx\n",
      "      - --ingress-class=nginx\n",
      "      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n",
      "      - --validating-webhook=:8443\n",
      "      - --validating-webhook-certificate=/usr/local/certificates/cert\n",
      "      - --validating-webhook-key=/usr/local/certificates/key\n",
      "      env:\n",
      "      - name: POD_NAME\n",
      "        valueFrom:\n",
      "          fieldRef:\n",
      "            apiVersion: v1\n",
      "            fieldPath: metadata.name\n",
      "      - name: POD_NAMESPACE\n",
      "        valueFrom:\n",
      "          fieldRef:\n",
      "            apiVersion: v1\n",
      "            fieldPath: metadata.namespace\n",
      "      - name: LD_PRELOAD\n",
      "        value: /usr/local/lib/libmimalloc.so\n",
      "      image: registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa\n",
      "      imagePullPolicy: IfNotPresent\n",
      "      lifecycle:\n",
      "        preStop:\n",
      "          exec:\n",
      "            command:\n",
      "            - /wait-shutdown\n",
      "      livenessProbe:\n",
      "        failureThreshold: 5\n",
      "        httpGet:\n",
      "          path: /healthz\n",
      "          port: 10254\n",
      "          scheme: HTTP\n",
      "        initialDelaySeconds: 10\n",
      "        periodSeconds: 10\n",
      "        successThreshold: 1\n",
      "        timeoutSeconds: 1\n",
      "      name: controller\n",
      "      ports:\n",
      "      - containerPort: 80\n",
      "        name: http\n",
      "        protocol: TCP\n",
      "      - containerPort: 443\n",
      "        name: https\n",
      "        protocol: TCP\n",
      "      - containerPort: 8443\n",
      "        name: webhook\n",
      "        protocol: TCP\n",
      "      readinessProbe:\n",
      "        failureThreshold: 3\n",
      "        httpGet:\n",
      "          path: /healthz\n",
      "          port: 10254\n",
      "          scheme: HTTP\n",
      "        initialDelaySeconds: 10\n",
      "        periodSeconds: 10\n",
      "        successThreshold: 1\n",
      "        timeoutSeconds: 1\n",
      "      resources:\n",
      "        requests:\n",
      "          cpu: 100m\n",
      "          memory: 90Mi\n",
      "      securityContext:\n",
      "        allowPrivilegeEscalation: false\n",
      "        capabilities:\n",
      "          add:\n",
      "          - NET_BIND_SERVICE\n",
      "          drop:\n",
      "          - ALL\n",
      "        readOnlyRootFilesystem: false\n",
      "        runAsGroup: 82\n",
      "        runAsNonRoot: true\n",
      "        runAsUser: 101\n",
      "        seccompProfile:\n",
      "          type: RuntimeDefault\n",
      "      terminationMessagePath: /dev/termination-log\n",
      "      terminationMessagePolicy: File\n",
      "      volumeMounts:\n",
      "      - mountPath: /usr/local/certificates/\n",
      "        name: webhook-cert\n",
      "        readOnly: true\n",
      "      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n",
      "        name: kube-api-access-dzngb\n",
      "        readOnly: true\n",
      "    dnsPolicy: ClusterFirst\n",
      "    enableServiceLinks: true\n",
      "    nodeName: docker-desktop\n",
      "    nodeSelector:\n",
      "      kubernetes.io/os: linux\n",
      "    preemptionPolicy: PreemptLowerPriority\n",
      "    priority: 0\n",
      "    restartPolicy: Always\n",
      "    schedulerName: default-scheduler\n",
      "    securityContext: {}\n",
      "    serviceAccount: ingress-nginx\n",
      "    serviceAccountName: ingress-nginx\n",
      "    terminationGracePeriodSeconds: 300\n",
      "    tolerations:\n",
      "    - effect: NoExecute\n",
      "      key: node.kubernetes.io/not-ready\n",
      "      operator: Exists\n",
      "      tolerationSeconds: 300\n",
      "    - effect: NoExecute\n",
      "      key: node.kubernetes.io/unreachable\n",
      "      operator: Exists\n",
      "      tolerationSeconds: 300\n",
      "    volumes:\n",
      "    - name: webhook-cert\n",
      "      secret:\n",
      "        defaultMode: 420\n",
      "        secretName: ingress-nginx-admission\n",
      "    - name: kube-api-access-dzngb\n",
      "      projected:\n",
      "        defaultMode: 420\n",
      "        sources:\n",
      "        - serviceAccountToken:\n",
      "            expirationSeconds: 3607\n",
      "            path: token\n",
      "        - configMap:\n",
      "            items:\n",
      "            - key: ca.crt\n",
      "              path: ca.crt\n",
      "            name: kube-root-ca.crt\n",
      "        - downwardAPI:\n",
      "            items:\n",
      "            - fieldRef:\n",
      "                apiVersion: v1\n",
      "                fieldPath: metadata.namespace\n",
      "              path: namespace\n",
      "  status:\n",
      "    conditions:\n",
      "    - lastProbeTime: null\n",
      "      lastTransitionTime: \"2025-03-09T13:00:41Z\"\n",
      "      status: \"True\"\n",
      "      type: Initialized\n",
      "    - lastProbeTime: null\n",
      "      lastTransitionTime: \"2025-03-09T13:00:41Z\"\n",
      "      message: 'containers with unready status: [controller]'\n",
      "      reason: ContainersNotReady\n",
      "      status: \"False\"\n",
      "      type: Ready\n",
      "    - lastProbeTime: null\n",
      "      lastTransitionTime: \"2025-03-09T13:00:41Z\"\n",
      "      message: 'containers with unready status: [controller]'\n",
      "      reason: ContainersNotReady\n",
      "      status: \"False\"\n",
      "      type: ContainersReady\n",
      "    - lastProbeTime: null\n",
      "      lastTransitionTime: \"2025-03-09T13:00:41Z\"\n",
      "      status: \"True\"\n",
      "      type: PodScheduled\n",
      "    containerStatuses:\n",
      "    - image: registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa\n",
      "      imageID: \"\"\n",
      "      lastState: {}\n",
      "      name: controller\n",
      "      ready: false\n",
      "      restartCount: 0\n",
      "      started: false\n",
      "      state:\n",
      "        waiting:\n",
      "          reason: ContainerCreating\n",
      "    hostIP: 192.168.65.4\n",
      "    phase: Pending\n",
      "    qosClass: Burstable\n",
      "    startTime: \"2025-03-09T13:00:41Z\"\n",
      "kind: List\n",
      "metadata:\n",
      "  resourceVersion: \"\"\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n ingress-nginx get pod -o yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "ingress-nginx-controller-56d8f56b79-qjpmx   1/1     Running   0          13m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --namespace=ingress-nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/demo created\n",
      "service/demo exposed\n",
      "ingress.networking.k8s.io/demo-localhost created\n"
     ]
    }
   ],
   "source": [
    "# local test\n",
    "# !docker pull httpd\n",
    "!kubectl create deployment demo --image=httpd --port=80\n",
    "!kubectl expose deployment demo\n",
    "!kubectl create ingress demo-localhost --class=nginx \\\n",
    "  --rule=\"demo.localdev.me/*=demo:80\"\n",
    "\n",
    "# !kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 18080:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><h1>It works!</h1></body></html>\n"
     ]
    }
   ],
   "source": [
    "# add in /etc/hosts:\n",
    "# 127.0.0.1 demo.localdev.me\n",
    "!curl -s http://demo.localdev.me:18080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: networking.k8s.io/v1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  creationTimestamp: \"2025-03-09T13:19:24Z\"\n",
      "  generation: 1\n",
      "  name: demo-localhost\n",
      "  namespace: default\n",
      "  resourceVersion: \"672236\"\n",
      "  uid: 7b24eb87-1737-4142-9945-ffc886b73851\n",
      "spec:\n",
      "  ingressClassName: nginx\n",
      "  rules:\n",
      "  - host: demo.localdev.me\n",
      "    http:\n",
      "      paths:\n",
      "      - backend:\n",
      "          service:\n",
      "            name: demo\n",
      "            port:\n",
      "              number: 80\n",
      "        path: /\n",
      "        pathType: Prefix\n",
      "status:\n",
      "  loadBalancer: {}\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress demo-localhost -o yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingress.networking.k8s.io \"demo-localhost\" deleted\n",
      "service \"demo\" deleted\n",
      "deployment.apps \"demo\" deleted\n"
     ]
    }
   ],
   "source": [
    "# 清理\n",
    "!kubectl delete ingress demo-localhost\n",
    "!kubectl delete service demo\n",
    "!kubectl delete deployment demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"ingress-nginx\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "!helm uninstall ingress-nginx --namespace ingress-nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path-based ingress traffic routing:\n",
    "- `pathType`: Exact, Prefix, ImplementationSpecific\n",
    "\n",
    "use multiple rules in an ingress object\n",
    "\n",
    "setting the default backed:\n",
    "- `defaultBackend`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLS for an Ingress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingress configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl explain ingress.spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IngressClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reource backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReplicaSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart here!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DaemonSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job, CronJob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
